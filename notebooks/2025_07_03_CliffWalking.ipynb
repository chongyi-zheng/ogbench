{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.envs.toy_text.cliffwalking import (\n",
    "    UP, RIGHT, DOWN, LEFT, POSITION_MAPPING\n",
    ")\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from IPython import display\n",
    "from PIL import Image, ImageEnhance\n",
    "import moviepy.editor as mpy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2467df1b",
   "metadata": {},
   "source": [
    "class CliffWalkingEnv(gymnasium.Wrapper):\n",
    "    def __init__(self, random_init_state=False, max_episode_steps=1000, \n",
    "                 render_mode=\"rgb_array\", **kwargs):\n",
    "        env = gymnasium.make(\n",
    "            \"CliffWalking-v1\",\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            render_mode=render_mode,\n",
    "            **kwargs\n",
    "        )\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.nS = self.env.get_wrapper_attr('nS')\n",
    "        self.nA = self.env.get_wrapper_attr('nA')\n",
    "        self.shape = self.env.get_wrapper_attr('shape')\n",
    "\n",
    "        # The original transition probabilities for absorbing states are not correct.\n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            P[s][UP] = self._calculate_transition_prob(position, UP)\n",
    "            P[s][RIGHT] = self._calculate_transition_prob(position, RIGHT)\n",
    "            P[s][DOWN] = self._calculate_transition_prob(position, DOWN)\n",
    "            P[s][LEFT] = self._calculate_transition_prob(position, LEFT)\n",
    "        self.env.set_wrapper_attr('P', P)\n",
    "\n",
    "        if random_init_state:\n",
    "            initial_state_distrib = np.ones(self.nS)\n",
    "            cliff_positions = np.asarray(np.where(env.get_wrapper_attr('_cliff')))\n",
    "            cliff_states = np.ravel_multi_index(cliff_positions, self.shape)\n",
    "            initial_state_distrib[cliff_states] = 0.0\n",
    "            initial_state_distrib[47] = 0.0\n",
    "            initial_state_distrib /= np.sum(initial_state_distrib, keepdims=True)\n",
    "            self.env.set_wrapper_attr('initial_state_distrib', initial_state_distrib)\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        rewards = np.full((self.nS, self.nA, self.nS), np.nan)\n",
    "        transition_probs = np.zeros((self.nS, self.nA, self.nS))\n",
    "        masks = np.zeros((self.nS, self.nA, self.nS))\n",
    "        for state in range(self.nS):\n",
    "            for action in range(self.nA):\n",
    "                _, next_state, reward, terminated = self.env.get_wrapper_attr('P')[state][action][0]\n",
    "                rewards[state, action, next_state] = reward\n",
    "                transition_probs[state, action, next_state] += 1.0\n",
    "                masks[state, action, next_state] = float(not terminated)\n",
    "        transition_probs /= np.sum(transition_probs, axis=-1, keepdims=True)\n",
    "        assert np.all(np.sum(transition_probs, axis=-1) == 1.0)\n",
    "        reward_max, reward_min = np.nanmax(rewards), np.nanmin(rewards)\n",
    "        rewards[np.isnan(rewards)] = reward_min\n",
    "        assert np.all((reward_min <= rewards) & (rewards <= reward_max))\n",
    "\n",
    "        self._orig_reward_min, self._orig_reward_max = reward_min, reward_max\n",
    "        self.orig_rewards = rewards\n",
    "        self.rewards = (rewards - reward_min) / (reward_max - reward_min)\n",
    "        self.transition_probs = transition_probs\n",
    "        self.masks = masks\n",
    "\n",
    "    def _calculate_transition_prob(self, current, move):\n",
    "        \"\"\"Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        \n",
    "        The original transition probabilities for absorbing states are not correct.\n",
    "        \"\"\"\n",
    "        if not self.env.get_wrapper_attr('is_slippery'):\n",
    "            deltas = [POSITION_MAPPING[move]]\n",
    "        else:\n",
    "            deltas = [\n",
    "                POSITION_MAPPING[act] for act in [(move - 1) % 4, move, (move + 1) % 4]\n",
    "            ]\n",
    "        outcomes = []\n",
    "\n",
    "        # the single absorbing state is the goal\n",
    "        goal_position = np.asarray([self.shape[0] - 1, self.shape[1] - 1])\n",
    "        goal_state = np.ravel_multi_index(goal_position, self.shape)\n",
    "        current_position = np.array(current)\n",
    "        current_state = np.ravel_multi_index(tuple(current_position), self.shape)\n",
    "        for delta in deltas:\n",
    "            if current_state == goal_state:\n",
    "                new_state = current_state\n",
    "                reward = 0\n",
    "                is_terminated = True\n",
    "            else:\n",
    "                new_position = current_position + np.array(delta)\n",
    "                new_position = self.env.get_wrapper_attr('_limit_coordinates')(new_position).astype(int)\n",
    "                new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "                if self.env.get_wrapper_attr('_cliff')[tuple(new_position)]:\n",
    "                    reward = -100\n",
    "                    new_state = self.env.get_wrapper_attr('start_state_index')\n",
    "                else:\n",
    "                    reward = -1\n",
    "                is_terminated = (new_state == goal_state)\n",
    "            outcomes.append((1 / len(deltas), new_state, reward, is_terminated))\n",
    "        return outcomes\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        self.env.set_wrapper_attr('start_state_index', obs)\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, orig_reward, terminated, truncated, info = super().step(action)\n",
    "        reward = (orig_reward - self._orig_reward_min) / (self._orig_reward_max - self._orig_reward_min)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6d45da",
   "metadata": {},
   "source": [
    "# collect dataset\n",
    "discount = 0.95\n",
    "max_episode_steps = 100\n",
    "env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "\n",
    "# uniform behavioral policy\n",
    "behavioral_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "dataset = defaultdict(list)\n",
    "\n",
    "# dataset size = 100K\n",
    "num_episodes = 1000\n",
    "max_episode_steps = 100\n",
    "num_transitions = 0\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    for step in range(max_episode_steps):\n",
    "        action = np.random.choice(np.arange(env.nA), p=behavioral_policy[obs])\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        dataset['observations'].append(obs)\n",
    "        dataset['actions'].append(action)\n",
    "        dataset['rewards'].append(reward)\n",
    "        dataset['next_observations'].append(next_obs)\n",
    "        dataset['masks'].append(not terminated)\n",
    "        dataset['terminals'].append(truncated)\n",
    "        \n",
    "        num_transitions += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    if k in ['observations', 'actions', 'next_observations']:\n",
    "        dtype = np.int32\n",
    "    elif k == 'terminals':\n",
    "        dtype = bool\n",
    "    else:\n",
    "        dtype = np.float32\n",
    "    dataset[k] = np.array(v, dtype=dtype)\n",
    "\n",
    "print(\"num of total transitions {}\".format(num_transitions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c6cf46",
   "metadata": {},
   "source": [
    "# value iteration to find the optimal Q\n",
    "rewards = env.rewards\n",
    "transition_probs = env.transition_probs\n",
    "\n",
    "opt_q = np.zeros([env.nS, env.nA], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  opt_q = np.sum(transition_probs * rewards, axis=-1) + discount * np.einsum('ijk,k->ij', transition_probs, np.max(opt_q, axis=-1))\n",
    "opt_v = np.max(opt_q, axis=-1)\n",
    "\n",
    "# deterministic optimal policy\n",
    "opt_policy = np.zeros([env.nS, env.nA])\n",
    "opt_policy[np.arange(env.nS), np.argmax(opt_q, axis=-1)] = 1.0\n",
    "\n",
    "# value iteration to find the behavioral Q\n",
    "behavioral_q = np.zeros([env.nS, env.nA], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  behavioral_q = np.sum(transition_probs * rewards, axis=-1) + discount * np.einsum('ijk,k->ij', transition_probs, np.sum(behavioral_policy * behavioral_q, axis=-1))\n",
    "behavioral_v = np.sum(behavioral_policy * behavioral_q, axis=-1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee456c20",
   "metadata": {},
   "source": [
    "print(\"optimal q and optimal policy: \")\n",
    "print(opt_q)\n",
    "print(opt_policy)\n",
    "\n",
    "print(\"behavioral q: \")\n",
    "print(behavioral_q)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2ef9c1",
   "metadata": {},
   "source": [
    "def get_video(renders):\n",
    "    \"\"\"Return a Weights & Biases video.\n",
    "\n",
    "    It takes a list of videos and reshapes them into a single video with the specified number of columns.\n",
    "\n",
    "    Args:\n",
    "        renders: List of videos. Each video should be a numpy array of shape (t, h, w, c).\n",
    "        n_cols: Number of columns for the reshaped video. If None, it is set to the square root of the number of videos.\n",
    "    \"\"\"\n",
    "    # Pad videos to the same length.\n",
    "    max_length = max([len(render) for render in renders])\n",
    "    for i, render in enumerate(renders):\n",
    "        assert render.dtype == np.uint8\n",
    "\n",
    "        # Decrease brightness of the padded frames.\n",
    "        final_frame = render[-1]\n",
    "        final_image = Image.fromarray(final_frame)\n",
    "        enhancer = ImageEnhance.Brightness(final_image)\n",
    "        final_image = enhancer.enhance(0.5)\n",
    "        final_frame = np.array(final_image)\n",
    "\n",
    "        pad = np.repeat(final_frame[np.newaxis, ...], max_length - len(render), axis=0)\n",
    "        renders[i] = np.concatenate([render, pad], axis=0)\n",
    "\n",
    "        # Add borders.\n",
    "        renders[i] = np.pad(renders[i], ((0, 0), (1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    renders = np.array(renders)  # (n, t, h, w, c)\n",
    "\n",
    "    return renders"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3993170e",
   "metadata": {},
   "source": [
    "# evaluation the optimal pi\n",
    "max_episode_steps = 100\n",
    "eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "\n",
    "num_episodes = 6\n",
    "successes = []\n",
    "renders = []\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, info = eval_env.reset()\n",
    "    render = eval_env.render().copy()\n",
    "    while not done:\n",
    "        action = np.random.choice(np.arange(eval_env.nA), p=opt_policy[obs])\n",
    "        next_obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_render = eval_env.render().copy()\n",
    "        \n",
    "        traj_dataset['observations'].append(obs)\n",
    "        traj_dataset['actions'].append(action)\n",
    "        traj_dataset['rewards'].append(reward)\n",
    "        traj_dataset['next_observations'].append(next_obs)\n",
    "        traj_dataset['renders'].append(render)\n",
    "        \n",
    "        obs = next_obs\n",
    "        render = next_render\n",
    "    traj_dataset['renders'].append(render)  # append the last frame\n",
    "    \n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "    renders.append(np.asarray(traj_dataset['renders']))\n",
    "\n",
    "sr = np.mean(successes)\n",
    "print(\"success rate = {}\".format(sr))\n",
    "\n",
    "videos = get_video(renders)\n",
    "\n",
    "fps = 15\n",
    "num_rows = 2\n",
    "num_cols = 3\n",
    "clip_array = []\n",
    "for row in range(num_rows):\n",
    "    clip_row = []\n",
    "    for col in range(num_cols):\n",
    "        idx = row * num_cols + col\n",
    "        \n",
    "        clip = mpy.ImageSequenceClip(list(videos[idx]), fps=fps)\n",
    "        clip_row.append(clip)\n",
    "    clip_array.append(clip_row)\n",
    "\n",
    "clip_array = mpy.clips_array(clip_array)\n",
    "clip_array.ipython_display(fps=fps, loop=True, autoplay=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d45e239a",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c45194",
   "metadata": {},
   "source": [
    "def sample_batch(batch_size):\n",
    "    dataset_size = len(dataset['observations'])\n",
    "    idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    batch = jax.tree_util.tree_map(lambda arr: arr[idxs], dataset)\n",
    "    \n",
    "    return batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d483e47",
   "metadata": {},
   "source": [
    "def plot_metrics(metrics, logyscale_stats=[], title=''):\n",
    "  # learning curves\n",
    "  nrows = np.ceil(len(metrics) / 4).astype(int)\n",
    "  ncols = 4\n",
    "  f, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "  if nrows == 1:\n",
    "    axes = np.array([axes])\n",
    "  f.set_figheight(3 * nrows)\n",
    "  f.set_figwidth(3 * ncols)\n",
    "\n",
    "  for idx, (name, val) in enumerate(metrics.items()):\n",
    "    v = np.array(val)\n",
    "    if len(v) == 0:\n",
    "      continue\n",
    "\n",
    "    x, y = v[:, 0], v[:, 1]\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "\n",
    "    if 'train' in name:\n",
    "      y = gaussian_filter1d(y, 100)\n",
    "    ax.plot(x, y)\n",
    "    if name in logyscale_stats:\n",
    "      ax.set_yscale('log')\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "  f.suptitle(title)\n",
    "\n",
    "  return f"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d49aa9f6",
   "metadata": {},
   "source": [
    "#### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b668f5",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    action = jax.nn.one_hot(action, env.nA)\n",
    "    inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "\n",
    "    qs = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    qs = qs.squeeze(-1)\n",
    "\n",
    "    return qs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a46bfe3",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "critic = Critic()\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q = critic.apply(params, batch['observations'], batch['actions'])\n",
    "  \n",
    "  # next_q1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']))\n",
    "  # next_q2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']))\n",
    "  # next_q = jnp.maximum(next_q1, next_q2)\n",
    "  next_observations = batch['next_observations'][:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  next_actions = jnp.arange(env.nA)[None, :].repeat(batch_size, axis=0).reshape(-1)\n",
    "  next_qs = critic.apply(target_params, next_observations, next_actions)\n",
    "  next_qs = next_qs.reshape([batch_size, env.nA])\n",
    "  next_q = jnp.max(next_qs, axis=-1)\n",
    "  target_q = batch['rewards'] + discount * next_q\n",
    "\n",
    "  loss = jnp.mean((q - target_q) ** 2)\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "# optimizer = splus(learning_rate=3e-4 * 512 * 2)\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(env.nS)[:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.nA)[None, :].repeat(env.nS, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions)\n",
    "  q = q.reshape([env.nS, env.nA])\n",
    "  # scaled_q = q * (opt_q / q).mean()\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q.reshape(-1),\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q))\n",
    "  # scaled_q_err_mean = jnp.mean(np.abs(scaled_q - opt_q))\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q.reshape(-1))[0, 1]\n",
    "  \n",
    "  # evaluate the policy\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  policy = jax.nn.one_hot(a, env.nA)\n",
    "  policy = np.asarray(policy)\n",
    "  \n",
    "  # evaluation\n",
    "  num_eval_episodes = 100\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=policy[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'success_rate': sr\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_batch(batch_size)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e974b03",
   "metadata": {},
   "source": [
    "q_learning_metrics = metrics\n",
    "print(q_learning_metrics['eval/q_err_mean'][-1])\n",
    "print(q_learning_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(q_learning_metrics['eval/q_corr_coef'][-1])\n",
    "print(q_learning_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d494c9d5",
   "metadata": {},
   "source": [
    "#### C51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261e58a9",
   "metadata": {},
   "source": [
    "class CategoricalCritic(nn.Module):\n",
    "  atoms: jnp.ndarray\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    action = jax.nn.one_hot(action, env.nA)\n",
    "    inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "\n",
    "    num_atoms = len(self.atoms)\n",
    "\n",
    "    logits = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(num_atoms),\n",
    "    ])(inputs)\n",
    "    \n",
    "    logits = logits.reshape([obs.shape[0], num_atoms])\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    qs = jnp.sum(probs * self.atoms[None], axis=-1)\n",
    "\n",
    "    return qs, probs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc64006",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_atoms = 51\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "rewards = env.rewards\n",
    "v_min = rewards.min() / (1 - discount)\n",
    "v_max = rewards.max() / (1 - discount)\n",
    "atoms = jnp.linspace(v_min, v_max, num_atoms)\n",
    "delta_atom = (v_max - v_min) / (num_atoms - 1)\n",
    "offset = jnp.arange(batch_size, dtype=jnp.int32) * num_atoms\n",
    "offset = offset[:, None]\n",
    "offset = jnp.broadcast_to(offset, (batch_size, num_atoms))\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2 ** 32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "cate_critic = CategoricalCritic(atoms)\n",
    "cate_critic_params = cate_critic.init(critic_key, example_batch['observations'], example_batch['actions'])\n",
    "target_cate_critic_params = copy.deepcopy(cate_critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q, probs = cate_critic.apply(params, batch['observations'], batch['actions'])\n",
    "\n",
    "  # next_q1, next_probs1 = cate_critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']))\n",
    "  # next_q2, next_probs2 = cate_critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']))\n",
    "  # next_q = jnp.stack([next_q1, next_q2], axis=1)\n",
    "  # next_probs = jnp.stack([next_probs1, next_probs2], axis=1)\n",
    "  # next_actions = jnp.argmax(next_q, axis=1)\n",
    "  next_observations = batch['next_observations'][:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  next_actions = jnp.arange(env.nA)[None, :].repeat(batch_size, axis=0).reshape(-1)\n",
    "  next_qs, next_probs = cate_critic.apply(target_params, next_observations, next_actions)\n",
    "  next_probs = next_probs.reshape([batch_size, env.nA, num_atoms])\n",
    "  next_qs = next_qs.reshape([batch_size, env.nA])\n",
    "  next_actions = jnp.argmax(next_qs, axis=-1)\n",
    "  next_probs = jnp.sum(next_probs * jax.nn.one_hot(next_actions, env.nA)[..., None], axis=1)\n",
    "\n",
    "  projected_atoms = jnp.clip(batch['rewards'][:, None] + discount * atoms[None], v_min, v_max)\n",
    "  projected_bins = (projected_atoms - v_min) / delta_atom\n",
    "  lower_bins = jnp.floor(projected_bins).astype(jnp.int32)\n",
    "  upper_bins = jnp.ceil(projected_bins).astype(jnp.int32)\n",
    "\n",
    "  delta_mass_lower = (upper_bins + (lower_bins == upper_bins).astype(jnp.int32) - projected_bins) * next_probs\n",
    "  delta_mass_upper = (projected_bins - lower_bins) * next_probs\n",
    "  \n",
    "  mass = jnp.zeros(batch_size * num_atoms)\n",
    "  mass = mass.at[(lower_bins + offset).ravel()].add(delta_mass_lower.ravel())\n",
    "  mass = mass.at[(upper_bins + offset).ravel()].add(delta_mass_upper.ravel())\n",
    "  mass = mass.reshape(batch_size, num_atoms)\n",
    "\n",
    "  loss = jnp.sum(-mass * jnp.log(probs + 1e-8), axis=-1).mean()\n",
    "  \n",
    "  # logging\n",
    "  # q = jnp.sum(qs * jax.nn.one_hot(batch['actions'], env.nA), axis=1)\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "# optimizer = splus(learning_rate=3e-4 * 512 * 2)\n",
    "optimizer = optax.adam(learning_rate=5e-4)\n",
    "opt_state = optimizer.init(cate_critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(env.nS)[:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.nA)[None, :].repeat(env.nS, axis=0).reshape(-1)\n",
    "  q, _ = cate_critic.apply(params, obs, actions)\n",
    "  q = q.reshape([env.nS, env.nA])\n",
    "  # scaled_q = q * (opt_q / q).mean()\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q.reshape(-1),\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q))\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q.reshape(-1))[0, 1]\n",
    "  \n",
    "  # evaluate the policy\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  policy = jax.nn.one_hot(a, env.nA)\n",
    "  policy = np.asarray(policy)\n",
    "  \n",
    "  # evaluation\n",
    "  num_eval_episodes = 100\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=policy[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_batch(batch_size)\n",
    "  cate_critic_params, target_cate_critic_params, opt_state, loss, info = update_fn(\n",
    "    cate_critic_params, target_cate_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(cate_critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd74ffd",
   "metadata": {},
   "source": [
    "c51_metrics = metrics\n",
    "\n",
    "print(c51_metrics['eval/q_err_mean'][-1])\n",
    "print(c51_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(c51_metrics['eval/q_corr_coef'][-1])\n",
    "print(c51_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c702ce75",
   "metadata": {},
   "source": [
    "### IQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea14c6c4",
   "metadata": {},
   "source": [
    "class CosineEmbedding(nn.Module):\n",
    "  num_cosines: int = 64\n",
    "  embedding_dim: int = 512\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, taus):\n",
    "    batch_size, num_taus = taus.shape[0:2]\n",
    "\n",
    "    freqs = jnp.pi * jnp.arange(1, self.num_cosines + 1)[None, None]\n",
    "    cos_embeddings = jnp.cos(freqs * taus[..., None]).reshape(\n",
    "        batch_size * num_taus, self.num_cosines)\n",
    "    tau_embeddings = nn.Sequential([\n",
    "        nn.Dense(self.embedding_dim),\n",
    "        nn.gelu,\n",
    "    ])(cos_embeddings)\n",
    "    \n",
    "    tau_embeddings = tau_embeddings.reshape(batch_size, num_taus, self.embedding_dim)\n",
    "    \n",
    "    return tau_embeddings\n",
    "\n",
    "class ImplicitQuantileCritic(nn.Module):\n",
    "  cos_embed: CosineEmbedding = CosineEmbedding()\n",
    "  embedding_dim: int = 512\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, obs, actions, taus):\n",
    "    tau_embeddings = self.cos_embed(taus)  # (batch_size, num_taus, embedding_dims)\n",
    "    \n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    actions = jax.nn.one_hot(actions, env.nA)\n",
    "    inputs = jnp.concatenate([obs, actions], axis=-1)\n",
    "\n",
    "    sa_embeddings = nn.Sequential([\n",
    "      nn.Dense(self.embedding_dim),\n",
    "      nn.gelu,\n",
    "    ])(inputs)\n",
    "    \n",
    "    embeddings = (sa_embeddings[:, None] * tau_embeddings)  # (batch_size, num_taus, embedding_dims)\n",
    "    quantiles = nn.Sequential([\n",
    "      nn.Dense(self.embedding_dim),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1)\n",
    "    ])(embeddings)  # (batch_size, num_taus, 1)\n",
    "\n",
    "    return quantiles"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd72fb72",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_taus = 32\n",
    "num_tau_primes = 32\n",
    "kappa = 1.0\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "rewards = env.rewards\n",
    "# v_min = rewards.min() / (1 - discount)\n",
    "# v_max = rewards.max() / (1 - discount)\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2 ** 32))\n",
    "key, iq_critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "iq_critic = ImplicitQuantileCritic()\n",
    "iq_critic_params = iq_critic.init(iq_critic_key, \n",
    "                                  example_batch['observations'], \n",
    "                                  example_batch['actions'], \n",
    "                                  example_batch['observations'][:, None].astype(jnp.float32))\n",
    "target_iq_critic_params = copy.deepcopy(iq_critic_params)\n",
    "\n",
    "def quantile_huber_loss(td_errors, taus, kappa):\n",
    "  element_wise_huber_loss = jnp.where(\n",
    "    jnp.abs(td_errors) <= kappa, \n",
    "    0.5 * td_errors ** 2,\n",
    "    kappa * (jnp.abs(td_errors) - 0.5 * kappa)\n",
    "  )\n",
    "  \n",
    "  # huber_loss = jnp.abs(\n",
    "  #   taus[..., None] - (jax.lax.stop_gradient(td_errors) < 0).astype(jnp.float32)\n",
    "  # ) * element_wise_huber_loss / kappa  # (batch_size, num_taus, num_tau_primes)\n",
    "  huber_loss = jnp.abs(\n",
    "    taus[..., None] - (td_errors < 0).astype(jnp.float32)\n",
    "  ) * element_wise_huber_loss / kappa  # (batch_size, num_taus, num_tau_primes)\n",
    "  \n",
    "  huber_loss = huber_loss.sum(axis=1).mean()\n",
    "  \n",
    "  return huber_loss\n",
    "\n",
    "def loss_fn(params, target_params, batch, rng):\n",
    "  rng, tau_rng, next_tau_rng, tau_prime_rng = jax.random.split(rng, 4)\n",
    "  \n",
    "  taus = jax.random.uniform(tau_rng, shape=(batch_size, num_taus))\n",
    "  quantiles = iq_critic.apply(params, batch['observations'], batch['actions'], taus)  # (batch_size, num_taus, 1)\n",
    "  \n",
    "  next_taus = jax.random.uniform(next_tau_rng, shape=(batch_size * env.nA, num_taus))\n",
    "  next_observations = batch['next_observations'][:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  next_actions = jnp.arange(env.nA)[None, :].repeat(batch_size, axis=0).reshape(-1)\n",
    "  next_quantiles = iq_critic.apply(target_params, next_observations, next_actions, next_taus)\n",
    "  next_quantiles = next_quantiles.reshape(batch_size, env.nA, num_taus)\n",
    "  next_qs = next_quantiles.mean(axis=-1)\n",
    "  next_actions = jnp.argmax(next_qs, axis=-1)\n",
    "  \n",
    "  # next_qs, next_probs = cate_critic.apply(target_params, next_observations, next_actions)\n",
    "  # next_probs = next_probs.reshape([batch_size, env.nA, num_atoms])\n",
    "  # next_qs = next_qs.reshape([batch_size, env.nA])\n",
    "  # next_actions = jnp.argmax(next_qs, axis=-1)\n",
    "  # next_probs = jnp.sum(next_probs * jax.nn.one_hot(next_actions, env.nA)[..., None], axis=1)\n",
    "  \n",
    "  tau_primes = jax.random.uniform(tau_prime_rng, shape=(batch_size, num_tau_primes))\n",
    "  next_quantiles = iq_critic.apply(target_params, batch['next_observations'], next_actions, tau_primes)\n",
    "  next_quantiles = next_quantiles.transpose([0, 2, 1])  # (batch_size, 1, num_tau_primes)\n",
    "  target_quantiles = batch['rewards'][:, None, None] + discount * next_quantiles\n",
    "  \n",
    "  td_errors = target_quantiles - quantiles  # (batch_size, num_taus, num_tau_primes)\n",
    "  \n",
    "  loss = quantile_huber_loss(td_errors, taus, kappa)\n",
    "  \n",
    "  # for logging\n",
    "  q = jnp.mean(quantiles.squeeze(-1), axis=1)\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=5e-4)\n",
    "opt_state = optimizer.init(iq_critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch, rng):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch, rng)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def evaluate_fn(params, rng):\n",
    "  obs = jnp.arange(env.nS)[:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.nA)[None, :].repeat(env.nS, axis=0).reshape(-1)\n",
    "  \n",
    "  rng, tau_rng = jax.random.split(rng, 2)\n",
    "  taus = jax.random.uniform(tau_rng, shape=(obs.shape[0], num_taus))\n",
    "  quantiles = iq_critic.apply(params, obs, actions, taus)  # (nS * nA, num_taus, 1)\n",
    "  q = jnp.mean(quantiles.squeeze(-1), axis=1)\n",
    "  q = q.reshape([env.nS, env.nA])\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q.reshape(-1),\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q))\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q.reshape(-1))[0, 1]\n",
    "  \n",
    "  # evaluate the policy\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  policy = jax.nn.one_hot(a, env.nA)\n",
    "  policy = np.asarray(policy)\n",
    "  \n",
    "  # evaluation\n",
    "  num_eval_episodes = 100\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=policy[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  key, train_key = jax.random.split(key)\n",
    "  \n",
    "  batch = sample_batch(batch_size)\n",
    "  iq_critic_params, target_iq_critic_params, opt_state, loss, info = update_fn(\n",
    "    iq_critic_params, target_iq_critic_params, opt_state, batch, train_key)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    key, eval_key = jax.random.split(key)\n",
    "    eval_info = evaluate_fn(iq_critic_params, eval_key)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc2582a",
   "metadata": {},
   "source": [
    "#### Flow distributional Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95c363c2",
   "metadata": {},
   "source": [
    "class CriticVectorField(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, ret, time, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    action = jax.nn.one_hot(action, env.nA)\n",
    "    inputs = jnp.concatenate([ret, time, obs, action], axis=-1)\n",
    "\n",
    "    vector_field = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      # nn.LayerNorm(),\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    return vector_field"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3e47d",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "lam = 0.0\n",
    "num_flow_steps = 10\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "rewards = env.rewards\n",
    "min_reward = rewards.min()\n",
    "max_reward = rewards.max()\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2 ** 32))\n",
    "key, critic_vf_key, target_critic_vf_key = jax.random.split(key, 3)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "critic_vf = CriticVectorField()\n",
    "\n",
    "ex_returns = jnp.ones((2, 1), dtype=example_batch['rewards'].dtype)\n",
    "ex_times = jnp.ones((2, 1), dtype=example_batch['rewards'].dtype)\n",
    "\n",
    "critic_vf_params = critic_vf.init(critic_vf_key, \n",
    "                                  ex_returns, ex_times, \n",
    "                                  example_batch['observations'], example_batch['actions'])\n",
    "target_critic_vf_params = copy.deepcopy(critic_vf_params)\n",
    "\n",
    "@jax.jit\n",
    "def compute_flow_returns(\n",
    "  params,\n",
    "  noises,\n",
    "  observations,\n",
    "  actions,\n",
    "  init_times=None,\n",
    "  end_times=None,\n",
    "):\n",
    "  \"\"\"Compute returns from the return flow model using the Euler method.\"\"\"\n",
    "  noisy_returns = noises\n",
    "  if init_times is None:\n",
    "    init_times = jnp.zeros((*noisy_returns.shape[:-1], 1), dtype=noisy_returns.dtype)\n",
    "  if end_times is None:\n",
    "    end_times = jnp.ones((*noisy_returns.shape[:-1], 1), dtype=noisy_returns.dtype)\n",
    "  step_size = (end_times - init_times) / num_flow_steps\n",
    "\n",
    "  def func(carry, i):\n",
    "    \"\"\"\n",
    "    carry: (noisy_goals, )\n",
    "    i: current step index\n",
    "    \"\"\"\n",
    "    (noisy_returns, ) = carry\n",
    "\n",
    "    times = i * step_size + init_times\n",
    "    \n",
    "    # euler method\n",
    "    vector_field = critic_vf.apply(\n",
    "      params, noisy_returns, times, observations, actions)\n",
    "    new_noisy_returns = noisy_returns + step_size * vector_field\n",
    "    \n",
    "    # midpoint method\n",
    "    # mid_vector_field = critic_vf.apply(\n",
    "    #   params, noisy_returns, times, observations, actions)\n",
    "    # mid_noisy_returns = noisy_returns + 0.5 * step_size * mid_vector_field\n",
    "    # mid_times = times + 0.5 * step_size\n",
    "    # vector_field = critic_vf.apply(\n",
    "    #   params, mid_noisy_returns, mid_times, observations, actions)\n",
    "    # new_noisy_returns = noisy_returns + step_size * vector_field\n",
    "    \n",
    "    new_noisy_returns = jnp.clip(\n",
    "      new_noisy_returns,\n",
    "      min_reward / (1.0 - discount),\n",
    "      max_reward / (1.0 - discount),\n",
    "    )\n",
    "\n",
    "    return (new_noisy_returns, ), None\n",
    "\n",
    "  # Use lax.scan to do the iteration\n",
    "  (noisy_returns, ), _ = jax.lax.scan(\n",
    "      func, (noisy_returns,), jnp.arange(num_flow_steps))\n",
    "  # noisy_returns = jnp.clip(\n",
    "  #   noisy_returns,\n",
    "  #   min_reward / (1.0 - discount),\n",
    "  #   max_reward / (1.0 - discount),\n",
    "  # )\n",
    "\n",
    "  return noisy_returns\n",
    "\n",
    "def loss_fn(params, target_params, batch, rng):\n",
    "  rng, q_noise_rng, noise_rng, time_rng, q_rng = jax.random.split(rng, 5)\n",
    "  \n",
    "  # q_noises = jax.random.uniform(q_noise_rng, (batch_size * env.nA, 1),\n",
    "  #                               minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  q_noises = jax.random.uniform(q_noise_rng, (batch_size * env.nA, 1))\n",
    "  # q_noises = jax.random.normal(q_noise_rng, (batch_size * env.nA, 1))\n",
    "  # q1 = (q_noises1 + critic_vf.apply(\n",
    "  #   target_params, q_noises1, jnp.zeros((batch_size, 1)), \n",
    "  #   batch['observations'], jnp.zeros_like(batch['observations'])))\n",
    "  # q2 = (q_noises2 + critic_vf.apply(\n",
    "  #   target_params, q_noises2, jnp.zeros((batch_size, 1)), \n",
    "  #   batch['observations'], jnp.ones_like(batch['observations'])))\n",
    "  # q = jnp.concatenate([q1, q2], axis=1)\n",
    "  next_observations = batch['next_observations'][:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  next_actions = jnp.arange(env.nA)[None, :].repeat(batch_size, axis=0).reshape(-1)\n",
    "  # qs = compute_flow_returns(\n",
    "  #   target_params, q_noises, next_observations, next_actions)\n",
    "  qs = (q_noises + critic_vf.apply(\n",
    "    target_params, q_noises, jnp.zeros_like(q_noises), next_observations, next_actions))\n",
    "  qs = qs.reshape([batch_size, env.nA])\n",
    "  next_actions = jnp.argmax(qs, axis=1)\n",
    "  next_actions = jax.lax.stop_gradient(next_actions)\n",
    "  # next_actions = batch['next_actions']\n",
    "  \n",
    "  times = jax.random.uniform(time_rng, (batch_size, 1))\n",
    "  # noises = jax.random.uniform(noise_rng, (batch_size, 1),\n",
    "  #                             minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  noises = jax.random.uniform(noise_rng, (batch_size, 1))\n",
    "  # noises = jax.random.normal(noise_rng, (batch_size, 1))\n",
    "  # next_returns = compute_flow_returns(\n",
    "  #   target_params, noises, batch['next_observations'], next_actions)\n",
    "  # noises = jnp.sum(q_noises.reshape([batch_size, env.nA, 1]) * jax.nn.one_hot(next_actions, env.nA)[..., None], axis=1)\n",
    "  next_returns = compute_flow_returns(\n",
    "    target_params, noises, batch['next_observations'], next_actions)\n",
    "  returns = jnp.expand_dims(batch['rewards'], axis=-1) + discount * next_returns\n",
    "  # returns = jnp.clip(\n",
    "  #   returns,\n",
    "  #   min_reward / (1.0 - discount),\n",
    "  #   max_reward / (1.0 - discount),\n",
    "  # )\n",
    "  \n",
    "  # rng, noise_rng = jax.random.split(rng)\n",
    "  # noises = jax.random.uniform(noise_rng, (batch_size, 1),\n",
    "  #                             minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  # noises = jax.random.normal(noise_rng, (batch_size, 1))\n",
    "  noisy_returns = times * returns + (1.0 - times) * noises\n",
    "  target_vector_field = returns - noises\n",
    "  \n",
    "  vector_field = critic_vf.apply(params, noisy_returns, times, batch['observations'], batch['actions'])\n",
    "  vector_field_loss = jnp.mean((vector_field - target_vector_field) ** 2)\n",
    "  \n",
    "  # rng, noise_rng = jax.random.split(rng)\n",
    "  # noises = jax.random.uniform(noise_rng, (batch_size, 1),\n",
    "  #                             minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  # noisy_next_returns = compute_flow_returns(\n",
    "  #   target_params, noises, batch['next_observations'], next_actions, end_times=times)\n",
    "  # transformed_noisy_returns = jnp.expand_dims(batch['rewards'], axis=-1) + discount * noisy_next_returns\n",
    "  # # transformed_noisy_returns = jnp.clip(\n",
    "  # #   transformed_noisy_returns,\n",
    "  # #   min_reward / (1.0 - discount),\n",
    "  # #   max_reward / (1.0 - discount),\n",
    "  # # )\n",
    "\n",
    "  # vector_field = critic_vf.apply(\n",
    "  #   params, transformed_noisy_returns, times, batch['observations'], batch['actions'])\n",
    "  # next_vector_field = critic_vf.apply(\n",
    "  #   params, noisy_next_returns, times, batch['next_observations'], next_actions)\n",
    "  # target_vector_field = critic_vf.apply(\n",
    "  #   target_params, transformed_noisy_returns, times, batch['observations'], batch['actions'])\n",
    "  # target_next_vector_field = critic_vf.apply(\n",
    "  #   target_params, noisy_next_returns, times, batch['next_observations'], next_actions)\n",
    "  # bootstrapped_vector_field_loss = jnp.mean((vector_field - target_next_vector_field) ** 2 +\n",
    "  #                                           (next_vector_field - target_vector_field) ** 2)\n",
    "  \n",
    "  # loss = vector_field_loss + lam * bootstrapped_vector_field_loss\n",
    "  loss = vector_field_loss\n",
    "  \n",
    "  # Additional metrics for logging.\n",
    "  # q_noises = jax.random.uniform(q_rng, (batch_size, 1),\n",
    "  #                               minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  q_noises = jax.random.uniform(q_rng, (batch_size, 1))\n",
    "  # q_noises = jax.random.normal(q_rng, (batch_size, 1))\n",
    "  # q = compute_flow_returns(\n",
    "  #   params, q_noises, batch['observations'], batch['actions'])\n",
    "  q = (q_noises + critic_vf.apply(\n",
    "    params, q_noises, jnp.zeros((batch_size, 1)), \n",
    "    batch['observations'], batch['actions']))\n",
    "\n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'vector_field_loss': vector_field_loss,\n",
    "    # 'bootstrapped_vector_field_loss': bootstrapped_vector_field_loss,\n",
    "    'q_mean': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "# optimizer = splus(learning_rate=5e-4 * 512 * 2)\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_vf_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch, rng):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch, rng)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def evaluate_fn(params, rng):\n",
    "  obs = jnp.arange(env.nS)[:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.nA)[None, :].repeat(env.nS, axis=0).reshape(-1)\n",
    "\n",
    "  # qs = []\n",
    "  # for _ in range(16):\n",
    "  #   # q_noises = jax.random.uniform(q_rng, (obs.shape[0], 1),\n",
    "  #   #                               minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  #   rng, q_rng = jax.random.split(rng)\n",
    "  #   # q_noises = jax.random.normal(q_rng, (obs.shape[0], 1))\n",
    "  #   q_noises = jax.random.uniform(q_rng, (obs.shape[0], 1))\n",
    "  #   # q = compute_flow_returns(\n",
    "  #   #   params, q_noises, obs, actions)\n",
    "  #   q = (q_noises + critic_vf.apply(\n",
    "  #     params, q_noises, jnp.zeros((obs.shape[0], 1)), \n",
    "  #     obs, actions))\n",
    "\n",
    "  rng, q_rng = jax.random.split(rng)\n",
    "  # q_noises = jax.random.normal(q_rng, (obs.shape[0], 1))\n",
    "  q_noises = jax.random.uniform(q_rng, (obs.shape[0], 1))\n",
    "  # q = compute_flow_returns(\n",
    "  #   params, q_noises, obs, actions)\n",
    "  q = (q_noises + critic_vf.apply(params, q_noises, jnp.zeros((obs.shape[0], 1)), obs, actions))\n",
    "  q = q.reshape(env.nS, env.nA)\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q.reshape(-1),\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q))\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q.reshape(-1))[0, 1]\n",
    "  \n",
    "  # evaluate the policy\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  policy = jax.nn.one_hot(a, env.nA)\n",
    "  policy = np.asarray(policy)\n",
    "  \n",
    "  # evaluation\n",
    "  num_eval_episodes = 100\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=policy[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  key, train_key = jax.random.split(key)\n",
    "  batch = sample_batch(batch_size)\n",
    "  \n",
    "  critic_vf_params, target_critic_vf_params, opt_state, loss, info = update_fn(\n",
    "    critic_vf_params, target_critic_vf_params, opt_state, batch, train_key)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    key, eval_key = jax.random.split(key)\n",
    "    eval_info = evaluate_fn(critic_vf_params, eval_key)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca453d7",
   "metadata": {},
   "source": [
    "# lam = 0.01\n",
    "fdql_metrics = metrics\n",
    "print(fdql_metrics['eval/q_err_mean'][-1])\n",
    "print(fdql_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(fdql_metrics['eval/q_corr_coef'][-1])\n",
    "print(fdql_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "421988b6",
   "metadata": {},
   "source": [
    "#### Flow distributional Q-learning (IQN style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1878806",
   "metadata": {},
   "source": [
    "class CosineEmbedding(nn.Module):\n",
    "  num_cosines: int = 64\n",
    "  embedding_dim: int = 512\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, returns):\n",
    "    batch_size, num_rand_samples = returns.shape[0:2]\n",
    "\n",
    "    freqs = jnp.pi * jnp.arange(1, self.num_cosines + 1)[None, None]\n",
    "    cos_embeddings = jnp.cos(freqs * returns).reshape(\n",
    "        batch_size * num_rand_samples, self.num_cosines)\n",
    "    return_embeddings = nn.Sequential([\n",
    "        nn.Dense(self.embedding_dim),\n",
    "        nn.gelu,\n",
    "    ])(cos_embeddings)\n",
    "    \n",
    "    return_embeddings = return_embeddings.reshape(batch_size, num_rand_samples, self.embedding_dim)\n",
    "    \n",
    "    return return_embeddings\n",
    "\n",
    "class CriticVectorField(nn.Module):\n",
    "  cos_embed: CosineEmbedding = CosineEmbedding()\n",
    "  embedding_dim: int = 512\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, returns, times, obs, actions):\n",
    "    return_embeddings = self.cos_embed(returns)  # (batch_size, num_rand_samples, embedding_dims)\n",
    "    \n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    actions = jax.nn.one_hot(actions, env.nA)\n",
    "    inputs = jnp.concatenate([obs, actions, times], axis=-1)\n",
    "\n",
    "    sat_embeddings = nn.Sequential([\n",
    "      nn.Dense(self.embedding_dim),\n",
    "      nn.gelu,\n",
    "    ])(inputs)\n",
    "    \n",
    "    embeddings = (sat_embeddings[:, None] * return_embeddings)  # (batch_size, num_rand_samples, embedding_dims)\n",
    "    vector_field = nn.Sequential([\n",
    "      nn.Dense(self.embedding_dim),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1)\n",
    "    ])(embeddings)  # (batch_size, num_rand_samples, 1)\n",
    "\n",
    "    return vector_field"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca09a",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "lam = 0.0\n",
    "num_rand_samples = 16\n",
    "num_flow_steps = 10\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "rewards = env.rewards\n",
    "min_reward = rewards.min()\n",
    "max_reward = rewards.max()\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2 ** 32))\n",
    "key, critic_vf_key, target_critic_vf_key = jax.random.split(key, 3)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "critic_vf = CriticVectorField()\n",
    "\n",
    "ex_returns = jnp.ones((2, 1), dtype=example_batch['rewards'].dtype)\n",
    "ex_times = jnp.ones((2, 1), dtype=example_batch['rewards'].dtype)\n",
    "\n",
    "critic_vf_params = critic_vf.init(critic_vf_key, \n",
    "                                  ex_returns, ex_times, \n",
    "                                  example_batch['observations'], example_batch['actions'])\n",
    "target_critic_vf_params = copy.deepcopy(critic_vf_params)\n",
    "\n",
    "@jax.jit\n",
    "def compute_flow_returns(\n",
    "  params,\n",
    "  noises,\n",
    "  observations,\n",
    "  actions,\n",
    "  init_times=None,\n",
    "  end_times=None,\n",
    "):\n",
    "  \"\"\"Compute returns from the return flow model using the Euler method.\"\"\"\n",
    "  noisy_returns = noises\n",
    "  if init_times is None:\n",
    "    init_times = jnp.zeros((*noisy_returns.shape[:-2], 1), dtype=noisy_returns.dtype)\n",
    "  if end_times is None:\n",
    "    end_times = jnp.ones((*noisy_returns.shape[:-2], 1), dtype=noisy_returns.dtype)\n",
    "  step_size = (end_times - init_times) / num_flow_steps\n",
    "\n",
    "  def func(carry, i):\n",
    "    \"\"\"\n",
    "    carry: (noisy_goals, )\n",
    "    i: current step index\n",
    "    \"\"\"\n",
    "    (noisy_returns, ) = carry\n",
    "\n",
    "    times = i * step_size + init_times\n",
    "    \n",
    "    # euler method\n",
    "    vector_field = critic_vf.apply(\n",
    "      params, noisy_returns, times, observations, actions)\n",
    "    new_noisy_returns = noisy_returns + step_size[:, None] * vector_field\n",
    "\n",
    "    \n",
    "    # midpoint method\n",
    "    # mid_vector_field = critic_vf.apply(\n",
    "    #   params, noisy_returns, times, observations, actions)\n",
    "    # mid_noisy_returns = noisy_returns + 0.5 * step_size * mid_vector_field\n",
    "    # mid_times = times + 0.5 * step_size\n",
    "    # vector_field = critic_vf.apply(\n",
    "    #   params, mid_noisy_returns, mid_times, observations, actions)\n",
    "    # new_noisy_returns = noisy_returns + step_size * vector_field\n",
    "    \n",
    "    new_noisy_returns = jnp.clip(\n",
    "      new_noisy_returns,\n",
    "      min_reward / (1.0 - discount),\n",
    "      max_reward / (1.0 - discount),\n",
    "    )\n",
    "\n",
    "    return (new_noisy_returns, ), None\n",
    "\n",
    "  # Use lax.scan to do the iteration\n",
    "  (noisy_returns, ), _ = jax.lax.scan(\n",
    "      func, (noisy_returns,), jnp.arange(num_flow_steps))\n",
    "  # noisy_returns = jnp.clip(\n",
    "  #   noisy_returns,\n",
    "  #   min_reward / (1.0 - discount),\n",
    "  #   max_reward / (1.0 - discount),\n",
    "  # )\n",
    "\n",
    "  return noisy_returns\n",
    "\n",
    "def loss_fn(params, target_params, batch, rng):\n",
    "  rng, q_noise_rng, noise_rng, time_rng, q_rng = jax.random.split(rng, 5)\n",
    "  \n",
    "  # q_noises = jax.random.uniform(q_noise_rng, (batch_size * env.nA, 1),\n",
    "  #                               minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  q_noises = jax.random.uniform(q_noise_rng, (batch_size * env.nA, num_rand_samples, 1))\n",
    "  # q_noises = jax.random.normal(q_noise_rng, (batch_size * env.nA, 1))\n",
    "  # q1 = (q_noises1 + critic_vf.apply(\n",
    "  #   target_params, q_noises1, jnp.zeros((batch_size, 1)), \n",
    "  #   batch['observations'], jnp.zeros_like(batch['observations'])))\n",
    "  # q2 = (q_noises2 + critic_vf.apply(\n",
    "  #   target_params, q_noises2, jnp.zeros((batch_size, 1)), \n",
    "  #   batch['observations'], jnp.ones_like(batch['observations'])))\n",
    "  # q = jnp.concatenate([q1, q2], axis=1)\n",
    "  next_observations = batch['next_observations'][:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  next_actions = jnp.arange(env.nA)[None, :].repeat(batch_size, axis=0).reshape(-1)\n",
    "  # qs = compute_flow_returns(\n",
    "  #   target_params, q_noises, next_observations, next_actions)\n",
    "  qs = critic_vf.apply(\n",
    "    target_params, q_noises, jnp.zeros((batch_size * env.nA, 1)), next_observations, next_actions)  # (batch_size * env.nA, num_rand_samples, 1)\n",
    "  qs = qs.reshape([batch_size, env.nA, num_rand_samples])\n",
    "  qs = jnp.mean(qs, axis=-1)\n",
    "  next_actions = jnp.argmax(qs, axis=-1)\n",
    "  next_actions = jax.lax.stop_gradient(next_actions)\n",
    "  \n",
    "  times = jax.random.uniform(time_rng, (batch_size, 1))\n",
    "  # noises = jax.random.uniform(noise_rng, (batch_size, 1),\n",
    "  #                             minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  noises = jax.random.uniform(noise_rng, (batch_size, num_rand_samples, 1))\n",
    "  # noises = jax.random.normal(noise_rng, (batch_size, 1))\n",
    "  # next_returns = compute_flow_returns(\n",
    "  #   target_params, noises, batch['next_observations'], next_actions)\n",
    "  # noises = jnp.sum(q_noises.reshape([batch_size, env.nA, 1]) * jax.nn.one_hot(next_actions, env.nA)[..., None], axis=1)\n",
    "  next_returns = compute_flow_returns(\n",
    "    target_params, noises, batch['next_observations'], next_actions)  # (batch_size, num_rand_samples, 1)\n",
    "  returns = batch['rewards'][:, None, None] + discount * next_returns\n",
    "  # returns = returns.transpose([0, 2, 1])  # (batch_size, 1, num_rand_samples)\n",
    "  # returns = jnp.clip(\n",
    "  #   returns,\n",
    "  #   min_reward / (1.0 - discount),\n",
    "  #   max_reward / (1.0 - discount),\n",
    "  # )\n",
    "  \n",
    "  rng, noise_rng = jax.random.split(rng)\n",
    "  noises = jax.random.uniform(noise_rng, (batch_size, num_rand_samples, 1))\n",
    "  # noises = jax.random.normal(noise_rng, (batch_size, 1))\n",
    "  noisy_returns = times[:, None] * returns + (1.0 - times[:, None]) * noises\n",
    "  target_vector_field = returns - noises\n",
    "  \n",
    "  vector_field = critic_vf.apply(params, noisy_returns, times, batch['observations'], batch['actions'])\n",
    "  vector_field_loss = jnp.mean((vector_field - target_vector_field) ** 2)\n",
    "  \n",
    "  # rng, noise_rng = jax.random.split(rng)\n",
    "  # noises = jax.random.uniform(noise_rng, (batch_size, num_rand_samples, 1))\n",
    "  # noisy_next_returns = compute_flow_returns(\n",
    "  #   target_params, noises, batch['next_observations'], next_actions, end_times=times)  # (batch_size, num_rand_samples, 1)\n",
    "  # transformed_noisy_returns = batch['rewards'][:, None, None] + discount * noisy_next_returns\n",
    "  # # transformed_noisy_returns = jnp.clip(\n",
    "  # #   transformed_noisy_returns,\n",
    "  # #   min_reward / (1.0 - discount),\n",
    "  # #   max_reward / (1.0 - discount),\n",
    "  # # )\n",
    "\n",
    "  # # transformed_noisy_returns = jnp.roll(transformed_noisy_returns, 1, axis=0)\n",
    "  # vector_field = critic_vf.apply(\n",
    "  #   params, transformed_noisy_returns, times, batch['observations'], batch['actions'])\n",
    "  # target_next_vector_field = critic_vf.apply(\n",
    "  #   target_params, noisy_next_returns, times, batch['next_observations'], next_actions)\n",
    "  # bootstrapped_vector_field_loss = jnp.mean((vector_field - target_next_vector_field) ** 2)\n",
    "  \n",
    "  # loss = vector_field_loss + lam * bootstrapped_vector_field_loss\n",
    "  loss = vector_field_loss\n",
    "  # loss = bootstrapped_vector_field_loss\n",
    "  \n",
    "  # Additional metrics for logging.\n",
    "  # q_noises = jax.random.uniform(q_rng, (batch_size, 1),\n",
    "  #                               minval=min_reward / (1.0 - discount), maxval=max_reward / (1.0 - discount))\n",
    "  q_noises = jax.random.uniform(q_rng, (batch_size, num_rand_samples, 1))\n",
    "  # q_noises = jax.random.normal(q_rng, (batch_size, 1))\n",
    "  # q = compute_flow_returns(\n",
    "  #   params, q_noises, batch['observations'], batch['actions'])\n",
    "  q = critic_vf.apply(\n",
    "    params, q_noises, jnp.zeros((batch_size, 1)), \n",
    "    batch['observations'], batch['actions'])  # (batch_size, num_rand_samples, 1)\n",
    "  q = jnp.mean(q.squeeze(-1), axis=1)\n",
    "\n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'vector_field_loss': vector_field_loss,\n",
    "    # 'bootstrapped_vector_field_loss': bootstrapped_vector_field_loss,\n",
    "    'q_mean': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_vf_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch, rng):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch, rng)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def evaluate_fn(params, rng):\n",
    "  obs = jnp.arange(env.nS)[:, None].repeat(env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.nA)[None, :].repeat(env.nS, axis=0).reshape(-1)\n",
    "\n",
    "  rng, q_rng = jax.random.split(rng)\n",
    "  # q_noises = jax.random.normal(q_rng, (obs.shape[0], 1))\n",
    "  q_noises = jax.random.uniform(q_rng, (obs.shape[0], num_rand_samples, 1))\n",
    "  # q = compute_flow_returns(\n",
    "  #   params, q_noises, obs, actions)\n",
    "  q = critic_vf.apply(params, q_noises, jnp.zeros((obs.shape[0], 1)), obs, actions)  # (batch_size, num_samples, 1)\n",
    "  q = jnp.mean(jnp.squeeze(q), axis=-1)\n",
    "  q = q.reshape(env.nS, env.nA)\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q.reshape(-1),\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q))\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q.reshape(-1))[0, 1]\n",
    "  \n",
    "  # evaluate the policy\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  policy = jax.nn.one_hot(a, env.nA)\n",
    "  policy = np.asarray(policy)\n",
    "  \n",
    "  # evaluation\n",
    "  num_eval_episodes = 100\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=policy[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  key, train_key = jax.random.split(key)\n",
    "  batch = sample_batch(batch_size)\n",
    "  \n",
    "  critic_vf_params, target_critic_vf_params, opt_state, loss, info = update_fn(\n",
    "    critic_vf_params, target_critic_vf_params, opt_state, batch, train_key)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    key, eval_key = jax.random.split(key)\n",
    "    eval_info = evaluate_fn(critic_vf_params, eval_key)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a36aecfc",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "040abf35",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3.2))\n",
    "\n",
    "metric_name = 'eval/q_err_mean'\n",
    "ax = axes[0]\n",
    "for algo, metrics in zip(['Q-learning', 'C51', 'FDQL'], [q_learning_metrics, c51_metrics, fdql_metrics]):\n",
    "    metrcs_arr = np.asarray(metrics[metric_name])\n",
    "    l, = ax.plot(metrcs_arr[:, 0], metrcs_arr[:, 1], label=algo, zorder=3)\n",
    "ax.set_ylabel(r'$| Q - Q^{\\star} |$', fontsize=14)\n",
    "ax.set_xlim([-4e2, 5e4 + 4e2])\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(zorder=3)     \n",
    "\n",
    "metric_name = 'eval/lstsq_q_err_mean'\n",
    "ax = axes[1]\n",
    "for algo, metrics in zip(['Q-learning', 'C51', 'FDQL'], [q_learning_metrics, c51_metrics, fdql_metrics]):\n",
    "    metrcs_arr = np.asarray(metrics[metric_name])\n",
    "    l, = ax.plot(metrcs_arr[:, 0], metrcs_arr[:, 1], label=algo, zorder=3)\n",
    "ax.set_ylabel(r'Least Squared $| Q - Q^{\\star} |$', fontsize=14)\n",
    "ax.set_xlim([-4e2, 5e4 + 4e2])\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "metric_name = 'eval/returns'\n",
    "ax = axes[2]\n",
    "for algo, metrics in zip(['Q-learning', 'C51', 'FDQL'], [q_learning_metrics, c51_metrics, fdql_metrics]):\n",
    "    metrcs_arr = np.asarray(metrics[metric_name])\n",
    "    l, = ax.plot(metrcs_arr[:, 0], metrcs_arr[:, 1], label=algo, zorder=3)\n",
    "ax.set_ylabel(r'returns', fontsize=14)\n",
    "ax.set_xlim([-4e2, 5e4 + 4e2])\n",
    "ax.legend()\n",
    "\n",
    "# ax.set_ylim([-5, 120 + 5])\n",
    "# ax.set_yticks([0, 120])\n",
    "# ax.yaxis.set_minor_locator(MultipleLocator(100))\n",
    "ax.grid(zorder=3)     \n",
    "\n",
    "fig.tight_layout(rect=(-0.026, -0.06, 1.02, 1.04))  # rect = (left, bottom, right, top), default: (0, 0, 1, 1)\n",
    "# filepath = \"/u/cz8792/research/ogbench/plot_scripts/figures/convergence_speed_ablation_lc.pdf\"\n",
    "# fig.savefig(filepath, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b162e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
