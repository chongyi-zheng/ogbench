{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95da631-4003-48f2-ae41-bd762b5a6052",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import gym\n",
    "import tqdm\n",
    "import jax\n",
    "import collections"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6e2844e1-2ac6-4e73-afdd-aef3bc2ec4fb",
   "metadata": {},
   "source": [
    "init_s = 0\n",
    "size = 5\n",
    "walls = np.zeros((size, size))\n",
    "gamma = 0.9\n",
    "\n",
    "def step(s, a):\n",
    "    i, j = np.unravel_index(s, walls.shape)\n",
    "    if a == 0:\n",
    "        j += 1\n",
    "    elif a == 1:\n",
    "        i -= 1\n",
    "    elif a == 2:\n",
    "        j -= 1\n",
    "    elif a == 3:\n",
    "        i += 1\n",
    "    else:\n",
    "        assert a == 4\n",
    "    if i < 0 or j < 0 or i >= size or j >= size or walls[i, j]:\n",
    "        return s\n",
    "    else:\n",
    "        return np.ravel_multi_index((i, j), walls.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ff28172-2a90-4c45-9076-4bc76f754fb9",
   "metadata": {},
   "source": [
    "s = init_s\n",
    "vec = []\n",
    "for _ in tqdm.trange(10_000):\n",
    "    vec.append(s)\n",
    "    s = step(s, np.random.choice(5))\n",
    "vec = np.array(vec)\n",
    "i, j = np.unravel_index(vec, walls.shape)\n",
    "plt.plot(i, j)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34bd882d-5b86-432a-ab77-83ee90b76eb8",
   "metadata": {},
   "source": [
    "# compute the true empirical discounted state occupancy measure\n",
    "d = [[] for s in range(size * size)]\n",
    "sr = jax.nn.one_hot(vec[-1], size*size)\n",
    "for t in tqdm.trange(len(vec)-1, -1, -1):\n",
    "    d[vec[t]] = d[vec[t]] + [sr]\n",
    "    sr = gamma * sr + (1 - gamma) * jax.nn.one_hot(vec[t], size*size)\n",
    "SR = np.array([np.mean(sr_vec, axis=0) for sr_vec in d])\n",
    "plt.imshow(SR)\n",
    "plt.show()\n",
    "\n",
    "counts = collections.Counter(vec)\n",
    "marginals = np.array([counts[s] for s in range(size * size)])\n",
    "marginals = marginals / np.sum(marginals)\n",
    "plt.imshow(marginals.reshape((size, size)))\n",
    "plt.show()\n",
    "\n",
    "log_ratio = jnp.log(SR / marginals[None])\n",
    "plt.imshow(log_ratio)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3a218db6-bce5-4acd-8011-be69e9a3388c",
   "metadata": {},
   "source": [
    "# learn the representations\n",
    "X = jnp.array(vec)\n",
    "\n",
    "# Settings that make sense:\n",
    "# * CS, log_linear=True, repr_activation=False\n",
    "\n",
    "repr_dim = 64\n",
    "batch_size = 256\n",
    "loss_type = 'ce'\n",
    "log_linear = False\n",
    "\n",
    "repr_activation = False\n",
    "if loss_type in ['ce', 'sce'] and not log_linear:\n",
    "    # If we want to use the linear parametrization with the CE or softmax CE loss,\n",
    "    # we apply an elementwise activation to the reps so that the dot product is positive.\n",
    "    # For the LSIF loss, it's fine if the dot product is neg.\n",
    "    repr_activation = True\n",
    "\n",
    "key = jax.random.key(1)\n",
    "key, rng1, rng2 = jax.random.split(key, 3)\n",
    "F = 1e-9 * jax.random.normal(rng1, shape=(size * size, repr_dim))\n",
    "B = 1e-9 * jax.random.normal(rng2, shape=(size * size, repr_dim))\n",
    "params = (F, B)\n",
    "\n",
    "\n",
    "I = jnp.eye(batch_size)\n",
    "optimizer = optax.adam(learning_rate=3e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "def get_data(key):\n",
    "    rng1, rng2 = jax.random.split(key, 2)\n",
    "    t0 = jax.random.choice(rng1, X.shape[0], (batch_size,))\n",
    "    delta = jax.random.geometric(rng2, 1 - gamma, (batch_size,))\n",
    "    t1 = t0 + delta    \n",
    "    return X[t0], X[t1]\n",
    "\n",
    "def get_probs(f, b):\n",
    "    if repr_activation:\n",
    "        f = jax.nn.softplus(f)\n",
    "        b = jax.nn.softplus(b)\n",
    "    probs = jnp.einsum('ik,jk->ij', f, b)\n",
    "    if log_linear:\n",
    "        probs = jnp.exp(probs)\n",
    "    return probs\n",
    "\n",
    "def loss_fn(params, x0, x1):\n",
    "    f = params[0][x0]\n",
    "    b = params[1][x1]\n",
    "    probs = get_probs(f, b)\n",
    "    if loss_type == 'ce': # binary cross entropy\n",
    "        mask = I + (1 - I) / (batch_size - 1)\n",
    "        loss = mask * optax.sigmoid_binary_cross_entropy(logits=jnp.log(probs), labels=I)\n",
    "    elif loss_type == 'sce':  # softmax cross entropy\n",
    "        loss = (optax.softmax_cross_entropy(logits=jnp.log(probs), labels=I) + optax.softmax_cross_entropy(logits=jnp.log(probs.T), labels=I)) / 2.0\n",
    "    elif loss_type == 'lsif':\n",
    "        loss = 0.5 * ((1 - I) * probs**2).sum() / (1 - I).sum() - jnp.diag(probs).sum() / I.sum()\n",
    "        # loss = 0.5 * (probs**2).mean() - jnp.diag(probs).mean()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return loss.sum()\n",
    "grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "@jax.jit\n",
    "def step_fn(params, opt_state, key):\n",
    "    key, rng = jax.random.split(key, 2)\n",
    "    x0, x1 = get_data(rng)\n",
    "    loss, grad = grad_fn(params, x0, x1)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, params, opt_state, key\n",
    "\n",
    "loss_vec = []\n",
    "for _ in tqdm.trange(10_000):\n",
    "    key, rng = jax.random.split(key)\n",
    "    loss, params, opt_state, key = step_fn(params, opt_state, key)\n",
    "    loss_vec.append(loss)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(131)\n",
    "plt.title('combined loss')\n",
    "plt.plot(loss_vec)\n",
    "plt.subplot(132)\n",
    "\n",
    "# compute the error\n",
    "f = params[0]\n",
    "b = params[1]\n",
    "logits = jnp.log(get_probs(f, b))\n",
    "if loss_type == 'sce':\n",
    "    logits = jax.nn.log_softmax(logits, axis=1) + jnp.log(size*size)  # This only works if dataset coverage is uniform\n",
    "\n",
    "plt.plot(logits.flatten(), log_ratio.flatten(), 'o', alpha=0.1)\n",
    "plt.plot([log_ratio.min(), log_ratio.max()],\n",
    "         [log_ratio.min(), log_ratio.max()], 'k--')\n",
    "plt.gca().set_aspect('equal')\n",
    "error = np.nanmean((logits - log_ratio)**2)\n",
    "plt.title('error = %.3f' % error)\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a82962c6-6c2f-4a89-8c85-a3c7a0a8cb0d",
   "metadata": {},
   "source": [
    "# learn the representations\n",
    "X = jnp.array(vec)\n",
    "\n",
    "def get_error(repr_dim, loss_type, log_linear):\n",
    "    repr_activation = False\n",
    "    if loss_type in ['ce', 'sce'] and not log_linear:\n",
    "        # If we want to use the linear parametrization with the CE or softmax CE loss,\n",
    "        # we apply an elementwise activation to the reps so that the dot product is positive.\n",
    "        # For the LSIF loss, it's fine if the dot product is neg.\n",
    "        repr_activation = True\n",
    "\n",
    "    key = jax.random.key(0)\n",
    "    key, rng1, rng2 = jax.random.split(key, 3)\n",
    "    F = 1e-6 * jax.random.normal(rng1, shape=(size * size, repr_dim))\n",
    "    B = 1e-6 * jax.random.normal(rng2, shape=(size * size, repr_dim))\n",
    "    params = (F, B)\n",
    "    \n",
    "    \n",
    "    I = jnp.eye(batch_size)\n",
    "    optimizer = optax.adam(learning_rate=3e-3)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    def get_data(key):\n",
    "        rng1, rng2 = jax.random.split(key, 2)\n",
    "        t0 = jax.random.choice(rng1, X.shape[0], (batch_size,))\n",
    "        delta = jax.random.geometric(rng2, 1 - gamma, (batch_size,))\n",
    "        t1 = t0 + delta    \n",
    "        return X[t0], X[t1]\n",
    "    \n",
    "    def get_probs(f, b):\n",
    "        if repr_activation:\n",
    "            f = jax.nn.softplus(f)\n",
    "            b = jax.nn.softplus(b)\n",
    "        probs = jnp.einsum('ik,jk->ij', f, b)\n",
    "        if log_linear:\n",
    "            probs = jnp.exp(probs)\n",
    "        return probs\n",
    "    \n",
    "    def loss_fn(params, x0, x1):\n",
    "        f = params[0][x0]\n",
    "        b = params[1][x1]\n",
    "        probs = get_probs(f, b)\n",
    "        if loss_type == 'ce': # binary cross entropy\n",
    "            mask = I + (1 - I) / (batch_size - 1)\n",
    "            loss = mask * optax.sigmoid_binary_cross_entropy(logits=jnp.log(probs), labels=I)\n",
    "        elif loss_type == 'sce':  # softmax cross entropy\n",
    "            loss = (optax.softmax_cross_entropy(logits=jnp.log(probs), labels=I) + optax.softmax_cross_entropy(logits=jnp.log(probs.T), labels=I)) / 2.0\n",
    "        elif loss_type == 'lsif':\n",
    "            loss = 0.5 * ((1 - I) * probs**2).sum() / (1 - I).sum() - jnp.diag(probs).sum() / I.sum()\n",
    "            # loss = 0.5 * (probs**2).mean() - jnp.diag(probs).mean()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return loss.sum()\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    \n",
    "    @jax.jit\n",
    "    def step_fn(params, opt_state, key):\n",
    "        key, rng = jax.random.split(key, 2)\n",
    "        x0, x1 = get_data(rng)\n",
    "        loss, grad = grad_fn(params, x0, x1)\n",
    "        updates, opt_state = optimizer.update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return loss, params, opt_state, key\n",
    "    \n",
    "    loss_vec = []\n",
    "    for _ in tqdm.trange(10_000):\n",
    "        key, rng = jax.random.split(key)\n",
    "        loss, params, opt_state, key = step_fn(params, opt_state, key)\n",
    "        loss_vec.append(loss)\n",
    "    \n",
    "    # plt.figure(figsize=(12, 3))\n",
    "    # plt.subplot(131)\n",
    "    # plt.title('combined loss')\n",
    "    # plt.plot(loss_vec)\n",
    "    # plt.subplot(132)\n",
    "    \n",
    "    # # compute the error\n",
    "    f = params[0]\n",
    "    b = params[1]\n",
    "    logits = jnp.log(get_probs(f, b))\n",
    "    if loss_type == 'sce':\n",
    "        logits = jax.nn.log_softmax(logits, axis=1) + jnp.log(size*size)  # This only works if dataset coverage is uniform\n",
    "    error = np.nanmean((logits - log_ratio)**2)\n",
    "    # plt.plot(logits.flatten(), log_ratio.flatten(), 'o', alpha=0.1)\n",
    "    # plt.plot([log_ratio.min(), log_ratio.max()],\n",
    "    #          [log_ratio.min(), log_ratio.max()], 'k--')\n",
    "    # plt.gca().set_aspect('equal')\n",
    "    # plt.title('error = %.3f' % error)\n",
    "    # plt.show()\n",
    "    return error\n",
    "\n",
    "\n",
    "repr_dim = 4\n",
    "batch_size = 256\n",
    "# loss_type = 'sce'\n",
    "# log_linear = False\n",
    "plt.figure(figsize=(6, 3))\n",
    "# repr_dim_vec = [4, 16, 64, 256]\n",
    "x = np.arange(3)\n",
    "loss_type_vec = ['ce', 'sce', 'lsif']\n",
    "width = 0.2\n",
    "for col, log_linear in enumerate([True, False]):\n",
    "    error_vec = []\n",
    "    for loss_type in loss_type_vec:\n",
    "        error = get_error(repr_dim, loss_type, log_linear)\n",
    "        print(repr_dim, loss_type, log_linear, error)\n",
    "        error_vec.append(error)\n",
    "    plt.bar(x - width / 2 if col == 0 else x + width / 2,\n",
    "            error_vec, width=width, label='log_linear=%s' % log_linear)\n",
    "    plt.xticks(x, loss_type_vec)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.xlabel('repr_dim')\n",
    "plt.ylabel('successor repr error')\n",
    "plt.title('repr_dim = %s' % (repr_dim,))\n",
    "plt.savefig('fig.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
