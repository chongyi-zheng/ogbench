{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.envs.toy_text.cliffwalking import (\n",
    "    UP, RIGHT, DOWN, LEFT, POSITION_MAPPING\n",
    ")\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2467df1b",
   "metadata": {},
   "source": [
    "class CliffWalkingEnv(gymnasium.Wrapper):\n",
    "    def __init__(self, random_init_state=False, max_episode_steps=1000, \n",
    "                 render_mode=\"rgb_array\", **kwargs):\n",
    "        env = gymnasium.make(\n",
    "            \"CliffWalking-v1\",\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            render_mode=render_mode,\n",
    "            **kwargs\n",
    "        )\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.nS = self.env.get_wrapper_attr('nS')\n",
    "        self.nA = self.env.get_wrapper_attr('nA')\n",
    "        self.shape = self.env.get_wrapper_attr('shape')\n",
    "\n",
    "        # The original transition probabilities for absorbing states are not correct.\n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            P[s][UP] = self._calculate_transition_prob(position, UP)\n",
    "            P[s][RIGHT] = self._calculate_transition_prob(position, RIGHT)\n",
    "            P[s][DOWN] = self._calculate_transition_prob(position, DOWN)\n",
    "            P[s][LEFT] = self._calculate_transition_prob(position, LEFT)\n",
    "        self.env.set_wrapper_attr('P', P)\n",
    "\n",
    "        if random_init_state:\n",
    "            initial_state_distrib = np.ones(self.nS)\n",
    "            cliff_positions = np.asarray(np.where(env.get_wrapper_attr('_cliff')))\n",
    "            cliff_states = np.ravel_multi_index(cliff_positions, self.shape)\n",
    "            initial_state_distrib[cliff_states] = 0.0\n",
    "            initial_state_distrib[47] = 0.0\n",
    "            initial_state_distrib /= np.sum(initial_state_distrib, keepdims=True)\n",
    "            self.env.set_wrapper_attr('initial_state_distrib', initial_state_distrib)\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        rewards = np.full((self.nS, self.nA, self.nS), np.nan)\n",
    "        transition_probs = np.zeros((self.nS, self.nA, self.nS))\n",
    "        masks = np.zeros((self.nS, self.nA, self.nS))\n",
    "        for state in range(self.nS):\n",
    "            for action in range(self.nA):\n",
    "                _, next_state, reward, terminated = self.env.get_wrapper_attr('P')[state][action][0]\n",
    "                rewards[state, action, next_state] = reward\n",
    "                transition_probs[state, action, next_state] += 1.0\n",
    "                masks[state, action, next_state] = float(not terminated)\n",
    "        transition_probs /= np.sum(transition_probs, axis=-1, keepdims=True)\n",
    "        assert np.all(np.sum(transition_probs, axis=-1) == 1.0)\n",
    "        reward_max, reward_min = np.nanmax(rewards), np.nanmin(rewards)\n",
    "        rewards[np.isnan(rewards)] = reward_min\n",
    "        assert np.all((reward_min <= rewards) & (rewards <= reward_max))\n",
    "\n",
    "        self._orig_reward_min, self._orig_reward_max = reward_min, reward_max\n",
    "        self.orig_rewards = rewards\n",
    "        self.rewards = (rewards - reward_min) / (reward_max - reward_min)\n",
    "        self.transition_probs = transition_probs\n",
    "        self.masks = masks\n",
    "\n",
    "    def _calculate_transition_prob(self, current, move):\n",
    "        \"\"\"Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        \n",
    "        The original transition probabilities for absorbing states are not correct.\n",
    "        \"\"\"\n",
    "        if not self.env.get_wrapper_attr('is_slippery'):\n",
    "            deltas = [POSITION_MAPPING[move]]\n",
    "        else:\n",
    "            deltas = [\n",
    "                POSITION_MAPPING[act] for act in [(move - 1) % 4, move, (move + 1) % 4]\n",
    "            ]\n",
    "        outcomes = []\n",
    "\n",
    "        # the single absorbing state is the goal\n",
    "        goal_position = np.asarray([self.shape[0] - 1, self.shape[1] - 1])\n",
    "        goal_state = np.ravel_multi_index(goal_position, self.shape)\n",
    "        current_position = np.array(current)\n",
    "        current_state = np.ravel_multi_index(tuple(current_position), self.shape)\n",
    "        for delta in deltas:\n",
    "            if current_state == goal_state:\n",
    "                new_state = current_state\n",
    "                reward = 0\n",
    "                is_terminated = True\n",
    "            else:\n",
    "                new_position = current_position + np.array(delta)\n",
    "                new_position = self.env.get_wrapper_attr('_limit_coordinates')(new_position).astype(int)\n",
    "                new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "                if self.env.get_wrapper_attr('_cliff')[tuple(new_position)]:\n",
    "                    reward = -100\n",
    "                    new_state = self.env.get_wrapper_attr('start_state_index')\n",
    "                else:\n",
    "                    reward = -1\n",
    "                is_terminated = (new_state == goal_state)\n",
    "            outcomes.append((1 / len(deltas), new_state, reward, is_terminated))\n",
    "        return outcomes\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        self.env.set_wrapper_attr('start_state_index', obs)\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, orig_reward, terminated, truncated, info = super().step(action)\n",
    "        reward = (orig_reward - self._orig_reward_min) / (self._orig_reward_max - self._orig_reward_min)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6d45da",
   "metadata": {},
   "source": [
    "# collect dataset\n",
    "discount = 0.95\n",
    "max_episode_steps = 100\n",
    "env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "\n",
    "# uniform behavioral policy\n",
    "behavioral_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "dataset = defaultdict(list)\n",
    "\n",
    "num_episodes = 1000\n",
    "num_transitions = 0\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    for _ in range(max_episode_steps):\n",
    "        action = np.random.choice(np.arange(env.nA), p=behavioral_policy[obs])\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        num_transitions += 1\n",
    "        dataset['observations'].append(obs)\n",
    "        dataset['actions'].append(action)\n",
    "        dataset['rewards'].append(reward)\n",
    "        dataset['next_observations'].append(next_obs)\n",
    "        dataset['masks'].append(not terminated)  # for absorbing states\n",
    "        dataset['terminals'].append(truncated)  # for the end of trajectories\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    if k in ['observations', 'actions', 'next_observations']:\n",
    "        dtype = np.int32\n",
    "    elif k == 'terminals':\n",
    "        dtype = bool\n",
    "    else:\n",
    "        dtype = np.float32\n",
    "    dataset[k] = np.array(v, dtype=dtype)\n",
    "\n",
    "print(\"num of total transitions {}\".format(num_transitions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d45e239a",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed29ee7f",
   "metadata": {},
   "source": [
    "(terminal_locs,) = np.nonzero(dataset['terminals'] > 0)\n",
    "assert terminal_locs[-1] == (len(dataset['observations']) - 1)\n",
    "\n",
    "def sample_batch(batch_size, p_curgoal=0.2, p_trajgoal=0.5, relabel_reward=False):\n",
    "    dataset_size = len(dataset['observations'])\n",
    "    idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    batch = jax.tree_util.tree_map(lambda arr: arr[idxs], dataset)\n",
    "    \n",
    "    final_state_idxs = terminal_locs[np.searchsorted(terminal_locs, idxs)]\n",
    "    \n",
    "    offsets = np.random.geometric(p=1 - discount, size=batch_size)  # in [1, inf)\n",
    "    traj_goal_idxs = np.minimum(idxs + offsets, final_state_idxs)\n",
    "    random_goal_idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    goal_idxs = np.where(\n",
    "        np.random.rand(batch_size) < p_trajgoal / (1.0 - p_curgoal), traj_goal_idxs, random_goal_idxs\n",
    "    )\n",
    "    goal_idxs = np.where(np.random.rand(batch_size) < p_curgoal, idxs, goal_idxs)\n",
    "    \n",
    "    batch['goals'] = jax.tree_util.tree_map(lambda arr: arr[goal_idxs], dataset['observations'])\n",
    "    if relabel_reward:\n",
    "        successes = np.logical_or((idxs + 1) == goal_idxs, idxs == goal_idxs).astype(float)\n",
    "        batch['masks'] = 1.0 - successes\n",
    "        batch['rewards'] = successes - 1.0  # 0 for goal and -1 for other states\n",
    "    \n",
    "    return batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d483e47",
   "metadata": {},
   "source": [
    "def plot_metrics(metrics, logyscale_stats=[], title=''):\n",
    "  # learning curves\n",
    "  nrows = np.ceil(len(metrics) / 4).astype(int)\n",
    "  ncols = 4\n",
    "  f, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "  if nrows == 1:\n",
    "    axes = np.array([axes])\n",
    "  f.set_figheight(3 * nrows)\n",
    "  f.set_figwidth(3 * ncols)\n",
    "\n",
    "  for idx, (name, val) in enumerate(metrics.items()):\n",
    "    v = np.array(val)\n",
    "    if len(v) == 0:\n",
    "      continue\n",
    "\n",
    "    x, y = v[:, 0], v[:, 1]\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "\n",
    "    if 'train' in name:\n",
    "      y = gaussian_filter1d(y, 100)\n",
    "    ax.plot(x, y)\n",
    "    if name in logyscale_stats:\n",
    "      ax.set_yscale('log')\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "  f.suptitle(title)\n",
    "\n",
    "  return f"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d70eeef0",
   "metadata": {},
   "source": [
    "### CRL + BNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4137f320",
   "metadata": {},
   "source": [
    "goal_marg = np.zeros(env.nS)\n",
    "for state in range(env.nS):\n",
    "    goal_marg[state] = np.sum(dataset['next_observations'] == state) / len(dataset['next_observations'])\n",
    "goal_marg = jnp.asarray(goal_marg)\n",
    "print(goal_marg)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7b7b51",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  repr_dim: int = 512\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action, future_obs):\n",
    "    obs = jax.nn.one_hot(obs, env.nS)\n",
    "    action = jax.nn.one_hot(action, env.nA)\n",
    "    future_obs = jax.nn.one_hot(future_obs, env.nS)\n",
    "    phi_inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "    psi_inputs = future_obs\n",
    "\n",
    "    phi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(phi_inputs)\n",
    "    psi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(psi_inputs)\n",
    "    \n",
    "    logits = jnp.einsum('ik,jk->ij', phi, psi)\n",
    "    logits = logits / jnp.sqrt(self.repr_dim)\n",
    "    \n",
    "    return logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150edf4a",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2, p_curgoal=0.0, p_trajgoal=1.0)\n",
    "critic = Critic(repr_dim=32)\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'], example_batch['next_observations'])\n",
    "\n",
    "def loss_fn(params, batch):\n",
    "  pos_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['goals'])\n",
    "  neg_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['next_observations'])\n",
    "  \n",
    "  I = jnp.eye(batch_size)\n",
    "  logits = I * pos_logits + (1 - I) * neg_logits\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=I).mean()\n",
    "  \n",
    "  plus_logits = critic.apply(params, batch['observations'], batch['actions'], \n",
    "                             jnp.full_like(batch['observations'], (env.nS - 2)))\n",
    "  plus_probs = jnp.exp(jnp.diag(plus_logits)) * goal_marg[env.nS - 2]\n",
    "  q = (1 + discount) * plus_probs\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  return params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=200, max_episode_steps=100):\n",
    "  eval_env = CliffWalkingEnv(random_init_state=True, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(eval_env.nS)[:, None].repeat(eval_env.nA, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(eval_env.nA)[None, :].repeat(eval_env.nS, axis=0).reshape(-1)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             jnp.full_like(obs, (eval_env.nS - 1)))\n",
    "  plus_logits = jnp.diag(plus_logits)\n",
    "  plus_logits = plus_logits.reshape([eval_env.nS, eval_env.nA])\n",
    "  a = jnp.argmax(plus_logits, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, eval_env.nA)\n",
    "  pi = np.asarray(pi)\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs, _ = eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(eval_env.nA), p=pi[obs])\n",
    "      next_obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      traj_dataset['next_observations'].append(next_obs)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(47 in traj_dataset['next_observations'])\n",
    "  sr = np.mean(successes)\n",
    "  \n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  sr = compute_success_rate(params)\n",
    "  \n",
    "  info = {\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_batch(batch_size, p_curgoal=0.0, p_trajgoal=1.0)\n",
    "  critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d2b5da",
   "metadata": {},
   "source": [
    "crl_metrics = metrics\n",
    "print(crl_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
