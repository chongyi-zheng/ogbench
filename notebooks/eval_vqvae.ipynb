{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ab8891",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "from absl import flags\n",
    "from ml_collections import config_flags\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from agents import agents\n",
    "from utils.env_utils import make_env_and_datasets\n",
    "from utils.flax_utils import restore_agent"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069bbc18",
   "metadata": {},
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('env_name', 'google_robot_pick_coke_can', 'Environment (dataset) name.')\n",
    "flags.DEFINE_integer('seed', 0, 'Random seed.')\n",
    "flags.DEFINE_float('p_aug', 0.5, 'Probability of applying image augmentation.')\n",
    "flags.DEFINE_integer('num_aug', 1, 'Number of image augmentations.')\n",
    "flags.DEFINE_integer('inplace_aug', 1, 'Whether to replace the original image after applying augmentations.')\n",
    "flags.DEFINE_integer('frame_stack', 3, 'Number of frames to stack.')\n",
    "config_flags.DEFINE_config_file('agent', '../impls/agents/vqvae.py', lock_config=False)\n",
    "\n",
    "if not FLAGS.is_parsed():\n",
    "    FLAGS(sys.argv, known_only=True)\n",
    "\n",
    "config = FLAGS.agent\n",
    "config['encoder'] = 'resnet_34'\n",
    "config['decoder'] = 'resnet_34'\n",
    "_, _, train_dataset, val_dataset = make_env_and_datasets(\n",
    "    FLAGS.env_name, frame_stack=FLAGS.frame_stack, max_size=10_000_000, action_clip_eps=None)\n",
    "\n",
    "# Initialize agent.\n",
    "random.seed(FLAGS.seed)\n",
    "np.random.seed(FLAGS.seed)\n",
    "tf.random.set_seed(FLAGS.seed)\n",
    "\n",
    "# Set up datasets.\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .shuffle(20_000)\n",
    "    .repeat()\n",
    "    .batch(config['batch_size'])\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "train_dataset_iter = train_dataset.as_numpy_iterator()\n",
    "val_dataset = (\n",
    "    val_dataset\n",
    "    .shuffle(2_000)\n",
    "    .repeat()\n",
    "    .batch(config['batch_size'])\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset_iter = val_dataset.as_numpy_iterator()\n",
    "\n",
    "example_batch = next(val_dataset_iter)\n",
    "\n",
    "agent_class = agents[config['agent_name']]\n",
    "agent = agent_class.create(\n",
    "    FLAGS.seed,\n",
    "    example_batch['observations'],\n",
    "    example_batch['actions'],\n",
    "    config,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874c067f",
   "metadata": {},
   "source": [
    "restore_path = '/home/cz8792/gpfs/exp_logs/ogbench_logs/vqvae/20250727_vqvae_google_robot_pick_coke_can/10/debug/sd010_s_66319823.0.20250727_165712'\n",
    "restore_epoch = 300000\n",
    "\n",
    "agent_class = agents[config['agent_name']]\n",
    "agent = agent_class.create(\n",
    "    FLAGS.seed,\n",
    "    example_batch['observations'],\n",
    "    example_batch['actions'],\n",
    "    config,\n",
    ")\n",
    "agent = restore_agent(agent, restore_path, restore_epoch)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2d373",
   "metadata": {},
   "source": [
    "num_recon_images = 16\n",
    "num_cols = 8\n",
    "\n",
    "val_batch = next(val_dataset_iter)\n",
    "images = val_batch['observations']\n",
    "\n",
    "recon_images = agent.reconstruct(images)  # use the original image as inputs\n",
    "images = images.astype(jnp.float32) / 127.5 - 1.0  # put inputs in [-1, 1]\n",
    "images = images.reshape([*images.shape[:-1], -1, 3])\n",
    "recon_images = recon_images.reshape([*recon_images.shape[:-1], -1, 3])\n",
    "\n",
    "images = jnp.clip((images + 1.0) / 2.0, 0.0, 1.0)\n",
    "recon_images = jnp.clip((recon_images + 1.0) / 2.0, 0.0, 1.0)\n",
    "\n",
    "# plot comparison with matplotlib. put each reconstruction side by side.\n",
    "fig, axes = plt.subplots((num_recon_images // num_cols) * 2, num_cols, figsize=(12, 6))\n",
    "for row in range(num_recon_images // num_cols):\n",
    "    for col in range(num_cols):\n",
    "        axes[2 * row, col].imshow(images[row * num_cols + col, :, :, 0], vmin=0.0, vmax=1.0)\n",
    "        axes[2 * row + 1, col].imshow(recon_images[row * num_cols + col, :, :, 0], vmin=0.0, vmax=1.0)\n",
    "\n",
    "        axes[2 * row, col].axis('off')\n",
    "        axes[2 * row + 1, col].axis('off')\n",
    "\n",
    "fig.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469ab80",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
