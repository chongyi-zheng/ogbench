{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from absl import flags\n",
    "from ml_collections import config_flags\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "\n",
    "import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from agents import agents\n",
    "from utils.env_utils import make_env_and_datasets\n",
    "from utils.flax_utils import restore_agent\n",
    "from utils.evaluation import supply_rng, add_to, flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e843796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    }
   ],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('env_name', 'antmaze-large-navigate-v0', 'Environment (dataset) name.')\n",
    "flags.DEFINE_integer('seed', 0, 'Random seed.')\n",
    "config_flags.DEFINE_config_file('agent', '../impls/agents/psiql.py', lock_config=False)\n",
    "\n",
    "if not FLAGS.is_parsed():\n",
    "    FLAGS(sys.argv, known_only=True)\n",
    "\n",
    "config = FLAGS.agent\n",
    "env, train_dataset, val_dataset = make_env_and_datasets(FLAGS.env_name, frame_stack=config['frame_stack'])\n",
    "\n",
    "# Initialize agent.\n",
    "random.seed(FLAGS.seed)\n",
    "np.random.seed(FLAGS.seed)\n",
    "\n",
    "example_batch = train_dataset.sample(1)\n",
    "\n",
    "agent_class = agents[config['agent_name']]\n",
    "agent = agent_class.create(\n",
    "    FLAGS.seed,\n",
    "    example_batch['observations'],\n",
    "    example_batch['actions'],\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb731f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /n/fs/rl-chongyiz/exp_logs/hdualrl_logs/psiql/20250618_psiql_antmaze-large-navigate-v0_expectile=0.7_alpha=3.0_discount=0.99/1/debug/sd001_s_24251254.0.20250618_134110/params_1000000.pkl\n"
     ]
    }
   ],
   "source": [
    "restore_dir = \"/n/fs/rl-chongyiz/exp_logs/hdualrl_logs/psiql/20250618_psiql_antmaze-large-navigate-v0_expectile=0.7_alpha=3.0_discount=0.99/1/debug/sd001_s_24251254.0.20250618_134110\"\n",
    "restore_epoch = 1_000_000\n",
    "\n",
    "agent = restore_agent(agent, restore_dir, restore_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7405b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n",
      "100%|██████████| 5/5 [01:27<00:00, 17.47s/it]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def sample_actions(\n",
    "    observations,\n",
    "    agent=agent,\n",
    "    goals=None,\n",
    "    seed=None,\n",
    "    temperature=1.0,\n",
    "):\n",
    "    \"\"\"Sample actions from the actor.\n",
    "\n",
    "    It first queries the high-level actor to obtain subgoal representations, and then queries the low-level actor\n",
    "    to obtain raw actions.\n",
    "    \"\"\"\n",
    "    # seed, waypoint_seed, action_seed = jax.random.split(seed, 3)\n",
    "\n",
    "    # TODO: posterior sampling\n",
    "    # candidates = jnp.concatenate([\n",
    "    #     jnp.expand_dims(observations, 0),\n",
    "    #     candidates,\n",
    "    #     jnp.expand_dims(goals, 0)\n",
    "    # ], axis=0)\n",
    "\n",
    "    # n_observations = jnp.repeat(jnp.expand_dims(observations, 0), candidates.shape[0], axis=0)\n",
    "    # n_goals = jnp.repeat(jnp.expand_dims(observations, 0), candidates.shape[0], axis=0)\n",
    "\n",
    "    # v_sw1, v_sw2 = agent.network.select('value')(n_observations, candidates)\n",
    "    # v_sw = (v_sw1 + v_sw2) / 2\n",
    "\n",
    "    # v_wg1, v_wg2 = agent.network.select('value')(candidates, n_goals)\n",
    "    # v_wg = (v_wg1 + v_wg2) / 2\n",
    "\n",
    "    # v_sg1, v_sg2 = agent.network.select('value')(n_observations, n_goals)\n",
    "    # v_sg = (v_sg1 + v_sg2) / 2\n",
    "\n",
    "    # logits = v_sw + v_wg - v_sg\n",
    "    # waypoint_idx = jax.random.categorical(waypoint_seed, logits)\n",
    "    # waypoint = candidates[waypoint_idx]\n",
    "\n",
    "    dist = agent.network.select('actor')(observations, goals, temperature=temperature)\n",
    "    actions = dist.sample(seed=seed)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "num_eval_episodes = 10\n",
    "eval_temperature = 1.0\n",
    "task_infos = env.unwrapped.task_infos if hasattr(env.unwrapped, 'task_infos') else env.task_infos\n",
    "num_tasks = len(task_infos)\n",
    "\n",
    "task_stats = defaultdict()\n",
    "for task_id in tqdm.trange(1, num_tasks + 1):\n",
    "    actor_fn = supply_rng(sample_actions, rng=jax.random.PRNGKey(np.random.randint(0, 2**32)))\n",
    "    trajs = []\n",
    "    stats = defaultdict(list)\n",
    "\n",
    "    renders = []\n",
    "    for i in tqdm.trange(num_eval_episodes):\n",
    "        traj = defaultdict(list)\n",
    "\n",
    "        observation, info = env.reset(options=dict(task_id=task_id))\n",
    "        goal = info.get('goal')\n",
    "        goal_frame = info.get('goal_rendered')\n",
    "        done = False\n",
    "        step = 0\n",
    "        render = []\n",
    "        while not done:\n",
    "            action = actor_fn(observations=observation, goals=goal, temperature=eval_temperature)\n",
    "            action = np.array(action)\n",
    "            action = np.clip(action, -1, 1)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "\n",
    "            transition = dict(\n",
    "                observation=observation,\n",
    "                next_observation=next_observation,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "                info=info,\n",
    "            )\n",
    "            add_to(traj, transition)\n",
    "            observation = next_observation\n",
    "        add_to(stats, flatten(info))\n",
    "        trajs.append(traj)\n",
    "\n",
    "    for k, v in stats.items():\n",
    "        stats[k] = np.mean(v)\n",
    "\n",
    "    for k, v in stats.items():\n",
    "        task_stats['task{}/'.format(task_id) + k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed32dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'task1/xy': 6.951728410839584, 'task1/prev_qpos': 0.9962415901716694, 'task1/prev_qvel': 0.03219964167625454, 'task1/qpos': 1.0012937151198646, 'task1/qvel': 0.15622723963146778, 'task1/success': 0.0, 'task2/xy': 13.918956919054352, 'task2/prev_qpos': 1.9387436680231431, 'task2/prev_qvel': 0.02402689661112348, 'task2/qpos': 1.9367531003961342, 'task2/qvel': -0.03792620451454055, 'task2/success': 0.0, 'task3/xy': 18.707416452640224, 'task3/prev_qpos': 2.5601216861492175, 'task3/prev_qvel': -0.05887238485842694, 'task3/qpos': 2.553674647545911, 'task3/qvel': -0.07394577234188338, 'task3/success': 0.0, 'task4/xy': 18.88307265518422, 'task4/prev_qpos': 2.561894354156127, 'task4/prev_qvel': -0.21806561071730365, 'task4/qpos': 2.5476660818221615, 'task4/qvel': -0.07427161360334389, 'task4/success': 0.0, 'task5/xy': 8.665013821602543, 'task5/prev_qpos': 1.213787660182286, 'task5/prev_qvel': 0.044064202611714766, 'task5/qpos': 1.2160577203422989, 'task5/qvel': 0.06447727329941773, 'task5/success': 0.0})\n"
     ]
    }
   ],
   "source": [
    "print(task_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f2b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
