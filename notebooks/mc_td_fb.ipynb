{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95da631-4003-48f2-ae41-bd762b5a6052",
   "metadata": {},
   "source": [
    "import collections\n",
    "import tqdm\n",
    "import copy\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import jax"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2844e1-2ac6-4e73-afdd-aef3bc2ec4fb",
   "metadata": {},
   "source": [
    "size = 5\n",
    "walls = np.zeros((size, size))\n",
    "gamma = 0.9\n",
    "\n",
    "num_observations = size * size\n",
    "num_actions = 5\n",
    "\n",
    "def step(s, a):\n",
    "    i, j = np.unravel_index(s, walls.shape)\n",
    "    if a == 0:\n",
    "        j += 1\n",
    "    elif a == 1:\n",
    "        i -= 1\n",
    "    elif a == 2:\n",
    "        j -= 1\n",
    "    elif a == 3:\n",
    "        i += 1\n",
    "    else:\n",
    "        assert a == 4\n",
    "    if i < 0 or j < 0 or i >= size or j >= size or walls[i, j]:\n",
    "        return s\n",
    "    else:\n",
    "        return np.ravel_multi_index((i, j), walls.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff28172-2a90-4c45-9076-4bc76f754fb9",
   "metadata": {},
   "source": [
    "dataset_size = 100_000\n",
    "num_trajectories = 100\n",
    "max_episode_lengths = 1000 + 1\n",
    "\n",
    "assert num_trajectories * (max_episode_lengths - 1) == dataset_size\n",
    "\n",
    "traj_dataset = collections.defaultdict(list)\n",
    "for traj_idx in range(num_trajectories):\n",
    "    # random sample initial observations\n",
    "    obs = np.random.randint(num_observations)\n",
    "    for t in range(max_episode_lengths):\n",
    "        action = np.random.choice(num_actions)\n",
    "\n",
    "        next_obs = step(obs, action)\n",
    "        \n",
    "        traj_dataset['observations'].append(obs)\n",
    "        traj_dataset['actions'].append(action)\n",
    "        traj_dataset['terminals'].append((t == max_episode_lengths - 1))\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "for k, v in traj_dataset.items():\n",
    "    if k in ['observations', 'actions']:\n",
    "        traj_dataset[k] = np.array(v, dtype=np.int32)\n",
    "    # elif k in ['terminals']:\n",
    "    #     traj_dataset[k] = np.array(v, dtype=bool)\n",
    "    else:\n",
    "        traj_dataset[k] = np.array(v, dtype=np.float32)\n",
    "\n",
    "masks = (traj_dataset['terminals'] == 0)\n",
    "# next_masks = (np.concatenate([[1.0], traj_dataset['terminals'][:-1]]) == 0)\n",
    "next_masks = np.concatenate([[False], masks[:-1]])\n",
    "new_terminals = np.concatenate([traj_dataset['terminals'][1:], [1.0]])\n",
    "\n",
    "dataset = dict(\n",
    "    observations=traj_dataset['observations'][masks],\n",
    "    actions=traj_dataset['actions'][masks],\n",
    "    next_observations=traj_dataset['observations'][next_masks],\n",
    "    next_actions=traj_dataset['actions'][next_masks],\n",
    "    terminals=new_terminals[masks],\n",
    ")\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    assert len(v) == dataset_size"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bd882d-5b86-432a-ab77-83ee90b76eb8",
   "metadata": {},
   "source": [
    "# (chongyi): this is not correct now\n",
    "# compute the true empirical discounted state occupancy measure\n",
    "# d = [[] for _ in range(num_observations)]\n",
    "# sr = jax.nn.one_hot(dataset['observations'][-1], size * size)\n",
    "# for t in tqdm.trange(dataset_size - 1, -1, -1):\n",
    "#     d[dataset['observations'][t]] = d[dataset['observations'][t]] + [sr]\n",
    "#     sr = gamma * sr + (1 - gamma) * jax.nn.one_hot(dataset['observations'][t], size*size)\n",
    "# SR = np.array([np.mean(sr_vec, axis=0) for sr_vec in d])\n",
    "# plt.imshow(SR)\n",
    "# plt.show()\n",
    "\n",
    "# compute the true discounted state occupancy measure\n",
    "beta = np.ones([num_observations, num_actions])\n",
    "beta /= num_actions\n",
    "transition_probs_sa = np.zeros([num_observations, num_actions, num_observations])\n",
    "for obs in range(num_observations):\n",
    "    for action in range(num_actions):\n",
    "        next_obs = step(obs, action)\n",
    "        transition_probs_sa[obs, action, next_obs] += 1\n",
    "transition_probs_sa = transition_probs_sa / np.sum(transition_probs_sa, axis=-1, keepdims=True)\n",
    "transition_probs_s = np.sum(transition_probs_sa * beta[..., None], axis=1)\n",
    "transition_probs_sa = jnp.array(transition_probs_sa)\n",
    "transition_probs_s = jnp.array(transition_probs_s)\n",
    "\n",
    "sr_sa = jnp.zeros([num_observations, num_actions, num_observations])\n",
    "sr_s = jnp.zeros([num_observations, num_observations])\n",
    "for _ in tqdm.trange(1000):\n",
    "    sr_s = (1 - gamma) * jnp.eye(num_observations) + gamma * jnp.einsum('ij,jk->ik', transition_probs_s, sr_s)\n",
    "    sr_sa = (1 - gamma) * jnp.eye(num_observations)[:, None] + gamma * jnp.einsum(\n",
    "        'ijk,kl->ijl',\n",
    "        transition_probs_sa,\n",
    "        jnp.sum(beta[..., None] * sr_sa, axis=1)\n",
    "    )\n",
    "\n",
    "counts = collections.Counter(dataset['observations'])\n",
    "marginal_s_beta = np.array([counts[obs] for obs in range(num_observations)])\n",
    "marginal_s_beta = marginal_s_beta / np.sum(marginal_s_beta)\n",
    "marginal_s_beta = jnp.array(marginal_s_beta)\n",
    "plt.imshow(marginal_s_beta.reshape((size, size)))\n",
    "plt.show()\n",
    "\n",
    "log_ratios = jnp.log(sr_s / marginal_s_beta[None])\n",
    "plt.imshow(log_ratios)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5e79d27",
   "metadata": {},
   "source": [
    "### Compute SR ratio prediction error for different methods and parameterizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c36e787",
   "metadata": {},
   "source": [
    "def plot_metrics(metrics, f_axes=None, logyscale_stats=[], title='', label=None):\n",
    "  # learning curves\n",
    "  if f_axes is None:\n",
    "    nrows = np.ceil(len(metrics) / 4).astype(int)\n",
    "    ncols = 4\n",
    "    f, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    if nrows == 1:\n",
    "        axes = np.array([axes])\n",
    "    f.set_figheight(3 * nrows)\n",
    "    f.set_figwidth(3 * ncols)\n",
    "  else:\n",
    "    f, axes = f_axes\n",
    "\n",
    "  for idx, (name, val) in enumerate(metrics.items()):\n",
    "    v = np.array(val)\n",
    "    if len(v) == 0:\n",
    "      continue\n",
    "\n",
    "    x, y = v[:, 0], v[:, 1]\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "\n",
    "    if 'train' in name:\n",
    "      y = gaussian_filter1d(y, 100)\n",
    "    ax.plot(x, y, label=label)\n",
    "    if name in logyscale_stats:\n",
    "      ax.set_yscale('log')\n",
    "    ax.set_title(name)\n",
    "    \n",
    "    # ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "  f.suptitle(title)\n",
    "\n",
    "  return f, axes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82962c6-6c2f-4a89-8c85-a3c7a0a8cb0d",
   "metadata": {},
   "source": [
    "# learn the representations\n",
    "repr_dim = 4\n",
    "batch_size = 256\n",
    "\n",
    "(terminal_locs,) = np.nonzero(dataset['terminals'] > 0)\n",
    "\n",
    "def get_data(batch_size):\n",
    "    idxs = np.random.randint(dataset_size, size=(batch_size,))\n",
    "    rand_idxs = np.random.randint(dataset_size, size=(batch_size,))\n",
    "    \n",
    "    offsets = np.random.geometric(p=1 - gamma, size=(batch_size,)) - 1  # in [0, inf)\n",
    "    final_state_idxs = terminal_locs[np.searchsorted(terminal_locs, idxs)]\n",
    "    future_idxs = np.minimum(idxs + offsets, final_state_idxs)\n",
    "    \n",
    "    batch = dict(\n",
    "        observations=dataset['observations'][idxs],\n",
    "        actions=dataset['actions'][idxs], \n",
    "        next_observations=dataset['next_observations'][idxs],\n",
    "        next_actions=dataset['next_actions'][idxs],\n",
    "        future_observations=dataset['observations'][future_idxs],\n",
    "        random_observations=dataset['observations'][rand_idxs],\n",
    "    )\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_prob_ratios(f, b, repr_activation=False, log_linear=False):\n",
    "    if repr_activation:\n",
    "        f = jax.nn.softplus(f)\n",
    "        b = jax.nn.softplus(b)\n",
    "    # f = f / jnp.linalg.norm(f, axis=-1, keepdims=True) * jnp.sqrt(f.shape[-1])\n",
    "    # b = b / jnp.linalg.norm(b, axis=-1, keepdims=True) * jnp.sqrt(b.shape[-1])\n",
    "    prob_ratios = jnp.einsum('ik,jk->ij', f, b)\n",
    "    if log_linear:\n",
    "        prob_ratios = jnp.exp(prob_ratios)\n",
    "    return prob_ratios\n",
    "\n",
    "def evaluate_fn(params, batch, repr_activation=False, log_linear=False):\n",
    "    # compute the error\n",
    "    f = params[0]\n",
    "    b = params[1]\n",
    "    random_b = params[1][batch['random_observations']]\n",
    "    log_ratio_preds = jnp.log(get_prob_ratios(f, b, repr_activation, log_linear))\n",
    "    if 'sce' in loss_type:\n",
    "        # log_ratio_preds = jax.nn.log_softmax(log_ratio_preds, axis=1) + jnp.log(num_observations)  # This only works if dataset coverage is uniform\n",
    "        \n",
    "        random_log_ratios = jnp.log(get_prob_ratios(f, random_b, repr_activation, log_linear))\n",
    "        log_ratio_preds = log_ratio_preds - jax.nn.logsumexp(random_log_ratios, axis=-1)[:, None] + jnp.log(batch_size)\n",
    "    error = np.nanmean((log_ratio_preds - log_ratios) ** 2)\n",
    "    \n",
    "    info = dict(\n",
    "        error=error,\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "\n",
    "def train_and_eval(repr_dim, loss_type, log_linear, num_training_steps=10_000, eval_interval=1_000):\n",
    "    repr_activation = False\n",
    "    if loss_type in ['mc_ce', 'mc_sce', 'td_ce', 'td_sce'] and not log_linear:\n",
    "        # If we want to use the linear parametrization with the CE or softmax CE loss,\n",
    "        # we apply an elementwise activation to the reps so that the dot product is positive.\n",
    "        # For the LSIF loss, it's fine if the dot product is neg.\n",
    "        repr_activation = True\n",
    "\n",
    "    rng = jax.random.key(np.random.randint(2 ** 32))\n",
    "    rng, f_rng, b_rng = jax.random.split(rng, 3)\n",
    "    F = 1e-6 * jax.random.normal(f_rng, shape=(num_observations, repr_dim))\n",
    "    B = 1e-6 * jax.random.normal(b_rng, shape=(num_observations, repr_dim))\n",
    "    params = (F, B)\n",
    "    \n",
    "    optimizer = optax.adam(learning_rate=3e-3)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, batch, rng):\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "        future_obs = batch['future_observations']\n",
    "        random_obs = batch['random_observations']\n",
    "        \n",
    "        I = jnp.eye(batch_size)\n",
    "        f = params[0][obs]\n",
    "        next_f = params[0][next_obs]\n",
    "        current_b = params[1][obs]\n",
    "        future_b = params[1][future_obs]\n",
    "        random_b = params[1][random_obs]\n",
    "        \n",
    "        current_prob_ratios = get_prob_ratios(f, current_b, repr_activation, log_linear)\n",
    "        future_prob_ratios = get_prob_ratios(f, future_b, repr_activation, log_linear)\n",
    "        random_prob_ratios = get_prob_ratios(f, random_b, repr_activation, log_linear)\n",
    "        \n",
    "        if loss_type in ['mc_ce', 'mc_sce', 'mc_lsif']:\n",
    "            prob_ratios = I * future_prob_ratios + (1 - I) * random_prob_ratios\n",
    "            if loss_type == 'mc_ce': # binary cross entropy\n",
    "                mask = I + (1 - I) / (batch_size - 1)\n",
    "                loss = mask * optax.sigmoid_binary_cross_entropy(logits=jnp.log(prob_ratios), labels=I)\n",
    "            elif loss_type == 'mc_sce':  # softmax cross entropy\n",
    "                loss = (optax.softmax_cross_entropy(logits=jnp.log(prob_ratios), labels=I) + optax.softmax_cross_entropy(logits=jnp.log(prob_ratios.T), labels=I)) / 2.0\n",
    "            elif loss_type == 'mc_lsif': # least square importance filtering\n",
    "                loss = 0.5 * ((1 - I) * prob_ratios**2).sum() / (1 - I).sum() - (I * prob_ratios).sum() / I.sum()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        elif loss_type in ['td_ce', 'td_sce', 'td_lsif']:\n",
    "            if loss_type == 'td_ce':\n",
    "                prob_ratios = I * current_prob_ratios + (1 - I) * random_prob_ratios\n",
    "                \n",
    "                weight_log_prob_ratios = jnp.log(get_prob_ratios(next_f, random_b, repr_activation, log_linear))\n",
    "                w = jnp.exp(weight_log_prob_ratios)\n",
    "                w = jax.lax.stop_gradient(w)\n",
    "                \n",
    "                mask1 = (1 - gamma) * I + (1 - I) / (batch_size - 1)\n",
    "                mask2 = gamma * (1 - I) * w / (batch_size - 1)\n",
    "                loss = mask1 * optax.sigmoid_binary_cross_entropy(logits=jnp.log(prob_ratios), labels=I) + mask2 * optax.sigmoid_binary_cross_entropy(logits=jnp.log(random_prob_ratios), labels=(1 - I))\n",
    "            elif loss_type == 'td_sce':\n",
    "                prob_ratios = I * current_prob_ratios + (1 - I) * random_prob_ratios\n",
    "                \n",
    "                weight_log_prob_ratios = jnp.log(get_prob_ratios(next_f, random_b, repr_activation, log_linear))\n",
    "                w = jax.nn.softmax(weight_log_prob_ratios, axis=-1)\n",
    "                w = jax.lax.stop_gradient(w)\n",
    "                \n",
    "                loss = (1 - gamma) * optax.softmax_cross_entropy(logits=jnp.log(prob_ratios), labels=I) + gamma * optax.softmax_cross_entropy(logits=jnp.log(random_prob_ratios), labels=w)\n",
    "            elif loss_type == 'td_lsif':\n",
    "                prob_ratios = I * current_prob_ratios + (1 - I) * random_prob_ratios\n",
    "                \n",
    "                w = get_prob_ratios(next_f, random_b, repr_activation, log_linear)\n",
    "                w = jax.lax.stop_gradient(w)\n",
    "                \n",
    "                loss = 0.5 * ((1 - I) * prob_ratios ** 2).sum() / (1 - I).sum() - (1 - gamma) * (I * prob_ratios).sum() / I.sum() - gamma * ((1 - I) * w * prob_ratios).sum() / (1 - I).sum()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        info = dict(\n",
    "            loss=loss,\n",
    "            pos_prob_ratios=jnp.mean(jnp.diag(prob_ratios)),\n",
    "            neg_prob_ratios=jnp.sum(prob_ratios * (1 - I)) / jnp.sum(1 - I),\n",
    "        )\n",
    "        \n",
    "        return loss, info\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    \n",
    "    @jax.jit\n",
    "    def update_fn(params, opt_state, batch, rng):\n",
    "        # key, rng = jax.random.split(key, 2)\n",
    "        rng, loss_rng = jax.random.split(rng, 2)\n",
    "        # batch = get_data(batch_size)\n",
    "        \n",
    "        (_, info), grad = grad_fn(params, batch, loss_rng)\n",
    "        updates, opt_state = optimizer.update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        return params, opt_state, rng, info\n",
    "    \n",
    "    metrics = collections.defaultdict(list)\n",
    "    for step in tqdm.trange(num_training_steps):\n",
    "        rng, update_rng = jax.random.split(rng)\n",
    "        batch = get_data(batch_size)\n",
    "        params, opt_state, rng, info = update_fn(params, opt_state, batch, update_rng)\n",
    "        \n",
    "        for k, v in info.items():\n",
    "            metrics['train/' + k].append(\n",
    "                np.array([step, v])\n",
    "            )\n",
    "        \n",
    "        if step == 1 or step % eval_interval == 0:\n",
    "            eval_info = evaluate_fn(params, batch, repr_activation, log_linear)\n",
    "            for k, v in eval_info.items():\n",
    "                metrics['eval/' + k].append(\n",
    "                    np.array([step, v])\n",
    "                )\n",
    "\n",
    "        # if step == 1 or step % plot_interval == 0:\n",
    "        #     label = loss_type + ' log_linear={}'.format(log_linear)\n",
    "        #     f = plot_metrics(metrics, logyscale_stats=[], label=label)\n",
    "        #     display.clear_output(wait=True)\n",
    "        #     f.tight_layout()\n",
    "        #     f.show()\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        metrics[k] = np.asarray(v)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "loss_type_vec = ['mc_ce', 'td_ce', 'mc_sce', 'td_sce', 'mc_lsif', 'td_lsif']\n",
    "# loss_type_vec = ['mc_ce']\n",
    "\n",
    "all_metrics = dict()\n",
    "f_axes = None\n",
    "for col, log_linear in enumerate([True, False]):\n",
    "    \n",
    "    for loss_type in loss_type_vec:\n",
    "        metrics = train_and_eval(repr_dim, loss_type, log_linear)\n",
    "        \n",
    "        label = loss_type + ' log_lin={}'.format(int(log_linear))\n",
    "        all_metrics[label] = metrics\n",
    "        \n",
    "        f_axes = plot_metrics(metrics, f_axes=f_axes, logyscale_stats=['eval/error'], label=label)\n",
    "f, axes = f_axes\n",
    "\n",
    "legend = axes[0, 0].legend(bbox_to_anchor=(-0.1, -0.22),\n",
    "                           loc=\"upper left\", labelspacing=0.5, columnspacing=1.0, fancybox=False,\n",
    "                           shadow=False, fontsize=10, borderpad=0.35, handlelength=1.0, ncol=6)\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(\"figures/mc_td_fb_lc.png\", bbox_extra_artists=(legend, ))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d386858",
   "metadata": {},
   "source": [
    "final_errors = collections.defaultdict(list)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1)\n",
    "f.set_figheight(3)\n",
    "f.set_figwidth(6)\n",
    "\n",
    "x = np.arange(6)\n",
    "width = 0.2\n",
    "for col, log_linear in enumerate([True, False]):\n",
    "    for loss_type in loss_type_vec:\n",
    "        label = loss_type + ' log_lin={}'.format(int(log_linear))\n",
    "        \n",
    "        final_error = all_metrics[label]['eval/error'][-1, 1]\n",
    "        final_errors['log_lin={}'.format(log_linear)].append(final_error)\n",
    "        \n",
    "    ax.bar(x - width / 2 if col == 0 else x + width / 2,\n",
    "           final_errors['log_lin={}'.format(log_linear)], width=width, label='log_linear=%s' % log_linear)\n",
    "    ax.set_xticks(x, loss_type_vec)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(\"figures/mc_td_fb.png\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16c12b",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
