{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2467df1b",
   "metadata": {},
   "source": [
    "class RiverSwim:\n",
    "    def __init__(self, current='WEAK', seed=1234, max_episode_steps=1001):\n",
    "        self.num_states = 6\n",
    "        self.num_actions = 2  # O <=> LEFT, 1 <=> RIGHT\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "\n",
    "        # Larger current makes it harder to swim up the river\n",
    "        self.currents = ['WEAK', 'MEDIUM', 'STRONG']\n",
    "        assert current in self.currents\n",
    "        self.current = self.currents.index(current) + 1\n",
    "        assert self.current in [1, 2, 3]\n",
    "\n",
    "        # Configure reward function\n",
    "        R = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        R[0, 0, 0] = 0.005\n",
    "        R[5, 1, 5] = 1.0\n",
    "\n",
    "        # Configure transition function\n",
    "        T = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "\n",
    "        # Encode initial and rewarding state transitions\n",
    "        left, right = 0, 1\n",
    "        T[0, left, 0] = 1.\n",
    "        T[0, right, 0] = 0.4\n",
    "        T[0, right, 1] = 0.6\n",
    "\n",
    "        T[5, left, 4] = 1.\n",
    "        T[5, right, 5] = 0.6\n",
    "        T[5, right, 4] = 0.4\n",
    "\n",
    "        # Encode intermediate state transitions\n",
    "        for s in range(1, self.num_states - 1):\n",
    "            # Going left always succeeds\n",
    "            T[s, left, s - 1] = 1.\n",
    "\n",
    "            # Going right sometimes succeeds\n",
    "            T[s, right, s] = 0.6\n",
    "            T[s, right, s - 1] = 0.05 * self.current\n",
    "            T[s, right, s + 1] = 0.4 - 0.05 * self.current\n",
    "            assert np.isclose(np.sum(T[s, right]), 1.)\n",
    "\n",
    "        self.R = np.array(R)\n",
    "        self.T = np.array(T)\n",
    "\n",
    "        self.reset()\n",
    "        # self.timestep = 0\n",
    "        # self.init_state = np.random.randint(self.num_states)\n",
    "        # self.curr_state = self.init_state\n",
    "\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def get_model(self):\n",
    "        return copy.deepcopy(self.R), copy.deepcopy(self.T)\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.init_state = np.random.randint(self.num_states)\n",
    "        self.curr_state = self.init_state\n",
    "        return self.curr_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(np.arange(self.num_states), p=self.T[self.curr_state, action])\n",
    "        reward = self.R[self.curr_state, action, next_state]\n",
    "        self.curr_state = next_state\n",
    "        \n",
    "        terminated = False\n",
    "        self.timestep += 1\n",
    "        truncated = (self.timestep >= self.max_episode_steps)\n",
    "        \n",
    "        return reward, next_state, terminated, truncated\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6d45da",
   "metadata": {},
   "source": [
    "# collect dataset\n",
    "seed = 100\n",
    "discount = 0.99\n",
    "max_episode_steps = 1001\n",
    "env = RiverSwim(seed=seed, max_episode_steps=max_episode_steps)\n",
    "\n",
    "# uniform behavioral policy\n",
    "behavioral_policy = np.ones([env.num_states, env.num_actions]) / env.num_actions\n",
    "\n",
    "dataset = defaultdict(list)\n",
    "\n",
    "# dataset size = 100K\n",
    "num_episodes = 100\n",
    "num_transitions = 0\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = np.random.choice(np.arange(env.num_actions), p=behavioral_policy[obs])\n",
    "        reward, next_obs, terminated, truncated = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        num_transitions += 1\n",
    "        dataset['observations'].append(obs)\n",
    "        dataset['actions'].append(action)\n",
    "        dataset['rewards'].append(reward)\n",
    "        dataset['masks'].append(not terminated)  # for absorbing states\n",
    "        dataset['terminals'].append(done)  # for the end of trajectori\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    if 'observations' in k or 'actions' in k:\n",
    "        dtype = np.int32\n",
    "    elif k == 'terminals':\n",
    "        dtype = bool\n",
    "    else:\n",
    "        dtype = np.float32\n",
    "    dataset[k] = np.array(v, dtype=dtype)\n",
    "\n",
    "train_dataset = dict()\n",
    "ob_mask = (1.0 - dataset['terminals']).astype(bool)\n",
    "next_ob_mask = np.concatenate([[False], ob_mask[:-1]])\n",
    "train_dataset['observations'] = dataset['observations'][ob_mask]\n",
    "train_dataset['next_observations'] = dataset['observations'][next_ob_mask]\n",
    "train_dataset['actions'] = dataset['actions'][ob_mask]\n",
    "train_dataset['next_actions'] = dataset['actions'][next_ob_mask]\n",
    "train_dataset['rewards'] = dataset['rewards'][ob_mask]\n",
    "new_masks = np.concatenate([dataset['masks'][1:], [0.0]])\n",
    "train_dataset['masks'] = new_masks[ob_mask].astype(np.float32)\n",
    "new_terminals = np.concatenate([dataset['terminals'][1:], [1.0]])\n",
    "train_dataset['terminals'] = new_terminals[ob_mask].astype(np.float32)\n",
    "\n",
    "\n",
    "# dataset size = 100K\n",
    "dataset_size = 100_000\n",
    "print(\"num of total transitions {}\".format(train_dataset['observations'].shape[0]))\n",
    "\n",
    "assert train_dataset['observations'].shape[0] >= dataset_size\n",
    "for k, v in train_dataset.items():\n",
    "    train_dataset[k] = v[:dataset_size]\n",
    "\n",
    "print(\"num of transitions in the dataset: {}\".format(train_dataset['observations'].shape[0]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28c6cf46",
   "metadata": {},
   "source": [
    "# value iteration to find the optimal Q\n",
    "rewards, transition_probs = env.get_model()\n",
    "\n",
    "opt_q = np.zeros([env.num_states, env.num_actions], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  opt_q = (1 - discount) * np.sum(transition_probs * rewards, axis=-1) + discount * np.einsum('ijk,k->ij', transition_probs, np.max(opt_q, axis=-1))\n",
    "opt_v = np.max(opt_q, axis=-1)\n",
    "\n",
    "# deterministic optimal policy\n",
    "opt_policy = np.zeros([env.num_states, env.num_actions])\n",
    "opt_policy[np.arange(env.num_states), np.argmax(opt_q, axis=-1)] = 1.0\n",
    "\n",
    "# value iteration to find the behavioral Q\n",
    "rewards, transition_probs = env.get_model()\n",
    "\n",
    "behavioral_q = np.zeros([env.num_states, env.num_actions], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  behavioral_q = (1 - discount) * np.sum(transition_probs * rewards, axis=-1) + discount * np.einsum('ijk,k->ij', transition_probs, np.sum(behavioral_policy * behavioral_q, axis=-1))\n",
    "behavioral_v = np.sum(behavioral_policy * behavioral_q, axis=-1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee456c20",
   "metadata": {},
   "source": [
    "print(\"optimal q and optimal policy: \")\n",
    "print(opt_q)\n",
    "print(opt_policy)\n",
    "\n",
    "print(\"behavioral q: \")\n",
    "print(behavioral_q)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "abd401a0",
   "metadata": {},
   "source": [
    "### Augmented env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c003a1",
   "metadata": {},
   "source": [
    "class AugmentedRiverSwim:\n",
    "    def __init__(self, discount=0.99, current='WEAK', seed=1234, max_episode_steps=1001):\n",
    "        self.num_states = 6 + 2  # add s_plus and s_minus\n",
    "        self.num_actions = 2  # O <=> LEFT, 1 <=> RIGHT\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        \n",
    "        self.discount = discount\n",
    "        # Larger current makes it harder to swim up the river\n",
    "        self.currents = ['WEAK', 'MEDIUM', 'STRONG']\n",
    "        assert current in self.currents\n",
    "        self.current = self.currents.index(current) + 1\n",
    "        assert self.current in [1, 2, 3]\n",
    "\n",
    "        # Configure reward function\n",
    "        left, right = 0, 1\n",
    "        orig_R = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        orig_R[0, 0, 0] = 0.005\n",
    "        orig_R[self.num_states - 3, 1, self.num_states - 3] = 1.0\n",
    "\n",
    "        # Configure transition function\n",
    "        T = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "\n",
    "        # Encode initial and rewarding state transitions\n",
    "        T[0, left, 0] = 1.\n",
    "        T[0, right, 0] = 0.4\n",
    "        T[0, right, 1] = 0.6\n",
    "\n",
    "        T[self.num_states - 3, left, self.num_states - 4] = 1.\n",
    "        T[self.num_states - 3, right, self.num_states - 3] = 0.6\n",
    "        T[self.num_states - 3, right, self.num_states - 4] = 0.4\n",
    "\n",
    "        # Encode intermediate state transitions\n",
    "        for s in range(1, self.num_states - 3):\n",
    "            # Going left always succeeds\n",
    "            T[s, left, s - 1] = 1.\n",
    "\n",
    "            # Going right sometimes succeeds\n",
    "            T[s, right, s] = 0.6\n",
    "            T[s, right, s - 1] = 0.05 * self.current\n",
    "            T[s, right, s + 1] = 0.4 - 0.05 * self.current\n",
    "            assert np.isclose(np.sum(T[s, right]), 1.)\n",
    "\n",
    "        # transite to s+ and s- using r(s, a)\n",
    "        T[:self.num_states - 2, :, self.num_states - 2] = (1. - discount) * np.sum(orig_R[:self.num_states - 2] * T[:self.num_states - 2], axis=-1)\n",
    "        T[:self.num_states - 2, :, self.num_states - 1] = (1. - discount) * (1. - np.sum(orig_R[:self.num_states - 2] * T[:self.num_states - 2], axis=-1))\n",
    "        \n",
    "        # discount every transitions in the original MDP\n",
    "        T[:self.num_states - 2, :, :self.num_states - 2] *= discount\n",
    "        \n",
    "        T[self.num_states - 2, left, self.num_states - 2] = 1.\n",
    "        T[self.num_states - 2, right, self.num_states - 2] = 1.\n",
    "        T[self.num_states - 1, left, self.num_states - 1] = 1.\n",
    "        T[self.num_states - 1, right, self.num_states - 1] = 1.\n",
    "        \n",
    "        assert np.all(np.isclose(np.sum(T, axis=-1), 1.))\n",
    "        \n",
    "        R = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        # R[:self.num_states - 2, :, self.num_states - 2] = 1.0  # transit to s+ from states in the original MDP gives us reward\n",
    "        R[..., self.num_states - 2] = 1.0  # transit to s+ from any states in the augmented GCMDP gives us reward\n",
    "        # R[6] = 1.0  # only s+ in the augmented GCMDP gives us reward\n",
    "        self.R = np.asarray(R)\n",
    "        self.T = np.asarray(T)\n",
    "\n",
    "        self.reset()\n",
    "        # selr.timestep = 0\n",
    "        # self.init_state = 0\n",
    "        # self.curr_state = self.init_state\n",
    "\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def get_model(self):\n",
    "        return copy.deepcopy(self.R), copy.deepcopy(self.T)\n",
    "\n",
    "    def reset(self):\n",
    "        # the initial state distribution is the same as the original MDP\n",
    "        self.timestep = 0\n",
    "        self.init_state = np.random.randint(self.num_states - 2)\n",
    "        self.curr_state = self.init_state\n",
    "        return self.curr_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        terminated = (self.curr_state == self.num_states - 2) or (self.curr_state == self.num_states - 1)\n",
    "        \n",
    "        next_state = np.random.choice(np.arange(self.num_states), p=self.T[self.curr_state, action])\n",
    "        reward = self.R[self.curr_state, action, next_state]\n",
    "        self.curr_state = next_state\n",
    "        \n",
    "        self.timestep += 1\n",
    "        truncated = (self.timestep >= self.max_episode_steps)\n",
    "        \n",
    "        return reward, next_state, terminated, truncated\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ba2e37",
   "metadata": {},
   "source": [
    "# collect dataset\n",
    "seed = 100\n",
    "discount = 0.99\n",
    "max_episode_steps = 1001\n",
    "augmented_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "\n",
    "# optimal behavioral policy with noise\n",
    "# behavioral_policy = np.ones([augmented_env.num_states, augmented_env.num_actions]) / augmented_env.num_actions\n",
    "noisy_action_prob = 0.05\n",
    "behavioral_policy = np.zeros([augmented_env.num_states, augmented_env.num_actions])\n",
    "behavioral_policy[:, 1] = 1.\n",
    "\n",
    "augmented_dataset = defaultdict(list)\n",
    "\n",
    "num_episodes = 1_011\n",
    "num_transitions = 0\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    done = False\n",
    "    obs = augmented_env.reset()\n",
    "    while not done:\n",
    "        action = np.random.choice(np.arange(augmented_env.num_actions), p=behavioral_policy[obs])\n",
    "        if np.random.rand() < noisy_action_prob:\n",
    "            action = 1 - action\n",
    "        reward, next_obs, terminated, truncated = augmented_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        num_transitions += 1\n",
    "        augmented_dataset['observations'].append(obs)\n",
    "        augmented_dataset['actions'].append(action)\n",
    "        augmented_dataset['rewards'].append(reward)\n",
    "        augmented_dataset['masks'].append(not terminated)  # for absorbing states\n",
    "        augmented_dataset['terminals'].append(done)  # for the end of trajectori\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "for k, v in augmented_dataset.items():\n",
    "    if k in ['observations', 'actions']:\n",
    "        dtype = np.int32\n",
    "    elif k in ['masks', 'terminals']:\n",
    "        dtype = bool\n",
    "    else:\n",
    "        dtype = np.float32\n",
    "    augmented_dataset[k] = np.array(v, dtype=dtype)\n",
    "\n",
    "augmented_train_dataset = dict()\n",
    "ob_mask = (1.0 - augmented_dataset['terminals']).astype(bool)\n",
    "next_ob_mask = np.concatenate([[False], ob_mask[:-1]])\n",
    "augmented_train_dataset['observations'] = augmented_dataset['observations'][ob_mask]\n",
    "augmented_train_dataset['next_observations'] = augmented_dataset['observations'][next_ob_mask]\n",
    "augmented_train_dataset['actions'] = augmented_dataset['actions'][ob_mask]\n",
    "augmented_train_dataset['next_actions'] = augmented_dataset['actions'][next_ob_mask]\n",
    "augmented_train_dataset['rewards'] = augmented_dataset['rewards'][ob_mask]\n",
    "new_masks = np.concatenate([augmented_dataset['masks'][1:], [0.0]])\n",
    "augmented_train_dataset['masks'] = new_masks[ob_mask].astype(np.float32)  # where the observations are absorbing states\n",
    "new_terminals = np.concatenate([augmented_dataset['terminals'][1:], [1.0]])\n",
    "augmented_train_dataset['terminals'] = new_terminals[ob_mask].astype(np.float32)\n",
    "\n",
    "# dataset size = 100K\n",
    "dataset_size = 100_000\n",
    "print(augmented_train_dataset['observations'].shape[0])\n",
    "\n",
    "# assert augmented_train_dataset['observations'].shape[0] >= dataset_size\n",
    "# for k, v in augmented_train_dataset.items():\n",
    "#     augmented_train_dataset[k] = v[:dataset_size]\n",
    "\n",
    "print(\"num of augmented transitions: {}\".format(augmented_train_dataset['observations'].shape[0]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2a88eb",
   "metadata": {},
   "source": [
    "# print(np.where(augmented_train_dataset['masks'] == False))\n",
    "# print(np.where(augmented_train_dataset['observations'] == 6))\n",
    "\n",
    "# print(augmented_train_dataset['observations'][112])\n",
    "print(np.sum(augmented_train_dataset['observations'] == 6))\n",
    "print(np.sum(augmented_train_dataset['next_observations'] == 6))\n",
    "print(np.sum(augmented_train_dataset['observations'] == 7))\n",
    "print(np.sum(augmented_train_dataset['next_observations'] == 7))\n",
    "print(np.sum(augmented_train_dataset['rewards']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6957d86d",
   "metadata": {},
   "source": [
    "print(augmented_env.get_model()[1][0, 0])\n",
    "print(augmented_env.get_model()[1][1, 0])\n",
    "print(augmented_env.get_model()[1][2, 0])\n",
    "print(augmented_env.get_model()[1][3, 0])\n",
    "print(augmented_env.get_model()[1][4, 0])\n",
    "print(augmented_env.get_model()[1][5, 0])\n",
    "print(augmented_env.get_model()[1][6, 0])\n",
    "print(augmented_env.get_model()[1][7, 0])\n",
    "\n",
    "print(\"---\")\n",
    "print(augmented_env.get_model()[1][0, 1])\n",
    "print(augmented_env.get_model()[1][1, 1])\n",
    "print(augmented_env.get_model()[1][2, 1])\n",
    "print(augmented_env.get_model()[1][3, 1])\n",
    "print(augmented_env.get_model()[1][4, 1])\n",
    "print(augmented_env.get_model()[1][5, 1])\n",
    "print(augmented_env.get_model()[1][6, 1])\n",
    "print(augmented_env.get_model()[1][7, 1])\n",
    "\n",
    "print(\"---\")\n",
    "print(augmented_env.get_model()[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd5212f",
   "metadata": {},
   "source": [
    "# value iteration to find the optimal Q in the MDP using gamma ** 2\n",
    "rewards, transition_probs = env.get_model()\n",
    "\n",
    "opt_q_square_discount = np.zeros([env.num_states, env.num_actions], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  opt_q_square_discount = (1 - discount ** 2) * np.sum(rewards * transition_probs, axis=-1) + discount ** 2 * np.einsum('ijk,k->ij', transition_probs, np.max(opt_q_square_discount, axis=-1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b74c9286",
   "metadata": {},
   "source": [
    "# value iteration to find the optimal discounted state occupancy measure in the GCMDP using gamma\n",
    "rewards, transition_probs = augmented_env.get_model()\n",
    "\n",
    "opt_d_sa = np.zeros([augmented_env.num_states, augmented_env.num_actions, augmented_env.num_states], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  opt_a = np.argmax(opt_d_sa[..., augmented_env.num_states - 2], axis=-1)\n",
    "  opt_pi = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "  opt_pi[np.arange(augmented_env.num_states), opt_a] = 1.\n",
    "  # opt_d_sa = (1 - discount) * np.eye(augmented_env.num_states)[:, None] + discount * np.einsum('ijk,kl->ijl', transition_probs, np.sum(opt_d_sa * opt_pi[..., None], axis=1))\n",
    "  opt_d_sa = (1 - discount) * np.eye(augmented_env.num_states)[:, None] + discount * np.einsum('ijk,kl->ijl', transition_probs, np.sum(opt_d_sa * opt_pi[..., None], axis=1))\n",
    "\n",
    "opt_a = np.argmax(opt_d_sa[..., augmented_env.num_states - 2], axis=-1)\n",
    "opt_pi = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "opt_pi[np.arange(augmented_env.num_states), opt_a] = 1.\n",
    "\n",
    "opt_d_s = np.sum(opt_d_sa * opt_pi[..., None], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c6451a",
   "metadata": {},
   "source": [
    "print(\"scaled optimal d_gamma(s+ | s, a) and optimal policy: \")\n",
    "scaled_opt_d_sa_splus = opt_d_sa[..., augmented_env.num_states - 2] * (1 + discount) / discount\n",
    "print(scaled_opt_d_sa_splus)\n",
    "print(opt_pi)\n",
    "\n",
    "print(\"optimal Q_gamma^2(s, a): \")\n",
    "print(opt_q_square_discount)\n",
    "\n",
    "assert np.allclose(scaled_opt_d_sa_splus[:augmented_env.num_states - 2], opt_q_square_discount)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c71b6f6",
   "metadata": {},
   "source": [
    "# evaluation the optimal pi\n",
    "seed = 100\n",
    "max_episode_steps = 1001\n",
    "augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "\n",
    "num_episodes = 2000\n",
    "successes = []\n",
    "for _ in tqdm.trange(num_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "        action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=opt_pi[obs])\n",
    "        reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        traj_dataset['observations'].append(obs)\n",
    "        traj_dataset['actions'].append(action)\n",
    "        traj_dataset['rewards'].append(reward)\n",
    "        \n",
    "        obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "sr = np.mean(successes)\n",
    "print(\"success rate = {}\".format(sr))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "003bf771",
   "metadata": {},
   "source": [
    "# value iteration to compute the optimal single goal-conditioned Q in the GCMDP using gamma\n",
    "rewards, transition_probs = augmented_env.get_model()\n",
    "\n",
    "opt_gc_q = np.zeros([augmented_env.num_states, augmented_env.num_actions], dtype=np.float32)\n",
    "for _ in range(10_000):\n",
    "  opt_gc_q = (1 - discount) * np.sum(rewards * transition_probs, axis=-1) + discount * np.einsum('ijk,k->ij', transition_probs, np.max(opt_gc_q, axis=-1))\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "scaled_opt_gc_q = opt_gc_q * discount + indicators * (1 - discount)\n",
    "scaled_opt_gc_q = scaled_opt_gc_q * (1 + discount) / discount\n",
    "\n",
    "assert np.allclose(scaled_opt_d_sa_splus[:augmented_env.num_states - 2], scaled_opt_gc_q[:augmented_env.num_states - 2])\n",
    "assert np.allclose(opt_q_square_discount, scaled_opt_gc_q[:augmented_env.num_states - 2])\n",
    "\n",
    "print(\"optimal Q_gamma^2(s, a): \")\n",
    "print(opt_q_square_discount)\n",
    "\n",
    "print(\"scaled optimal goal-conditioned Q: \")\n",
    "print(scaled_opt_gc_q[:env.num_states])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d889b3",
   "metadata": {},
   "source": [
    "print(augmented_env.get_model()[1][0, 0])\n",
    "print(augmented_env.get_model()[1][1, 0])\n",
    "print(augmented_env.get_model()[1][2, 0])\n",
    "print(augmented_env.get_model()[1][3, 0])\n",
    "print(augmented_env.get_model()[1][4, 0])\n",
    "print(augmented_env.get_model()[1][5, 0])\n",
    "print(augmented_env.get_model()[1][6, 0])\n",
    "print(augmented_env.get_model()[1][7, 0])\n",
    "\n",
    "print(\"---\")\n",
    "print(augmented_env.get_model()[1][0, 1])\n",
    "print(augmented_env.get_model()[1][1, 1])\n",
    "print(augmented_env.get_model()[1][2, 1])\n",
    "print(augmented_env.get_model()[1][3, 1])\n",
    "print(augmented_env.get_model()[1][4, 1])\n",
    "print(augmented_env.get_model()[1][5, 1])\n",
    "print(augmented_env.get_model()[1][6, 1])\n",
    "print(augmented_env.get_model()[1][7, 1])\n",
    "\n",
    "print(augmented_env.get_model()[1][0, 0])\n",
    "print(augmented_env.get_model()[1][1, 0])\n",
    "print(augmented_env.get_model()[1][2, 0])\n",
    "print(augmented_env.get_model()[1][3, 0])\n",
    "print(augmented_env.get_model()[1][4, 0])\n",
    "print(augmented_env.get_model()[1][5, 0])\n",
    "print(augmented_env.get_model()[1][6, 0])\n",
    "print(augmented_env.get_model()[1][7, 0])\n",
    "\n",
    "print(\"---\")\n",
    "print(augmented_env.get_model()[1][0, 1])\n",
    "print(augmented_env.get_model()[1][1, 1])\n",
    "print(augmented_env.get_model()[1][2, 1])\n",
    "print(augmented_env.get_model()[1][3, 1])\n",
    "print(augmented_env.get_model()[1][4, 1])\n",
    "print(augmented_env.get_model()[1][5, 1])\n",
    "print(augmented_env.get_model()[1][6, 1])\n",
    "print(augmented_env.get_model()[1][7, 1])\n",
    "\n",
    "print(\"---\")\n",
    "print(augmented_env.get_model()[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d45e239a",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c45194",
   "metadata": {},
   "source": [
    "def sample_batch(batch_size):\n",
    "    dataset_size = len(train_dataset['observations'])\n",
    "    idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    batch = jax.tree_util.tree_map(lambda arr: arr[idxs], train_dataset)\n",
    "    \n",
    "    return batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed29ee7f",
   "metadata": {},
   "source": [
    "(terminal_locs,) = np.nonzero(augmented_train_dataset['terminals'] > 0)\n",
    "assert terminal_locs[-1] == (len(augmented_train_dataset['observations']) - 1)\n",
    "\n",
    "def sample_augmented_batch(batch_size, p_curgoal=0.2, p_trajgoal=0.5, relabel_reward=False):\n",
    "    dataset_size = len(augmented_train_dataset['observations'])\n",
    "    idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    batch = jax.tree_util.tree_map(lambda arr: arr[idxs], augmented_train_dataset)\n",
    "    \n",
    "    final_state_idxs = terminal_locs[np.searchsorted(terminal_locs, idxs)]\n",
    "    \n",
    "    offsets = np.random.geometric(p=1 - discount, size=batch_size)  # in [1, inf)\n",
    "    traj_goal_idxs = np.minimum(idxs + offsets, final_state_idxs)\n",
    "    random_goal_idxs = np.random.randint(dataset_size, size=batch_size)\n",
    "    goal_idxs = np.where(\n",
    "        np.random.rand(batch_size) < p_trajgoal / (1.0 - p_curgoal), traj_goal_idxs, random_goal_idxs\n",
    "    )\n",
    "    goal_idxs = np.where(np.random.rand(batch_size) < p_curgoal, idxs, goal_idxs)\n",
    "\n",
    "    batch['goals'] = jax.tree_util.tree_map(lambda arr: arr[goal_idxs], augmented_train_dataset['observations'])\n",
    "    if relabel_reward:\n",
    "        successes = np.logical_or((idxs + 1) == goal_idxs, idxs == goal_idxs).astype(float)\n",
    "        batch['masks'] = 1.0 - successes\n",
    "        batch['rewards'] = successes\n",
    "    \n",
    "    return batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d483e47",
   "metadata": {},
   "source": [
    "def plot_metrics(metrics, logyscale_stats=[], title=''):\n",
    "  # learning curves\n",
    "  nrows = np.ceil(len(metrics) / 4).astype(int)\n",
    "  ncols = 4\n",
    "  f, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "  if nrows == 1:\n",
    "    axes = np.array([axes])\n",
    "  f.set_figheight(3 * nrows)\n",
    "  f.set_figwidth(3 * ncols)\n",
    "\n",
    "  for idx, (name, val) in enumerate(metrics.items()):\n",
    "    v = np.array(val)\n",
    "    if len(v) == 0:\n",
    "      continue\n",
    "\n",
    "    x, y = v[:, 0], v[:, 1]\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "\n",
    "    if 'train' in name:\n",
    "      y = gaussian_filter1d(y, 100)\n",
    "    ax.plot(x, y)\n",
    "    if name in logyscale_stats:\n",
    "      ax.set_yscale('log')\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "  f.suptitle(title)\n",
    "\n",
    "  return f"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e954e82",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bcccafc",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, env.num_states)\n",
    "    action = jax.nn.one_hot(action, env.num_actions)\n",
    "    inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "\n",
    "    qs = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    qs = qs.squeeze(-1)\n",
    "\n",
    "    return qs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08aeabb2",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "critic = Critic()\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q = critic.apply(params, batch['observations'], batch['actions'])\n",
    "  \n",
    "  next_q1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']))\n",
    "  next_q2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']))\n",
    "  next_q = jnp.maximum(next_q1, next_q2)\n",
    "  target_q = (1 - discount ** 2) * batch['rewards'] + discount ** 2 * batch['masks'] * next_q\n",
    "\n",
    "  loss = jnp.mean((q - target_q) ** 2)\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions)\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "      traj_dataset = defaultdict(list)\n",
    "\n",
    "      done = False\n",
    "      obs = augmented_eval_env.reset()\n",
    "      while not done:\n",
    "          action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "          reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "          done = terminated or truncated\n",
    "          \n",
    "          traj_dataset['observations'].append(obs)\n",
    "          traj_dataset['actions'].append(action)\n",
    "          traj_dataset['rewards'].append(reward)\n",
    "          \n",
    "          obs = next_obs\n",
    "      successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(env.num_states)[:, None].repeat(env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(env.num_actions)[None, :].repeat(env.num_states, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions)\n",
    "  q = q.reshape([env.num_states, env.num_actions])\n",
    "  # scaled_q = q * (opt_q_square_discount / q).mean()\n",
    "  # q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  # scaled_q_err_mean = jnp.mean(np.abs(scaled_q - opt_q_square_discount))\n",
    "  # q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_batch(batch_size)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/q_corr_coef', 'eval/lstsq_q_err_mean'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f614c27d",
   "metadata": {},
   "source": [
    "q_learning_metrics = metrics\n",
    "print(q_learning_metrics['eval/q_err_mean'][-1])\n",
    "print(q_learning_metrics['eval/q_corr_coef'][-1])\n",
    "print(q_learning_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(q_learning_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "24e92ba0",
   "metadata": {},
   "source": [
    "### SGCQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bbfd919",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "    \n",
    "    qs = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    qs = qs.squeeze(-1)\n",
    "\n",
    "    return qs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dabc7358",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "indicators = jnp.asarray(indicators)\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_augmented_batch(2)\n",
    "critic = Critic()\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q = critic.apply(params, batch['observations'], batch['actions'])\n",
    "  \n",
    "  next_q1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']))\n",
    "  next_q2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']))\n",
    "  next_q = jnp.maximum(next_q1, next_q2)\n",
    "  target_q = (1 - discount) * batch['rewards'] + discount * batch['masks'] * next_q\n",
    "\n",
    "  loss = jnp.mean((q - target_q) ** 2)\n",
    "  \n",
    "  scaled_q = q * discount + indicators[batch['observations'], batch['actions']] * (1 - discount)\n",
    "  scaled_q = scaled_q * (1 + discount) / discount\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': scaled_q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions)\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  gc_q = critic.apply(params, obs, actions)\n",
    "  gc_q = gc_q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  # q = gc_q[:env.num_states]\n",
    "  # q = q * (1 + discount) / discount\n",
    "  scaled_gc_q = gc_q * discount + indicators * (1 - discount)\n",
    "  scaled_gc_q = scaled_gc_q * (1 + discount) / discount\n",
    "  q = scaled_gc_q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_augmented_batch(batch_size)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/q_corr_coef', 'eval/lstsq_q_err_mean'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed84c0ec",
   "metadata": {},
   "source": [
    "sgcql_metrics = metrics\n",
    "print(sgcql_metrics['eval/q_err_mean'][-1])\n",
    "print(sgcql_metrics['eval/q_corr_coef'][-1])\n",
    "print(sgcql_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(sgcql_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35f2c3a9",
   "metadata": {},
   "source": [
    "# evaluation the pi\n",
    "obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "gc_q = critic.apply(critic_params, obs, actions)\n",
    "gc_q = gc_q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "gc_a = jnp.argmax(gc_q, axis=-1)\n",
    "pi = jax.nn.one_hot(gc_a, augmented_env.num_actions)\n",
    "pi = np.asarray(pi)\n",
    "\n",
    "print(\"pi: \")\n",
    "print(pi)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4d5a64b8",
   "metadata": {},
   "source": [
    "### GCQL + HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c917188",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action, goals):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    goals = jax.nn.one_hot(goals, augmented_env.num_states)\n",
    "    inputs = jnp.concatenate([obs, action, goals], axis=-1)\n",
    "\n",
    "    qs = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    qs = qs.squeeze(-1)\n",
    "\n",
    "    return qs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03342b4a",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "indicators = jnp.asarray(indicators)\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_augmented_batch(2, relabel_reward=True)\n",
    "critic = Critic()\n",
    "critic_params = critic.init(critic_key, \n",
    "                            example_batch['observations'], \n",
    "                            example_batch['actions'], \n",
    "                            example_batch['goals'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q = critic.apply(params, batch['observations'], batch['actions'], batch['goals'])\n",
    "  \n",
    "  next_q1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']), batch['goals'])\n",
    "  next_q2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']), batch['goals'])\n",
    "  next_q = jnp.maximum(next_q1, next_q2)\n",
    "  target_q = (1 - discount) * batch['rewards'] + discount * batch['masks'] * next_q\n",
    "\n",
    "  loss = jnp.mean((q - target_q) ** 2)\n",
    "\n",
    "  # for logging\n",
    "  gc_q = critic.apply(params, batch['observations'], batch['actions'], (augmented_env.num_states - 2) * jnp.ones_like(batch['goals']))  \n",
    "  scaled_gc_q = gc_q * discount + indicators[batch['observations'], batch['actions']] * (1 - discount)\n",
    "  scaled_gc_q = scaled_gc_q * (1 + discount) / discount\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions, (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  gc_q = critic.apply(params, obs, actions, (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  gc_q = gc_q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  # q = gc_q[:env.num_states]\n",
    "  # q = q * (1 + discount) / discount\n",
    "  scaled_gc_q = gc_q * discount + indicators * (1 - discount)\n",
    "  scaled_gc_q = scaled_gc_q * (1 + discount) / discount\n",
    "  q = scaled_gc_q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_augmented_batch(batch_size, relabel_reward=True)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/q_corr_coef', 'eval/lstsq_q_err_mean'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d38d3e61",
   "metadata": {},
   "source": [
    "gcql_her_metrics = metrics\n",
    "print(gcql_her_metrics['eval/q_err_mean'][-1])\n",
    "print(gcql_her_metrics['eval/q_corr_coef'][-1])\n",
    "print(gcql_her_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(gcql_her_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4ff0640",
   "metadata": {},
   "source": [
    "# evaluation the pi\n",
    "obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "q = critic.apply(critic_params, obs, actions, (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "a = jnp.argmax(q, axis=-1)\n",
    "pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "pi = np.asarray(pi)\n",
    "\n",
    "print(\"pi: \")\n",
    "print(pi)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd56d8c9",
   "metadata": {},
   "source": [
    "### CRL + Binary NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4137f320",
   "metadata": {},
   "source": [
    "large_batch = sample_augmented_batch(1_000_000, p_curgoal=0.0, p_trajgoal=1.0)\n",
    "\n",
    "goal_marg = np.zeros(augmented_env.num_states)\n",
    "for state in range(augmented_env.num_states):\n",
    "    # goal_marg[state] = np.sum(large_batch['next_observations'] == state) / len(large_batch['next_observations'])\n",
    "    goal_marg[state] = np.sum(large_batch['next_observations'] == state) / len(large_batch['next_observations'])\n",
    "goal_marg = jnp.asarray(goal_marg)\n",
    "print(goal_marg)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab7b7b51",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  repr_dim: int = 512\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action, future_obs):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    future_obs = jax.nn.one_hot(future_obs, augmented_env.num_states)\n",
    "    phi_inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "    psi_inputs = future_obs\n",
    "\n",
    "    phi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(phi_inputs)\n",
    "    psi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(psi_inputs)\n",
    "    \n",
    "    logits = jnp.einsum('ik,jk->ij', phi, psi)\n",
    "    logits = logits / jnp.sqrt(self.repr_dim)\n",
    "    \n",
    "    return logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "150edf4a",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_augmented_batch(2, p_curgoal=0.0, p_trajgoal=1.0)\n",
    "critic = Critic(repr_dim=32)\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'], example_batch['next_observations'])\n",
    "\n",
    "def loss_fn(params, batch):\n",
    "  pos_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['goals'])\n",
    "  neg_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['next_observations'])\n",
    "  \n",
    "  I = jnp.eye(batch_size)\n",
    "  logits = I * pos_logits + (1 - I) * neg_logits\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=I).mean()\n",
    "  \n",
    "  plus_logits = critic.apply(params, batch['observations'], batch['actions'], \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']))\n",
    "  plus_probs = jnp.exp(jnp.diag(plus_logits)) * goal_marg[augmented_env.num_states - 2]\n",
    "  q = (1 + discount) / discount * plus_probs\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  return params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  plus_logits = jnp.diag(plus_logits)\n",
    "  plus_logits = plus_logits.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  a = jnp.argmax(plus_logits, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "  \n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  plus_logits = critic.apply(critic_params, obs, actions, \n",
    "                            (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  plus_probs = jnp.exp(jnp.diag(plus_logits)) * goal_marg[augmented_env.num_states - 2]\n",
    "  q = (1 + discount) / discount * plus_probs\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  q = q[:env.num_states]\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  plus_probs = jnp.exp(jnp.diag(plus_logits)) * goal_marg[augmented_env.num_states - 2]\n",
    "  q = (1 + discount) / discount * plus_probs\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  q = q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params)\n",
    "  # scaled_q = q * (opt_q_square_discount / q).mean()\n",
    "  # q_err_mean = jnp.mean(jnp.abs(q - opt_q_square_discount))\n",
    "  # scaled_q_err_mean = jnp.mean(jnp.abs(scaled_q - opt_q_square_discount))\n",
    "  # q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  \n",
    "  # random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  # logits = critic.apply(params, batch['observations'], batch['actions'], random_states)\n",
    "  # ratios = jax.nn.softmax(logits, axis=-1) * batch_size\n",
    "  # probs = jnp.diag(ratios) * s_marg[random_states]\n",
    "  # prob_err_mean = jnp.mean(jnp.abs(probs - opt_d_sa[batch['observations'], batch['actions'], random_states]))\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_augmented_batch(batch_size, p_curgoal=0.0, p_trajgoal=1.0)\n",
    "  critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6d2b5da",
   "metadata": {},
   "source": [
    "crl_metrics = metrics\n",
    "print(crl_metrics['eval/q_err_mean'][-1])\n",
    "print(crl_metrics['eval/q_corr_coef'][-1])\n",
    "print(crl_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(crl_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5ab3ae",
   "metadata": {},
   "source": [
    "# evaluation the pi\n",
    "obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "\n",
    "plus_logits = critic.apply(critic_params, obs, actions, \n",
    "                           (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "plus_logits = jnp.diag(plus_logits)\n",
    "plus_logits = plus_logits.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "a = jnp.argmax(plus_logits, axis=-1)\n",
    "pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "pi = np.asarray(pi)\n",
    "\n",
    "print(\"pi: \")\n",
    "print(pi)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "be60efcf",
   "metadata": {},
   "source": [
    "### SGC TD InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17a05a2c",
   "metadata": {},
   "source": [
    "s_marg = np.zeros(augmented_env.num_states)\n",
    "for state in range(augmented_env.num_states):\n",
    "    s_marg[state] = np.sum(augmented_train_dataset['next_observations'] == state) / len(augmented_train_dataset['next_observations'])\n",
    "s_marg = jnp.asarray(s_marg)\n",
    "print(s_marg)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20d1a821",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  repr_dim: int = 512\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action, future_obs):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    future_obs = jax.nn.one_hot(future_obs, augmented_env.num_states)\n",
    "    phi_inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "    psi_inputs = future_obs\n",
    "\n",
    "    phi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(phi_inputs)\n",
    "    psi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(psi_inputs)\n",
    "    \n",
    "    logits = jnp.einsum('ik,jk->ij', phi, psi)\n",
    "    logits = logits / jnp.sqrt(self.repr_dim)\n",
    "    \n",
    "    return logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7737a553",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "indicators = jnp.asarray(indicators)\n",
    "s_plus_marg = jnp.sum(augmented_train_dataset['next_observations'] == env.num_states - 2) / len(augmented_train_dataset['next_observations'])\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_augmented_batch(2)\n",
    "critic = Critic(repr_dim=32)\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'], example_batch['next_observations'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['next_observations'])\n",
    "  \n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  random_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], random_states)\n",
    "  \n",
    "  I = jnp.eye(batch_size)\n",
    "  logits = I * logits + (1 - I) * random_logits\n",
    "  cur_loss = optax.softmax_cross_entropy(logits=logits, labels=I)\n",
    "  \n",
    "  future_logit1 = critic.apply(\n",
    "    target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']), random_states)\n",
    "  future_logit2 = critic.apply(\n",
    "    target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']), random_states)\n",
    "  plus_logits1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']), \n",
    "                              (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']))\n",
    "  plus_logits2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']), \n",
    "                              (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']))\n",
    "  plus_log_probs1 = jnp.diag(plus_logits1) - jax.nn.logsumexp(future_logit1, axis=-1)\n",
    "  plus_log_probs2 = jnp.diag(plus_logits2) - jax.nn.logsumexp(future_logit2, axis=-1)\n",
    "  plus_log_probs = jnp.stack([plus_log_probs1, plus_log_probs2], axis=-1)\n",
    "  next_actions = jnp.argmax(plus_log_probs, axis=-1)\n",
    "  next_actions = jax.lax.stop_gradient(next_actions)\n",
    "  \n",
    "  w_logits = critic.apply(\n",
    "    target_params, batch['next_observations'], next_actions, random_states)\n",
    "  w = jax.nn.softmax(w_logits, axis=-1)\n",
    "  w = jax.lax.stop_gradient(w) * batch_size * I\n",
    "  # w = jnp.clip(w, 0.0, 100.0 / batch_size)\n",
    "  \n",
    "  future_loss = optax.softmax_cross_entropy(logits=random_logits, labels=w)\n",
    "  \n",
    "  loss = (1 - discount) * cur_loss + discount * batch['masks'] * future_loss\n",
    "  loss = jnp.mean(loss)\n",
    "  \n",
    "  plus_logits = critic.apply(params, batch['observations'], batch['actions'], \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']))\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  probs = jnp.exp(plus_log_probs) * batch_size * s_plus_marg\n",
    "  scaled_probs = discount * probs + (1 - discount) * indicators[batch['observations'], batch['actions']]\n",
    "  q = scaled_probs * (1 + discount) / discount\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, batch, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  # plus_logits = critic.apply(params, obs, actions, \n",
    "  #                            (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  # plus_logits = jnp.diag(plus_logits)\n",
    "  # plus_logits = plus_logits.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(random_states))\n",
    "  random_logits = critic.apply(params, obs, actions, random_states)\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  plus_log_probs = plus_log_probs.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  \n",
    "  a = jnp.argmax(plus_log_probs, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "  \n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params, batch):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(random_states))\n",
    "  random_logits = critic.apply(params, obs, actions, random_states)\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  plus_probs = jnp.exp(plus_log_probs) * batch_size * s_plus_marg\n",
    "  plus_probs = plus_probs.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  scaled_plus_probs = discount * plus_probs + (1 - discount) * indicators\n",
    "  q = scaled_plus_probs * (1 + discount) / discount\n",
    "  q = q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params, batch)\n",
    "  \n",
    "  # random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  # logits = critic.apply(params, batch['observations'], batch['actions'], random_states)\n",
    "  # ratios = jax.nn.softmax(logits, axis=-1) * batch_size\n",
    "  # probs = jnp.diag(ratios) * s_marg[random_states]\n",
    "  # prob_err_mean = jnp.mean(jnp.abs(probs - opt_d_sa[batch['observations'], batch['actions'], random_states]))\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "    # 'prob_err_mean': prob_err_mean,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_augmented_batch(batch_size)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params, batch)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a207f7",
   "metadata": {},
   "source": [
    "sgc_td_infonce_metrics = metrics\n",
    "print(sgc_td_infonce_metrics['eval/q_err_mean'][-1])\n",
    "print(sgc_td_infonce_metrics['eval/q_corr_coef'][-1])\n",
    "print(sgc_td_infonce_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(sgc_td_infonce_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7a759",
   "metadata": {},
   "source": [
    "# evaluation the pi\n",
    "batch = sample_augmented_batch(batch_size)\n",
    "\n",
    "obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "random_states = jnp.roll(batch['observations'], -1, axis=0)\n",
    "plus_logits = critic.apply(critic_params, obs, actions, \n",
    "                            (augmented_env.num_states - 2) * jnp.ones_like(random_states))\n",
    "random_logits = critic.apply(critic_params, obs, actions, random_states)\n",
    "plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "plus_log_probs = plus_log_probs.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "\n",
    "a = jnp.argmax(plus_log_probs, axis=-1)\n",
    "pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "pi = np.asarray(pi)\n",
    "\n",
    "print(\"pi: \")\n",
    "print(pi)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "850d6c3e",
   "metadata": {},
   "source": [
    "### GC TD InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "636e4757",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  repr_dim: int = 512\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action, goals, future_obs):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    goals = jax.nn.one_hot(goals, augmented_env.num_states)\n",
    "    future_obs = jax.nn.one_hot(future_obs, augmented_env.num_states)\n",
    "    phi_inputs = jnp.concatenate([obs, action, goals], axis=-1)\n",
    "    psi_inputs = future_obs\n",
    "\n",
    "    phi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(phi_inputs)\n",
    "    psi = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(self.repr_dim),\n",
    "    ])(psi_inputs)\n",
    "    \n",
    "    logits = jnp.einsum('ik,jk->ij', phi, psi)\n",
    "    logits = logits / jnp.sqrt(self.repr_dim)\n",
    "    \n",
    "    return logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60158f2e",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "indicators = jnp.asarray(indicators)\n",
    "s_plus_marg = jnp.sum(augmented_train_dataset['next_observations'] == env.num_states - 2) / len(augmented_train_dataset['next_observations'])\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_augmented_batch(2)\n",
    "critic = Critic(repr_dim=16)\n",
    "critic_params = critic.init(critic_key, \n",
    "                            example_batch['observations'], example_batch['actions'], \n",
    "                            example_batch['goals'], example_batch['next_observations'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['goals'], batch['next_observations'])\n",
    "  \n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  random_logits = critic.apply(\n",
    "    params, batch['observations'], batch['actions'], batch['goals'], random_states)\n",
    "  \n",
    "  I = jnp.eye(batch_size)\n",
    "  logits = I * logits + (1 - I) * random_logits\n",
    "  cur_loss = optax.softmax_cross_entropy(logits=logits, labels=I)\n",
    "  \n",
    "  future_logit1 = critic.apply(\n",
    "    target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']), batch['goals'], random_states)\n",
    "  future_logit2 = critic.apply(\n",
    "    target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']), batch['goals'], random_states)\n",
    "  plus_logits1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']), \n",
    "                              batch['goals'], batch['goals'])\n",
    "  plus_logits2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']), \n",
    "                              batch['goals'], batch['goals'])\n",
    "  plus_log_probs1 = jnp.diag(plus_logits1) - jax.nn.logsumexp(future_logit1, axis=-1)\n",
    "  plus_log_probs2 = jnp.diag(plus_logits2) - jax.nn.logsumexp(future_logit2, axis=-1)\n",
    "  plus_log_probs = jnp.stack([plus_log_probs1, plus_log_probs2], axis=-1)\n",
    "  next_actions = jnp.argmax(plus_log_probs, axis=-1)\n",
    "  next_actions = jax.lax.stop_gradient(next_actions)\n",
    "  \n",
    "  w_logits = critic.apply(\n",
    "    target_params, batch['next_observations'], next_actions, batch['goals'], random_states)\n",
    "  w = jax.nn.softmax(w_logits, axis=-1)\n",
    "  w = jax.lax.stop_gradient(w)\n",
    "  # w = jnp.clip(w, 0.0, 100.0 / batch_size)\n",
    "  \n",
    "  future_loss = optax.softmax_cross_entropy(logits=random_logits, labels=w)\n",
    "  \n",
    "  loss = (1 - discount) * cur_loss + discount * batch['masks'] * future_loss\n",
    "  loss = jnp.mean(loss)\n",
    "  \n",
    "  plus_logits = critic.apply(params, batch['observations'], batch['actions'], \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']),\n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']))\n",
    "  random_logits = critic.apply(params, batch['observations'], batch['actions'],\n",
    "                               (augmented_env.num_states - 2) * jnp.ones_like(batch['observations']),\n",
    "                               random_states)\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  probs = jnp.exp(plus_log_probs) * batch_size * s_plus_marg\n",
    "  scaled_probs = discount * probs + (1 - discount) * indicators[batch['observations'], batch['actions']]\n",
    "  q = scaled_probs * (1 + discount) / discount\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, batch, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  # plus_logits = critic.apply(params, obs, actions, \n",
    "  #                            (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  # plus_logits = jnp.diag(plus_logits)\n",
    "  # plus_logits = plus_logits.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  plus_logits = critic.apply(params, obs, actions, \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs),\n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  random_logits = critic.apply(params, obs, actions, \n",
    "                               (augmented_env.num_states - 2) * jnp.ones_like(obs),\n",
    "                               random_states)\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  plus_log_probs = plus_log_probs.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  \n",
    "  a = jnp.argmax(plus_log_probs, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "  \n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params, batch):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  plus_logits = critic.apply(params, obs, actions,\n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs), \n",
    "                             (augmented_env.num_states - 2) * jnp.ones_like(obs))\n",
    "  random_logits = critic.apply(params, obs, actions, \n",
    "                               (augmented_env.num_states - 2) * jnp.ones_like(obs),\n",
    "                               random_states)\n",
    "  plus_log_probs = jnp.diag(plus_logits) - jax.nn.logsumexp(random_logits, axis=-1)\n",
    "  plus_probs = jnp.exp(plus_log_probs) * batch_size * s_plus_marg\n",
    "  plus_probs = plus_probs.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  scaled_plus_probs = discount * plus_probs + (1 - discount) * indicators\n",
    "  q = scaled_plus_probs * (1 + discount) / discount\n",
    "  q = q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params, batch)\n",
    "  \n",
    "  # random_states = jnp.roll(batch['next_observations'], -1, axis=0)\n",
    "  # logits = critic.apply(params, batch['observations'], batch['actions'], random_states)\n",
    "  # ratios = jax.nn.softmax(logits, axis=-1) * batch_size\n",
    "  # probs = jnp.diag(ratios) * s_marg[random_states]\n",
    "  # prob_err_mean = jnp.mean(jnp.abs(probs - opt_d_sa[batch['observations'], batch['actions'], random_states]))\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "    # 'prob_err_mean': prob_err_mean,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_augmented_batch(batch_size)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params, batch)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/lstsq_q_err_mean', 'eval/q_corr_coef'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5989e06",
   "metadata": {},
   "source": [
    "td_infonce_metrics = metrics\n",
    "print(td_infonce_metrics['eval/q_err_mean'][-1])\n",
    "print(td_infonce_metrics['eval/q_corr_coef'][-1])\n",
    "print(td_infonce_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(td_infonce_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b65afa15",
   "metadata": {},
   "source": [
    "### SGCQL + data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "75500468",
   "metadata": {},
   "source": [
    "sample_batch(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e00c5525",
   "metadata": {},
   "source": [
    "def augment_batch(batch):\n",
    "    aug_batch = batch['observations']\n",
    "    \n",
    "    is_splus = np.random.rand() < batch['rewards']\n",
    "    aug_next_observations = np.where(\n",
    "        is_splus,\n",
    "        augmented_env.num_states - 2,\n",
    "        augmented_env.num_states - 1,\n",
    "    )\n",
    "    aug_rewards = np.where(\n",
    "        is_splus,\n",
    "        np.ones_like(batch['rewards']),\n",
    "        np.zeros_like(batch['rewards']),\n",
    "    )\n",
    "    aug_masks = np.zeros_like(aug_next_observations)\n",
    "    \n",
    "    batch['rewards'][:] = 0.0\n",
    "    aug_batch = jax.tree_util.tree_map(lambda arr: arr.copy(), batch)\n",
    "    aug_batch['next_observations'] = aug_next_observations\n",
    "    aug_batch['rewards'] = aug_rewards\n",
    "    aug_batch['masks'] = aug_masks\n",
    "    \n",
    "    aug_batch = jax.tree_util.tree_map(\n",
    "        lambda arr1, arr2: np.concatenate([arr1, arr2], axis=0), \n",
    "        batch, \n",
    "        aug_batch\n",
    "    )\n",
    "    \n",
    "    return aug_batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "17cbdab1",
   "metadata": {},
   "source": [
    "class Critic(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, obs, action):\n",
    "    obs = jax.nn.one_hot(obs, augmented_env.num_states)\n",
    "    action = jax.nn.one_hot(action, augmented_env.num_actions)\n",
    "    inputs = jnp.concatenate([obs, action], axis=-1)\n",
    "    \n",
    "    qs = nn.Sequential([\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(512),\n",
    "      nn.gelu,\n",
    "      nn.Dense(1),\n",
    "    ])(inputs)\n",
    "    \n",
    "    qs = qs.squeeze(-1)\n",
    "\n",
    "    return qs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c94d3447",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "tau = 0.005\n",
    "num_iterations = 50_000\n",
    "eval_interval = 5_000\n",
    "log_interval = 5_000\n",
    "\n",
    "indicators = np.zeros((augmented_env.num_states, augmented_env.num_actions))\n",
    "indicators[augmented_env.num_states - 2] = 1.\n",
    "indicators = jnp.asarray(indicators)\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 2**32))\n",
    "key, critic_key = jax.random.split(key)\n",
    "\n",
    "example_batch = sample_batch(2)\n",
    "example_batch = augment_batch(example_batch)\n",
    "critic = Critic()\n",
    "critic_params = critic.init(critic_key, example_batch['observations'], example_batch['actions'])\n",
    "target_critic_params = copy.deepcopy(critic_params)\n",
    "\n",
    "def loss_fn(params, target_params, batch):\n",
    "  q = critic.apply(params, batch['observations'], batch['actions'])\n",
    "  \n",
    "  next_q1 = critic.apply(target_params, batch['next_observations'], jnp.zeros_like(batch['next_observations']))\n",
    "  next_q2 = critic.apply(target_params, batch['next_observations'], jnp.ones_like(batch['next_observations']))\n",
    "  next_q = jnp.maximum(next_q1, next_q2)\n",
    "  target_q = (1 - discount) * batch['rewards'] + discount * batch['masks'] * next_q\n",
    "\n",
    "  loss = jnp.mean((q - target_q) ** 2)\n",
    "  \n",
    "  scaled_q = q * discount + indicators[batch['observations'], batch['actions']] * (1 - discount)\n",
    "  scaled_q = scaled_q * (1 + discount) / discount\n",
    "  \n",
    "  info = {\n",
    "    'loss': loss,\n",
    "    'q': scaled_q.mean(),\n",
    "  }\n",
    "  \n",
    "  return loss, info\n",
    "\n",
    "optimizer = optax.adam(learning_rate=3e-4)\n",
    "opt_state = optimizer.init(critic_params)\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(params, target_params, opt_state, batch):\n",
    "  (loss, info), grads = grad_fn(params, target_params, batch)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  target_params = jax.tree_util.tree_map(\n",
    "    lambda p, tp: p * tau + tp * (1 - tau),\n",
    "    params, target_params,\n",
    "  )\n",
    "  \n",
    "  return params, target_params, opt_state, loss, info\n",
    "\n",
    "def compute_success_rate(params, num_eval_episodes=2000, max_episode_steps=1001):\n",
    "  seed = 100\n",
    "  augmented_eval_env = AugmentedRiverSwim(discount=discount, seed=seed, max_episode_steps=max_episode_steps)\n",
    "  \n",
    "  # compute the pi\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  q = critic.apply(params, obs, actions)\n",
    "  q = q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  a = jnp.argmax(q, axis=-1)\n",
    "  pi = jax.nn.one_hot(a, augmented_env.num_actions)\n",
    "  pi = np.asarray(pi)\n",
    "\n",
    "  # evaluation\n",
    "  successes = []\n",
    "  for _ in tqdm.trange(num_eval_episodes):\n",
    "    traj_dataset = defaultdict(list)\n",
    "\n",
    "    done = False\n",
    "    obs = augmented_eval_env.reset()\n",
    "    while not done:\n",
    "      action = np.random.choice(np.arange(augmented_eval_env.num_actions), p=pi[obs])\n",
    "      reward, next_obs, terminated, truncated = augmented_eval_env.step(action)\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      traj_dataset['observations'].append(obs)\n",
    "      traj_dataset['actions'].append(action)\n",
    "      traj_dataset['rewards'].append(reward)\n",
    "      \n",
    "      obs = next_obs\n",
    "    successes.append(np.sum(traj_dataset['rewards']) >= 1.0)\n",
    "\n",
    "  sr = np.mean(successes)\n",
    "\n",
    "  return sr\n",
    "\n",
    "def evaluate_fn(params):\n",
    "  obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "  actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "  gc_q = critic.apply(params, obs, actions)\n",
    "  gc_q = gc_q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "  # q = gc_q[:env.num_states]\n",
    "  # q = q * (1 + discount) / discount\n",
    "  scaled_gc_q = gc_q * discount + indicators * (1 - discount)\n",
    "  scaled_gc_q = scaled_gc_q * (1 + discount) / discount\n",
    "  q = scaled_gc_q[:env.num_states]\n",
    "  \n",
    "  slopes, intercepts = jnp.linalg.lstsq(\n",
    "    jnp.stack([q.reshape(-1), jnp.ones_like(q.reshape(-1))], axis=1), \n",
    "    opt_q_square_discount.reshape(-1)\n",
    "  )[0]\n",
    "  lstsq_q = slopes * q + intercepts\n",
    "  \n",
    "  q_err_mean = jnp.mean(np.abs(q - opt_q_square_discount))\n",
    "  q_corr_coef = jnp.corrcoef(q.reshape(-1), opt_q_square_discount.reshape(-1))[0, 1]\n",
    "  lstsq_q_err_mean = jnp.mean(np.abs(lstsq_q - opt_q_square_discount))\n",
    "  \n",
    "  sr = compute_success_rate(params)\n",
    "  \n",
    "  info = {\n",
    "    'q_err_mean': q_err_mean,\n",
    "    'q_corr_coef': q_corr_coef,\n",
    "    'lstsq_q_err_mean': lstsq_q_err_mean,\n",
    "    'success_rate': sr,\n",
    "  }\n",
    "  \n",
    "  return info\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for i in tqdm.trange(1, num_iterations + 1):\n",
    "  batch = sample_batch(batch_size)\n",
    "  aug_batch = augment_batch(batch)\n",
    "  critic_params, target_critic_params, opt_state, loss, info = update_fn(\n",
    "    critic_params, target_critic_params, opt_state, aug_batch)\n",
    "\n",
    "  for k, v in info.items():\n",
    "    metrics['train/' + k].append(\n",
    "      np.array([i, v])\n",
    "    )\n",
    "\n",
    "  if i == 1 or i % eval_interval == 0:\n",
    "    eval_info = evaluate_fn(critic_params)\n",
    "    for k, v in eval_info.items():\n",
    "      metrics['eval/' + k].append(\n",
    "        np.array([i, v])\n",
    "      )\n",
    "\n",
    "  if i == 1 or i % log_interval == 0:\n",
    "    plot_metrics(metrics, logyscale_stats=['eval/q_err_mean', 'eval/q_corr_coef', 'eval/lstsq_q_err_mean'])\n",
    "    display.clear_output(wait=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7dfe7044",
   "metadata": {},
   "source": [
    "sgcql_data_aug_metrics = metrics\n",
    "print(sgcql_data_aug_metrics['eval/q_err_mean'][-1])\n",
    "print(sgcql_data_aug_metrics['eval/q_corr_coef'][-1])\n",
    "print(sgcql_data_aug_metrics['eval/lstsq_q_err_mean'][-1])\n",
    "print(sgcql_data_aug_metrics['eval/success_rate'][-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "697480fb",
   "metadata": {},
   "source": [
    "# evaluation the pi\n",
    "obs = jnp.arange(augmented_env.num_states)[:, None].repeat(augmented_env.num_actions, axis=1).reshape(-1)\n",
    "actions = jnp.arange(augmented_env.num_actions)[None, :].repeat(augmented_env.num_states, axis=0).reshape(-1)\n",
    "gc_q = critic.apply(critic_params, obs, actions)\n",
    "gc_q = gc_q.reshape([augmented_env.num_states, augmented_env.num_actions])\n",
    "gc_a = jnp.argmax(gc_q, axis=-1)\n",
    "pi = jax.nn.one_hot(gc_a, augmented_env.num_actions)\n",
    "pi = np.asarray(pi)\n",
    "\n",
    "print(\"pi: \")\n",
    "print(pi)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040abf35",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.2))\n",
    "\n",
    "metric_name = 'eval/q_err_mean'\n",
    "ax = axes[0]\n",
    "for algo, metrics in zip(['Q-learning', 'C51', 'FDQL'], [q_learning_metrics, c51_metrics, fdql_metrics]):\n",
    "    metrcs_arr = np.asarray(metrics[metric_name])\n",
    "    l, = ax.plot(metrcs_arr[:, 0], metrcs_arr[:, 1], label=algo, zorder=3)\n",
    "ax.set_ylabel(r'$| Q - Q^{\\star} |$', fontsize=14)\n",
    "ax.set_xlim([-4e2, 5e4 + 4e2])\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(zorder=3)\n",
    "\n",
    "metric_name = 'eval/scaled_q_err_mean'\n",
    "ax = axes[1]\n",
    "for algo, metrics in zip(['Q-learning', 'C51', 'FDQL'], [q_learning_metrics, c51_metrics, fdql_metrics]):\n",
    "    metrcs_arr = np.asarray(metrics[metric_name])\n",
    "    l, = ax.plot(metrcs_arr[:, 0], metrcs_arr[:, 1], label=algo, zorder=3)\n",
    "ax.set_ylabel(r'scaled $| Q - Q^{\\star} |$', fontsize=14)\n",
    "ax.set_xlim([-4e2, 5e4 + 4e2])\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "# ax.set_ylim([-5, 120 + 5])\n",
    "# ax.set_yticks([0, 120])\n",
    "# ax.yaxis.set_minor_locator(MultipleLocator(100))\n",
    "ax.grid(zorder=3)     \n",
    "\n",
    "fig.tight_layout(rect=(-0.026, -0.06, 1.02, 1.04))  # rect = (left, bottom, right, top), default: (0, 0, 1, 1)\n",
    "# filepath = \"/u/cz8792/research/ogbench/plot_scripts/figures/convergence_speed_ablation_lc.pdf\"\n",
    "# fig.savefig(filepath, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b162e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
