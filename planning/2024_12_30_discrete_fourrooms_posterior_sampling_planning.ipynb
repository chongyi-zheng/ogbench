{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libs"
   ],
   "metadata": {
    "id": "ihlsmnHF4tXy"
   },
   "id": "ihlsmnHF4tXy"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import tqdm\n",
    "from IPython import display\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import gymnasium as gym\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import common_utils\n",
    "import optax"
   ],
   "metadata": {
    "id": "Hbm5ghYo4t4K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155793422,
     "user_tz": 300,
     "elapsed": 1478,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    }
   },
   "id": "Hbm5ghYo4t4K",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discrete FourRooms Env"
   ],
   "metadata": {
    "id": "K5Wg66n4N45l"
   },
   "id": "K5Wg66n4N45l"
  },
  {
   "cell_type": "code",
   "source": [
    "WALLS = {\n",
    "  \"FourRooms\": np.array(\n",
    "    [\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    "  ),\n",
    "}\n",
    "\n",
    "def resize_walls(walls, factor):\n",
    "  \"\"\"Increase the environment by rescaling.\n",
    "\n",
    "  Args:\n",
    "    walls: 0/1 array indicating obstacle locations.\n",
    "    factor: (int) factor by which to rescale the environment.\n",
    "  \"\"\"\n",
    "  (height, width) = walls.shape\n",
    "  row_indices = np.array([i for i in range(height) for _ in range(factor)])\n",
    "  col_indices = np.array([i for i in range(width) for _ in range(factor)])\n",
    "  walls = walls[row_indices]\n",
    "  walls = walls[:, col_indices]\n",
    "  assert walls.shape == (factor * height, factor * width)\n",
    "  return walls\n",
    "\n",
    "\n",
    "class DiscretePointEnv:\n",
    "  \"\"\"Abstract class for 2D navigation environments.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    walls='FourRooms',\n",
    "    random_initial_state=True,\n",
    "    gamma=0.95,\n",
    "  ):\n",
    "    \"\"\"Initialize the point environment.\n",
    "\n",
    "    Args:\n",
    "      walls: (str) Name of one of the maps defined above.\n",
    "      random_initial_state: (bool) Whether the initial state should be chosen\n",
    "        uniformly across the state space, or fixed at (0, 0).\n",
    "      gamma: (float) The discount factor.\n",
    "    \"\"\"\n",
    "    self._walls = WALLS[walls]\n",
    "    (height, width) = self._walls.shape\n",
    "    self._height = height\n",
    "    self._width = width\n",
    "    self._random_initial_state = random_initial_state\n",
    "\n",
    "    self.candidate_states = np.where(self._walls == 0)\n",
    "\n",
    "    self.state_dim = 1\n",
    "    self.action_dim = 1\n",
    "    self.action_space = gym.spaces.Discrete(5)  # null - 0, up - 1, left - 2, down - 3, right - 4\n",
    "    self.observation_space = gym.spaces.Discrete(len(self.candidate_states[0]))\n",
    "\n",
    "    self.state = 0\n",
    "    self.goal = self.candidate_states[-1]\n",
    "\n",
    "  def _sample_empty_state(self):\n",
    "    state = int(self.observation_space.sample())\n",
    "    return state\n",
    "\n",
    "  def state2coordinates(self, state):\n",
    "    row = self.candidate_states[0][state]\n",
    "    col = self.candidate_states[1][state]\n",
    "\n",
    "    return row, col\n",
    "\n",
    "  def state2xys(self, state):\n",
    "    row, col = self.state2coordinates(state)\n",
    "\n",
    "    return np.stack([col, row], axis=-1)\n",
    "\n",
    "  def coordinates2state(self, row, col):\n",
    "    state = int(np.sum(self._walls.flatten()[:row * self._width + col] == 0))\n",
    "\n",
    "    return state\n",
    "\n",
    "  def compute_reward(self, state, goal=None):\n",
    "    if goal is None:\n",
    "      goal = self.goal\n",
    "    if state == goal:\n",
    "      return (1 - gamma)\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def get_next_state(self, state, action):\n",
    "    curr_state = self.state\n",
    "    self.state = state\n",
    "    next_state = self.step(action)[0]\n",
    "    self.state = curr_state\n",
    "\n",
    "    return next_state\n",
    "\n",
    "  def reset(self, state=None, goal=None):\n",
    "    if self._random_initial_state:\n",
    "      self.state = self._sample_empty_state()\n",
    "    else:\n",
    "      self.state = state if state is not None else 0\n",
    "    self.goal = goal if goal is not None else self._sample_empty_state()\n",
    "\n",
    "    info = dict(goal=self.goal)\n",
    "    return self.state, info\n",
    "\n",
    "  def step(self, action):\n",
    "    reward = self.compute_reward(self.state, self.goal)\n",
    "\n",
    "    row, col = self.state2coordinates(self.state)\n",
    "    if action == 0:  # noop\n",
    "      pass\n",
    "    elif action == 1:  # up\n",
    "      if (row - 1 >= 0) and (not self._walls[row - 1, col]):\n",
    "        self.state = self.coordinates2state(row - 1, col)\n",
    "    elif action == 2:  # left\n",
    "      if (col - 1 >= 0) and (not self._walls[row, col - 1]):\n",
    "        self.state = self.coordinates2state(row, col - 1)\n",
    "    elif action == 3:  # down\n",
    "      if (row + 1 <= self._height - 1) and (not self._walls[row + 1, col]):\n",
    "        self.state = self.coordinates2state(row + 1, col)\n",
    "    elif action == 4:  # right\n",
    "      if (col + 1 <= self._width - 1) and (not self._walls[row, col + 1]):\n",
    "        self.state = self.coordinates2state(row, col + 1)\n",
    "    else:\n",
    "      raise RuntimeError(\"Invalid action: {}\".format(a))\n",
    "\n",
    "    done = float(self.state == self.goal)\n",
    "    success = float(self.state == self.goal)\n",
    "    info = dict(goal=self.goal, success=success)\n",
    "\n",
    "    return self.state, reward, done, info"
   ],
   "metadata": {
    "id": "jSrFtrRiOGjg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155793440,
     "user_tz": 300,
     "elapsed": 7,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    }
   },
   "id": "jSrFtrRiOGjg",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# collect dataset\n",
    "gamma = 0.95\n",
    "env = DiscretePointEnv(walls='FourRooms', gamma=gamma)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "goal = num_states // 2\n",
    "beta = np.ones((num_states, num_actions)) / num_actions\n",
    "num_trajs = 1_000  # 100K dataset\n",
    "max_episode_steps = 100\n",
    "\n",
    "dataset_size = num_trajs * max_episode_steps\n",
    "traj_dataset = []\n",
    "\n",
    "for traj_idx in tqdm.trange(num_trajs):\n",
    "  traj = []\n",
    "  # randomized the initial state\n",
    "  s, _ = env.reset(goal=goal)\n",
    "  for t in range(max_episode_steps):\n",
    "    a = np.random.choice(num_actions, p=beta[s])\n",
    "    next_s, r, d, _ = env.step(a)\n",
    "\n",
    "    traj.append((s, a, r, d, next_s, traj_idx, t))\n",
    "    s = next_s\n",
    "  traj_dataset.append(traj)\n",
    "\n",
    "traj_dataset = np.array(traj_dataset)  # (num_traj, max_episode_steps, 5)\n",
    "traj_dataset_dict = {\n",
    "  's': traj_dataset[..., 0].astype(np.int64),\n",
    "  'a': traj_dataset[..., 1].astype(np.int64),\n",
    "  'r': traj_dataset[..., 2],\n",
    "  'd': traj_dataset[..., 3],\n",
    "  'next_s': traj_dataset[..., 4].astype(np.int64),\n",
    "  'traj_idx': traj_dataset[..., 5].astype(np.int64),\n",
    "  't': traj_dataset[..., 6].astype(np.int64),\n",
    "}\n",
    "\n",
    "dataset_dict = dict(traj_dataset_dict)\n",
    "for k, v in dataset_dict.items():\n",
    "  dataset_dict[k] = v[:, :-1].flatten()\n",
    "dataset_dict['next_a'] = traj_dataset_dict['a'][:, 1:].flatten()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av6pZQacQhFF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155796415,
     "user_tz": 300,
     "elapsed": 2965,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "0a0ebb58-92ca-4f83-b5b2-ef393cda17aa"
   },
   "id": "av6pZQacQhFF",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute the discounted state occupancy measure"
   ],
   "metadata": {
    "id": "W2asX8d6rg20"
   },
   "id": "W2asX8d6rg20"
  },
  {
   "cell_type": "code",
   "source": [
    "# compute the ground truth future state distribution\n",
    "sa_transition_prob = np.zeros((num_states, num_actions, num_states))\n",
    "for s in range(num_states):\n",
    "  for a in range(num_actions):\n",
    "    next_s = env.get_next_state(s, a)\n",
    "    sa_transition_prob[s, a, next_s] = 1\n",
    "s_transition_prob = np.sum(sa_transition_prob * beta[:, :, None], axis=1)\n",
    "\n",
    "inv = np.linalg.inv(np.eye(num_states) - gamma * s_transition_prob)\n",
    "sa_future_state_prob = (1 - gamma) * np.einsum(\"ijk,kh->ijh\", sa_transition_prob, inv)\n",
    "\n",
    "# start from the current timestep\n",
    "# s_future_state_prob = (1 - gamma) * inv\n",
    "# start from the next timestep\n",
    "s_future_state_prob = (1 - gamma) * inv @ s_transition_prob\n",
    "s_future_state_prob = np.clip(s_future_state_prob, 0.0, 1.0)\n",
    "\n",
    "# the marginal state distribution is uniform\n",
    "s_marginal = np.ones(num_states)\n",
    "s_marginal /= np.sum(s_marginal, axis=-1, keepdims=True)\n",
    "\n",
    "# future state marginal distribution\n",
    "s_future_marginal = np.sum(s_transition_prob * s_marginal[:, None], axis=0)\n",
    "\n",
    "gt_gamma_to_hitting_time = s_future_state_prob / jnp.diag(s_future_state_prob)[None, :]"
   ],
   "metadata": {
    "id": "YYD5DfyEUDv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155797139,
     "user_tz": 300,
     "elapsed": 713,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    }
   },
   "id": "YYD5DfyEUDv1",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# estimate the KL divergence\n",
    "cond_kl = jnp.sum(s_future_state_prob * jnp.log(s_future_state_prob / s_future_marginal[None]), axis=-1)\n",
    "kl = jnp.sum(s_marginal * cond_kl)\n",
    "print(kl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtrzAPeQsmJB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155797334,
     "user_tz": 300,
     "elapsed": 194,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "cf08e894-f8e2-4581-c75b-78b08b1bdec9"
   },
   "id": "HtrzAPeQsmJB",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the ground truth ratios"
   ],
   "metadata": {
    "id": "T85Upqp4Sz-A"
   },
   "id": "T85Upqp4Sz-A"
  },
  {
   "cell_type": "code",
   "source": [
    "# empty_mask = (env._walls == 0).flatten()\n",
    "\n",
    "# log p(g | s) - log p(g)\n",
    "rows, cols = env.state2coordinates(np.arange(num_states))\n",
    "gt_crl_log_ratios = np.ones((env._height, env._width, env._height, env._width)) * np.nan\n",
    "gt_crl_log_ratios[rows[:, None], cols[:, None], rows[None, :], cols[None, :]] = np.log(s_future_state_prob) - np.log(s_marginal[None, :])\n",
    "\n",
    "# log p(g | s) - log p(g | g)\n",
    "gt_cmd_log_ratios = np.ones((env._height, env._width, env._height, env._width)) * np.nan\n",
    "gt_cmd_log_ratios[rows[:, None], cols[:, None], rows[None, :], cols[None, :]] = np.log(s_future_state_prob) - np.log(np.diag(s_future_state_prob)[:, None])\n",
    "\n",
    "NUM_PLOTS = 5\n",
    "fig, axes = plt.subplots(nrows=4, ncols=NUM_PLOTS)\n",
    "fig.set_figheight(4 * 4)\n",
    "fig.set_figwidth(4 * NUM_PLOTS)\n",
    "\n",
    "cmap = plt.get_cmap()\n",
    "cmap.set_bad([1, 1, 1, 1])\n",
    "\n",
    "for idx, state in enumerate([0, num_states // 4, num_states // 2, 3 * num_states // 4, num_states - 1]):\n",
    "  ax = axes[0, idx]\n",
    "  row, col = env.state2coordinates(state)\n",
    "  im = ax.imshow(gt_crl_log_ratios[row, col], cmap=cmap)\n",
    "  ax.plot([col], [row], 'rx', markersize=15)\n",
    "  plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "  ax.set_title(r\"GT CRL log ratio: $\\log p(g \\mid s_0) - \\log p(g)$\")\n",
    "\n",
    "  ax = axes[1, idx]\n",
    "  row, col = env.state2coordinates(state)\n",
    "  im = ax.imshow(gt_crl_log_ratios[:, :, row, col], cmap=cmap)\n",
    "  ax.plot([col], [row], 'rx', markersize=15)\n",
    "  plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "  ax.set_title(r\"GT CRL log ratio: $\\log p(g_0 \\mid w) - \\log p(g_0)$\")\n",
    "\n",
    "  # mask = ~(np.isnan(gt_crl_log_ratios[row, col]) | np.isnan(gt_crl_log_ratios[:, :, row, col]))\n",
    "  # print(np.allclose(gt_crl_log_ratios[row, col][mask], gt_crl_log_ratios[:, :, row, col][mask]))\n",
    "  # print(np.nanmin(gt_crl_log_ratios[row, col]))\n",
    "  # print(np.nanmin(gt_crl_log_ratios[:, :, row, col]))\n",
    "\n",
    "  ax = axes[2, idx]\n",
    "  row, col = env.state2coordinates(state)\n",
    "  im = ax.imshow(gt_cmd_log_ratios[row, col], cmap=cmap)\n",
    "  ax.plot([col], [row], 'rx', markersize=15)\n",
    "  plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "  ax.set_title(r\"GT CMD log ratio: $\\log p(g \\mid s_0) - \\log p(g \\mid g)$\")\n",
    "\n",
    "  ax = axes[3, idx]\n",
    "  row, col = env.state2coordinates(state)\n",
    "  im = ax.imshow(gt_cmd_log_ratios[:, :, row, col], cmap=cmap)\n",
    "  ax.plot([col], [row], 'rx', markersize=15)\n",
    "  plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "  ax.set_title(r\"GT CMD log ratio: $\\log p(g_0 \\mid w) - \\log p(g_0 \\mid g_0)$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nHMBsF6fS2o1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155800751,
     "user_tz": 300,
     "elapsed": 3417,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "6ee7fdf5-a745-42c1-ebcb-f1ee8aea802f"
   },
   "id": "nHMBsF6fS2o1",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Planning with Posterior sampling planner"
   ],
   "metadata": {
    "id": "SQ5-o0M4VO5r"
   },
   "id": "SQ5-o0M4VO5r"
  },
  {
   "cell_type": "code",
   "source": [
    "# locally optimal scripted policy\n",
    "candidate_states = jnp.array(env.candidate_states)\n",
    "\n",
    "def state2xys(state):\n",
    "  row = candidate_states[0, state]\n",
    "  col = candidate_states[1, state]\n",
    "\n",
    "  return jnp.stack([col, row], axis=-1)\n",
    "\n",
    "def scripted_policy(state, goal):\n",
    "  state_xy, goal_xy = state2xys([state, goal])\n",
    "  dcol = goal_xy[0] - state_xy[0]\n",
    "  drow = goal_xy[1] - state_xy[1]\n",
    "\n",
    "  action = jnp.select(condlist=[dcol > 0, dcol < 0, drow > 0, drow < 0],\n",
    "                      choicelist=[4, 2, 3, 1],\n",
    "                      default=0)\n",
    "\n",
    "  return action\n",
    "\n",
    "# posterior sampling planner\n",
    "class Policy:\n",
    "  def __init__(self, dist_type='analytical_crl_ratio',\n",
    "               planning_per_steps=1,\n",
    "               num_waypoints_in_sequence=10,\n",
    "               num_waypoint_sequences=1024,\n",
    "               use_scripted_policy=True):\n",
    "    self.dist_type = dist_type\n",
    "    self.planning_per_steps = planning_per_steps\n",
    "    self.num_waypoints_in_sequence = num_waypoints_in_sequence\n",
    "    self.num_waypoint_sequences = num_waypoint_sequences\n",
    "    self.use_scripted_policy = use_scripted_policy\n",
    "\n",
    "    # self.s_future_marginal = jnp.asarray(s_future_marginal)\n",
    "\n",
    "  def get_action(self, obs, goal, key, t, last_waypoint_seqs, last_waypoint_seq_idx, last_waypoint):\n",
    "    def planning_f(key):\n",
    "      key, marginal_key, score_key = jax.random.split(key, num=3)\n",
    "      waypoint_seqs = jax.random.choice(\n",
    "        marginal_key, num_states,\n",
    "        shape=(self.num_waypoint_sequences, self.num_waypoints_in_sequence),\n",
    "        p=jnp.asarray(s_future_marginal),\n",
    "      )\n",
    "      # xs, ys = jnp.meshgrid(jnp.arange(num_states), jnp.arange(num_states))\n",
    "      # waypoint_seqs = jnp.stack([xs.flatten(), ys.flatten()], axis=-1)\n",
    "\n",
    "      if self.dist_type == 'analytical_crl_ratio':\n",
    "        ratio = jnp.log(s_future_state_prob) - jnp.log(s_future_marginal[None, :])\n",
    "      elif self.dist_type == 'empirical_crl_ratio':\n",
    "        # ratio = jnp.log(empirical_s_future_state_prob) - jnp.log(empirical_future_s_marginal_prob[None, :])\n",
    "        raise NotImplementedError\n",
    "\n",
    "      sw_logits = ratio[obs, waypoint_seqs[:, 0]]  # (N, ), log p(w_1 | s) - log p(w_1)\n",
    "      ww_logits = ratio[waypoint_seqs[:, :-1], waypoint_seqs[:, 1:]]  # (N, T - 1), log p(w_{t + 1} | w_t) - log p(w_{t + 1})\n",
    "      wg_logits = ratio[waypoint_seqs[:, -1], goal]  # (N, )  log p(g | w_T) - log p(g)\n",
    "\n",
    "      log_scores = jnp.concatenate([sw_logits[:, None], ww_logits, wg_logits[:, None]], axis=-1)  # (N, T + 1)\n",
    "      log_scores = jnp.sum(log_scores, axis=-1)\n",
    "      scores = jax.nn.softmax(log_scores)\n",
    "\n",
    "      w_seq_idx = jax.random.choice(score_key, self.num_waypoint_sequences, p=scores)\n",
    "      # w_seq_idx = jnp.argmax(scores)\n",
    "      waypoint = waypoint_seqs[w_seq_idx, 0]\n",
    "\n",
    "      return waypoint_seqs, w_seq_idx, waypoint\n",
    "\n",
    "    key, planning_key = jax.random.split(key)\n",
    "    waypoint_seqs, waypoint_seq_idx, waypoint = jax.lax.cond(\n",
    "      t % self.planning_per_steps == 0,\n",
    "      planning_f,\n",
    "      lambda k: (last_waypoint_seqs, last_waypoint_seq_idx, last_waypoint),\n",
    "      planning_key,\n",
    "    )\n",
    "\n",
    "    last_waypoint_seqs = waypoint_seqs\n",
    "    last_waypoint_seq_idx = waypoint_seq_idx\n",
    "    last_waypoint = waypoint\n",
    "\n",
    "    if self.use_scripted_policy:\n",
    "      return scripted_policy(obs, waypoint), waypoint_seqs, waypoint_seq_idx, waypoint\n",
    "    else:\n",
    "      raise NotImplementedError"
   ],
   "metadata": {
    "id": "tjV-1_OvK0Z4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155800760,
     "user_tz": 300,
     "elapsed": 2,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    }
   },
   "id": "tjV-1_OvK0Z4",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### CRL log ratio dist Dijkstra planner + scripted policy"
   ],
   "metadata": {
    "id": "7P0Fj_ly87Od"
   },
   "id": "7P0Fj_ly87Od"
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_EPISODES = 100\n",
    "successes = np.zeros(NUM_EPISODES)\n",
    "trajs = {\n",
    "  'obs': [],\n",
    "  'action': [],\n",
    "  'reward': [],\n",
    "  'done': [],\n",
    "  'discrete_success': [],\n",
    "  'success': [],\n",
    "  'waypoint_seqs': [],\n",
    "  'waypoint_seq_idxs': [],\n",
    "  'waypoints': [],\n",
    "  'goal': [],\n",
    "}\n",
    "\n",
    "num_waypoints_in_sequence = 5\n",
    "num_waypoint_sequences = 10_000\n",
    "policy = Policy(dist_type='analytical_crl_ratio',\n",
    "                planning_per_steps=2,\n",
    "                num_waypoints_in_sequence=num_waypoints_in_sequence,\n",
    "                num_waypoint_sequences=num_waypoint_sequences,\n",
    "                use_scripted_policy=True)\n",
    "policy.get_action = jax.jit(policy.get_action)\n",
    "\n",
    "for episode_idx in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "  obs, info = env.reset()\n",
    "  goal = info['goal']\n",
    "  trajs['goal'].append(goal)\n",
    "\n",
    "  key = jax.random.PRNGKey(episode_idx)\n",
    "  done = False\n",
    "  episode = {}\n",
    "  for k in trajs.keys():\n",
    "    if k not in ['goal']:\n",
    "      episode[k] = []\n",
    "\n",
    "  waypoint_seqs = -jnp.ones((num_waypoint_sequences, num_waypoints_in_sequence), dtype=jnp.int32)\n",
    "  waypoint_seq_idx = -jnp.ones((), dtype=jnp.int32)\n",
    "  waypoint = -jnp.ones((), dtype=jnp.int32)\n",
    "  for t in range(max_episode_steps):\n",
    "    key, policy_key = jax.random.split(key)\n",
    "\n",
    "    action, waypoint_seqs, waypoint_seq_idx, waypoint = policy.get_action(\n",
    "      obs, goal, policy_key, t, waypoint_seqs, waypoint_seq_idx, waypoint)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    episode['obs'].append(obs)\n",
    "    episode['action'].append(action)\n",
    "    episode['reward'].append(reward)\n",
    "    episode['done'].append(done)\n",
    "    episode['success'].append(info['success'])\n",
    "    episode['waypoint_seqs'].append(waypoint_seqs)\n",
    "    episode['waypoint_seq_idxs'].append(waypoint_seq_idx)\n",
    "    episode['waypoints'].append(waypoint)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "  for k in trajs.keys():\n",
    "    if k not in ['goal']:\n",
    "      trajs[k].append(episode[k])\n",
    "\n",
    "for k, v in trajs.items():\n",
    "  trajs[k] = np.array(v)\n",
    "\n",
    "success_rate = np.mean(np.any(trajs['success'] > 0, axis=-1))\n",
    "\n",
    "print()\n",
    "print(\"success rate = {}\".format(success_rate))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fccs3Qzh9Msi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155898547,
     "user_tz": 300,
     "elapsed": 14250,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "23ebb88d-cb1f-49b9-8ea2-ed6fb70f4a7a"
   },
   "id": "Fccs3Qzh9Msi",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2D contours\n",
    "obs = trajs['obs']\n",
    "action = trajs['action']\n",
    "goal = trajs['goal']\n",
    "waypoint_seqs = trajs['waypoint_seqs']\n",
    "waypoint_seq_idxs = trajs['waypoint_seq_idxs']\n",
    "waypoints = trajs['waypoints']\n",
    "\n",
    "NUM_PLOTS = 8\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=NUM_PLOTS)\n",
    "fig.set_figheight((3 + 1.5) * 1)\n",
    "fig.set_figwidth(3 * NUM_PLOTS)\n",
    "\n",
    "size = 10.0\n",
    "for episode_idx in range(NUM_PLOTS):\n",
    "  states = np.arange(num_states)\n",
    "  state_xys = env.state2xys(states)\n",
    "\n",
    "  obs_xys = env.state2xys(obs[episode_idx])\n",
    "  waypoint_seq_idx = waypoint_seq_idxs[episode_idx]\n",
    "  sampled_waypoint_seq = waypoint_seqs[episode_idx][np.arange(max_episode_steps), waypoint_seq_idx]\n",
    "  sampled_waypoint_seq_xys = env.state2xys(sampled_waypoint_seq)\n",
    "  # print(sampled_waypoint_seq_xys.shape)\n",
    "  waypoint_xys = env.state2xys(waypoints[episode_idx])\n",
    "  goal_xy = env.state2xys(goal[episode_idx])\n",
    "\n",
    "  ax = axes[episode_idx]\n",
    "  ax.scatter(state_xys[:, 0], state_xys[:, 1], s=size, marker='s', color='tab:blue')\n",
    "  ax.scatter(obs_xys[0, 0], obs_xys[0, 1], s=5 * size, marker='x', color='red', label=r'$s_0$')\n",
    "  ax.plot(obs_xys[:, 0], obs_xys[:, 1], color='k', label=r'$s_t$')\n",
    "  ax.scatter(waypoint_xys[:, 0], waypoint_xys[:, 1], s=size, marker='s', color='orange', label='$w$')\n",
    "  # ax.scatter(waypoint_seq_xys[0, 0, :, 0], waypoint_seq_xys[0, 0, :, 1], s=size, marker='s', color='orange', label='$w$')\n",
    "  # ax.scatter(sampled_waypoint_seq_xys[0, :, 0], sampled_waypoint_seq_xys[0, :, 1], s=20, marker='s', color='orange', label='$w$')\n",
    "  ax.scatter(sampled_waypoint_seq_xys[0, 0, 0], sampled_waypoint_seq_xys[0, 0, 1], s=size, marker='s', color='tab:purple', label='first $w$')\n",
    "  ax.scatter(goal_xy[0], goal_xy[1], s=5 * size, marker='*', color='green', label=r'$g$')\n",
    "\n",
    "  ax.set_xlabel(r\"$x$\")\n",
    "  ax.set_ylabel(r\"$y$\")\n",
    "  ax.set_xlim([-1.1, 11.1])\n",
    "  ax.set_ylim([-1.1, 11.1])\n",
    "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=2)\n",
    "  ax.set_title(r\"Analytical $\\log\\frac{p(g | s)}{p(g)}$ posterior\" + \"\\nsampling planner + scripted policy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "iNh7_dV7Z8zk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1735155899741,
     "user_tz": 300,
     "elapsed": 1192,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "e13bde22-f4c4-487b-b4b6-80a815022866"
   },
   "id": "iNh7_dV7Z8zk",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### CMD log ratio dist Dijkstra planner + scripted policy"
   ],
   "metadata": {
    "id": "Dp-bVMVZW-Ul"
   },
   "id": "Dp-bVMVZW-Ul"
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_EPISODES = 100\n",
    "successes = np.zeros(NUM_EPISODES)\n",
    "trajs = {\n",
    "  'obs': [],\n",
    "  'action': [],\n",
    "  'reward': [],\n",
    "  'done': [],\n",
    "  'success': [],\n",
    "  'waypoint_idx': [],\n",
    "  'waypoint': [],\n",
    "  'goal': [],\n",
    "}\n",
    "\n",
    "policy = Policy(dist_type='cmd_dist',\n",
    "                max_dist=1.5, min_dist=0.01,\n",
    "                planning_per_steps=1,\n",
    "                use_scripted_policy=True)\n",
    "policy.get_action = jax.jit(policy.get_action)\n",
    "\n",
    "w_idx = -jnp.ones((), dtype=jnp.int32)\n",
    "waypoint = -jnp.ones((), dtype=jnp.int32)\n",
    "for episode_idx in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "  obs, info = env.reset()\n",
    "  goal = info['goal']\n",
    "  trajs['goal'].append(goal)\n",
    "\n",
    "  key = jax.random.PRNGKey(episode_idx)\n",
    "  done = False\n",
    "  episode = {}\n",
    "  for k in trajs.keys():\n",
    "    if k not in ['goal']:\n",
    "      episode[k] = []\n",
    "  for t in range(max_episode_steps):\n",
    "    key, policy_key = jax.random.split(key)\n",
    "    action, w_idx, waypoint = policy.get_action(obs, goal, policy_key, t, w_idx, waypoint)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    episode['obs'].append(obs)\n",
    "    episode['action'].append(action)\n",
    "    episode['reward'].append(reward)\n",
    "    episode['done'].append(done)\n",
    "    episode['success'].append(info['success'])\n",
    "    episode['waypoint_idx'].append(w_idx)\n",
    "    episode['waypoint'].append(waypoint)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "  for k in trajs.keys():\n",
    "    if k not in ['goal']:\n",
    "      trajs[k].append(episode[k])\n",
    "\n",
    "for k, v in trajs.items():\n",
    "  trajs[k] = np.array(v)\n",
    "\n",
    "success_rate = np.mean(np.any(trajs['success'] > 0, axis=-1))\n",
    "\n",
    "print()\n",
    "print(\"success rate = {}\".format(success_rate))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQFfwZP4XNmT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1731960942202,
     "user_tz": 300,
     "elapsed": 15231,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "82bd8188-a006-44a4-ada6-4191fadce5ab"
   },
   "id": "tQFfwZP4XNmT",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2D contours\n",
    "obs = trajs['obs']\n",
    "action = trajs['action']\n",
    "goal = trajs['goal']\n",
    "waypoint_idx = trajs['waypoint_idx']\n",
    "waypoint = trajs['waypoint']\n",
    "\n",
    "NUM_PLOTS = 8\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=NUM_PLOTS)\n",
    "fig.set_figheight((3 + 1.5) * 1)\n",
    "fig.set_figwidth(3 * NUM_PLOTS)\n",
    "\n",
    "size = 10.0\n",
    "for episode_idx in range(NUM_PLOTS):\n",
    "  states = np.arange(num_states)\n",
    "  state_xys = env.state2xys(states)\n",
    "\n",
    "  obs_xys = env.state2xys(obs[episode_idx])\n",
    "  waypoint_xys = env.state2xys(waypoint[episode_idx])\n",
    "  goal_xy = env.state2xys(goal[episode_idx])\n",
    "\n",
    "  ax = axes[episode_idx]\n",
    "  ax.scatter(state_xys[:, 0], state_xys[:, 1], s=size, marker='s', color='tab:blue')\n",
    "  ax.scatter(obs_xys[0, 0], obs_xys[0, 1], s=5 * size, marker='x', color='red', label=r'$s_0$')\n",
    "  ax.plot(obs_xys[:, 0], obs_xys[:, 1], color='k', label=r'$s_t$')\n",
    "  ax.scatter(waypoint_xys[:, 0], waypoint_xys[:, 1], s=size, marker='s', color='orange', label='$w$')\n",
    "  ax.scatter(goal_xy[0], goal_xy[1], s=5 * size, marker='*', color='green', label=r'$g$')\n",
    "\n",
    "  ax.set_xlabel(r\"$x$\")\n",
    "  ax.set_ylabel(r\"$y$\")\n",
    "  ax.set_xlim([-1.1, 11.1])\n",
    "  ax.set_ylim([-1.1, 11.1])\n",
    "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=2)\n",
    "  ax.set_title(r\"$-\\log\\frac{\\hat{p}(g | s)}{\\hat{p}(g | g)}$ Dijkstra planner\" + \"\\n + scripted policy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "l_DMAPiiXvcq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1731960943193,
     "user_tz": 300,
     "elapsed": 985,
     "user": {
      "displayName": "Chongyi Zheng",
      "userId": "02526635062686944878"
     }
    },
    "outputId": "ae464924-0085-41e5-a137-1153aa913e37"
   },
   "id": "l_DMAPiiXvcq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xA7qI3BWbzFd"
   },
   "id": "xA7qI3BWbzFd",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
